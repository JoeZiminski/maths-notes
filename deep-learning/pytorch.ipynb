{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introduction to Neural Networks\n",
    "\n",
    "Neural networks are a very flexible model by which to learn complex, high dimensional functions. For example, if we have a dataset (X, y)\n",
    "where x \\in X are 28 x 28 images, and y \\in Y is an label, taking the form of an integer {1...10}. Our model could learn a function\n",
    "that takes x as an input and outputs the most likely label i.e. f(x) : x \\rightarrow y. Typically we will learn the condiional distribution\n",
    "p(y|x). Here y is a discrete random variable (1..10) and x is an image, so p(y|X) will be a 10-element discrete probability distribution.\n",
    "Note the model does not learn the probability distribution directly, but outputs 'logits' that we normalise with the softmax function\n",
    "to get a probability distribution\n",
    "so we will map x to l \\in R^10 where each entry is\n",
    " represented by the vector $\\mathbf{x}$ and a corresponding set of labels\n",
    "p(y|X).\n",
    "\n",
    "Therefore our model will approximate this function.\n",
    "\n",
    "In this example, we will follow the [Pytorch](https://docs.pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html) 'Getting Started' page to\n",
    "train a simple model. Then, we will get a deeper grib on how these models work by implementing the model ourselves in Numpy, which requires calculating\n",
    "the model update step ourselves.\n",
    "\n",
    "## Coding up a Neural Network in Pytorch\n",
    "\n",
    "This section follows [the pytorch inroducion](https://docs.pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)\n",
    "almost exactly, so the notes will be light here.\n",
    "\n",
    "\n",
    "### The data\n",
    "\n",
    "We will use the MINST dataset, containing 28x28 images and associtesd label. e.g. an Image of a cap, with the label 0.\n",
    "And image of a X, with the label 1. etc. for XXX images.\n",
    "\n",
    "We will split the dataset into training and testing data.\n"
   ],
   "id": "7d06a3a649e5399c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T16:42:06.245516Z",
     "start_time": "2025-12-09T16:42:06.226668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# DataLoader is an iterable around DataSet, which stores samples and their corresponding labels.\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",  # root directory\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n"
   ],
   "id": "4c808e140659e8f8",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[47]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m nn\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# DataLoader is an iterable around DataSet, which stores samples and their corresponding labels.\u001B[39;00m\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'torch'"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T16:42:06.253063800Z",
     "start_time": "2025-12-08T21:29:57.385627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n"
   ],
   "id": "932122acd4ca09d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This makes sense, our batch is (batch size, num channels (RGB), image height, image width) and for each image in the batch we have a single label (e.g. X = t-shirt).",
   "id": "e5f981b4ece35e93"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### The Model\n",
    "\n",
    "The very cool thing, we can just define the inputs, outputs, and define an architecture that we will be able to mould into the function of interest. Note the amazing beauty of how flexible this is, because of our set up. We simply define inputs, outputs and a black-box architecture (for now). Then we don't need to think about whats going on inside, we 'shape' this through iterative training of the weights on our known data. Once we have got a good mould we are done!\n"
   ],
   "id": "3652af145a41a46b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T16:42:06.253063800Z",
     "start_time": "2025-12-08T21:40:56.999160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Linear layers are:  yᵢ = φ( ∑ⱼ xⱼ Wᵀⱼᵢ + bᵢ ) so together y = φ(xW^T + b)\n",
    "        # where W is a (output_features, input_features) set of weights, b is vector of biases\n",
    "        # so we can easily control output shape. Layers are fully connected.\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "\n"
   ],
   "id": "b0e4b7b8959226d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### What is the Model Learning?\n",
    "\n",
    "Let's just look again what the model is learning. To fully characterise the dataset,  we would have the joint distribution\n",
    "between the data and the labels p(x, y). We can draw this out in the simple case:\n",
    "\n",
    "\n",
    "This also gives us information about the relative frequencies of the labels, y and images. Then we could convert to p(y|X) by dividing by p(x) i.e. Bayes\n",
    "\n",
    "\n",
    "However, this is much more complicated, and the aim of generative models. This is what we need to learn for generative models! but, it is more complex.\n",
    "Can maginalise out\n",
    "\n",
    "In our case, we will take a shortcut and just learn p(y|x). Basically it learns the feature space and how that maps to 28*28 dimension space and partitions it\n",
    "into 10 classes. The decision boundary is a hyper-surface in the space. Of course, this tells us nothing about p(x, y) because XXX. but it is a nice way of p(y|X).\n",
    "\n",
    "A simple drawing!\n",
    "\n",
    "but in reality, it is not a simple plane but a manifold in much hig\n",
    "\n",
    "hmmm, still not really sure what exactly we are learning. DOes this literally funnel an unkonwn x into a already-known x and then map p(y|x)?\n",
    "\n",
    "https://arxiv.org/abs/1311.2901\n",
    "http://neuralnetworksanddeeplearning.com/\n",
    "https://cs231n.github.io/linear-classify/\n",
    "https://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf"
   ],
   "id": "55eee77b6273f905"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T16:42:06.253063800Z",
     "start_time": "2025-12-08T21:29:59.166008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "# We can see we have Weights, Biases, Weights, Biases, Weights, Biases (3 layers)\n",
    "print(f\"The model parameters\")\n",
    "for name, param in model.named_parameters():\n",
    "      print(\n",
    "          f\"Name: {name}\\n\"\n",
    "          f\"Type: {type(param)}\\n\"\n",
    "          f\"Size: {param.size()}\\n\"\n",
    "          f\"e.g. {param[:5]}\\n\\n\")"
   ],
   "id": "8970d571e3f7ae62",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model parameters\n",
      "Name: linear_relu_stack.0.weight\n",
      "Type: <class 'torch.nn.parameter.Parameter'>\n",
      "Size: torch.Size([512, 784])\n",
      "e.g. tensor([[ 0.0192,  0.0357, -0.0006,  ..., -0.0333, -0.0235,  0.0033],\n",
      "        [-0.0218,  0.0026,  0.0204,  ...,  0.0252, -0.0259, -0.0091],\n",
      "        [ 0.0164, -0.0108,  0.0284,  ..., -0.0228,  0.0354,  0.0250],\n",
      "        [ 0.0121, -0.0232, -0.0101,  ..., -0.0092,  0.0006,  0.0351],\n",
      "        [-0.0173,  0.0102,  0.0229,  ...,  0.0273, -0.0032,  0.0100]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "\n",
      "Name: linear_relu_stack.0.bias\n",
      "Type: <class 'torch.nn.parameter.Parameter'>\n",
      "Size: torch.Size([512])\n",
      "e.g. tensor([ 0.0227,  0.0245,  0.0095,  0.0304, -0.0260], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "\n",
      "Name: linear_relu_stack.2.weight\n",
      "Type: <class 'torch.nn.parameter.Parameter'>\n",
      "Size: torch.Size([512, 512])\n",
      "e.g. tensor([[-0.0316, -0.0208, -0.0376,  ..., -0.0434,  0.0045, -0.0224],\n",
      "        [ 0.0171, -0.0396,  0.0083,  ..., -0.0402, -0.0002, -0.0139],\n",
      "        [-0.0201, -0.0098, -0.0215,  ..., -0.0074, -0.0322, -0.0273],\n",
      "        [-0.0105,  0.0341, -0.0043,  ..., -0.0437,  0.0024,  0.0096],\n",
      "        [ 0.0003,  0.0159,  0.0422,  ...,  0.0259, -0.0254, -0.0288]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "\n",
      "Name: linear_relu_stack.2.bias\n",
      "Type: <class 'torch.nn.parameter.Parameter'>\n",
      "Size: torch.Size([512])\n",
      "e.g. tensor([ 0.0285, -0.0168, -0.0206,  0.0379,  0.0348], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "\n",
      "Name: linear_relu_stack.4.weight\n",
      "Type: <class 'torch.nn.parameter.Parameter'>\n",
      "Size: torch.Size([10, 512])\n",
      "e.g. tensor([[-0.0252, -0.0008, -0.0320,  ..., -0.0011,  0.0215, -0.0009],\n",
      "        [-0.0426,  0.0246,  0.0369,  ...,  0.0417,  0.0145, -0.0030],\n",
      "        [ 0.0393, -0.0421, -0.0057,  ..., -0.0249,  0.0145,  0.0342],\n",
      "        [-0.0205, -0.0179,  0.0157,  ...,  0.0191, -0.0093, -0.0026],\n",
      "        [ 0.0191, -0.0117,  0.0228,  ...,  0.0283, -0.0263,  0.0300]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "\n",
      "Name: linear_relu_stack.4.bias\n",
      "Type: <class 'torch.nn.parameter.Parameter'>\n",
      "Size: torch.Size([10])\n",
      "e.g. tensor([-0.0314, -0.0343,  0.0079, -0.0357, -0.0442], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training and Evaluating the Model",
   "id": "b26828e71b6375e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T16:42:06.253063800Z",
     "start_time": "2025-12-08T21:29:59.393830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train(mode=True)  # put into 'training mode'\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Pred is (64, 10) tuple of predictions for this batch\n",
    "        # y is (64, 1) (classes)\n",
    "        # Cross entropy loss https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()  # perform one step θt <- f(θ_{t-1})\n",
    "        optimizer.zero_grad()  # zero the accumulated gradients, ready for the next step\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss = loss.item()  # Note `loss` is a object, we use `item()` to get the scalar loss\n",
    "            current = (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "train(train_dataloader, model, loss_fn, optimizer)"
   ],
   "id": "da357b6fe6802954",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.297308  [   64/60000]\n",
      "loss: 2.285923  [ 6464/60000]\n",
      "loss: 2.268851  [12864/60000]\n",
      "loss: 2.269377  [19264/60000]\n",
      "loss: 2.253051  [25664/60000]\n",
      "loss: 2.225462  [32064/60000]\n",
      "loss: 2.232326  [38464/60000]\n",
      "loss: 2.198354  [44864/60000]\n",
      "loss: 2.198508  [51264/60000]\n",
      "loss: 2.172440  [57664/60000]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T16:42:06.253063800Z",
     "start_time": "2025-12-08T21:30:05.019095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    \"\"\"\"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    model.eval()  # Go from train to eval mode\n",
    "\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():  # this just turns of gradient computation for speed\n",
    "\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            # pred_i = torch.argmax(torch.exp(pred) / torch.sum(torch.exp(pred)), axis=1)\n",
    "            pred_i = pred.argmax(1)  # of course, it doesn't matter if the logits are passed through softmax, which maintains transitivity\n",
    "            correct += (pred_i == y).type(torch.float).sum().item()\n",
    "            test_loss += loss_fn(pred, y)\n",
    "\n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "print(\"Epoch 1\")\n",
    "test(test_dataloader, model, loss_fn)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Epoch {i + 2}\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n"
   ],
   "id": "7d2a9d6f22016a93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Test Error: \n",
      " Accuracy: 43.0%, Avg loss: 2.161597 \n",
      "\n",
      "Epoch 2\n",
      "loss: 2.166717  [   64/60000]\n",
      "loss: 2.156315  [ 6464/60000]\n",
      "loss: 2.102063  [12864/60000]\n",
      "loss: 2.127685  [19264/60000]\n",
      "loss: 2.076803  [25664/60000]\n",
      "loss: 2.019103  [32064/60000]\n",
      "loss: 2.050828  [38464/60000]\n",
      "loss: 1.969575  [44864/60000]\n",
      "loss: 1.976774  [51264/60000]\n",
      "loss: 1.917304  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 1.904784 \n",
      "\n",
      "Epoch 3\n",
      "loss: 1.932660  [   64/60000]\n",
      "loss: 1.901088  [ 6464/60000]\n",
      "loss: 1.786190  [12864/60000]\n",
      "loss: 1.840296  [19264/60000]\n",
      "loss: 1.725365  [25664/60000]\n",
      "loss: 1.674734  [32064/60000]\n",
      "loss: 1.705723  [38464/60000]\n",
      "loss: 1.596955  [44864/60000]\n",
      "loss: 1.619671  [51264/60000]\n",
      "loss: 1.533497  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 1.537534 \n",
      "\n",
      "Epoch 4\n",
      "loss: 1.597141  [   64/60000]\n",
      "loss: 1.561808  [ 6464/60000]\n",
      "loss: 1.412652  [12864/60000]\n",
      "loss: 1.498589  [19264/60000]\n",
      "loss: 1.372249  [25664/60000]\n",
      "loss: 1.364505  [32064/60000]\n",
      "loss: 1.383409  [38464/60000]\n",
      "loss: 1.301641  [44864/60000]\n",
      "loss: 1.334398  [51264/60000]\n",
      "loss: 1.251118  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.267292 \n",
      "\n",
      "Epoch 5\n",
      "loss: 1.339720  [   64/60000]\n",
      "loss: 1.318575  [ 6464/60000]\n",
      "loss: 1.158088  [12864/60000]\n",
      "loss: 1.273363  [19264/60000]\n",
      "loss: 1.141379  [25664/60000]\n",
      "loss: 1.163889  [32064/60000]\n",
      "loss: 1.185737  [38464/60000]\n",
      "loss: 1.119894  [44864/60000]\n",
      "loss: 1.158803  [51264/60000]\n",
      "loss: 1.088780  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.100131 \n",
      "\n",
      "Epoch 6\n",
      "loss: 1.169723  [   64/60000]\n",
      "loss: 1.166807  [ 6464/60000]\n",
      "loss: 0.991699  [12864/60000]\n",
      "loss: 1.133397  [19264/60000]\n",
      "loss: 0.996681  [25664/60000]\n",
      "loss: 1.029001  [32064/60000]\n",
      "loss: 1.064936  [38464/60000]\n",
      "loss: 1.004613  [44864/60000]\n",
      "loss: 1.043977  [51264/60000]\n",
      "loss: 0.988165  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.991927 \n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Add a note here on generative vs discrimiantive. Here we only learn p(y|x) NOT p(x, y)! Here we simply do ML on p(y|x) !!!!\n",
    "\n"
   ],
   "id": "d22ca23278e0e345"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training a Neural Network by Hand - A Simple Example\n",
    "\n",
    "Pytorch...\n",
    "\n",
    "### The Model\n",
    "\n",
    "Let $l_1$, $l_2$ and $l_3$ be vector-valued functions representing our layers. $l_1$ takes our length $784$ row vector $\\mathbf{x}$ and returns a length $512$ vector. $l_2$ takes as input, and outputs, a length $512$ vector. Finally, $l_3$ takes a length $512$ vector as input and outputs a length $10$ vector. To make the mathematics clear, we will first use a very simple model, that has no bias or non-linear rectifying functions.\n",
    "\n",
    "\\begin{aligned}\n",
    "    &l_1(\\mathbf{x}) = \\mathbf{x}W_1 \\ \\ \\ \\ \\text{(1, 784) x (784, 512) = (1, 512)} \\\\\n",
    "    &l_2(\\mathbf{l_1}) = \\mathbf{l_1}W_2 \\ \\ \\ \\ \\text{(1, 512) x (512, 512) = (1, 512)} \\\\\n",
    "    &l_3(\\mathbf{l_2}) = \\mathbf{l_2}W_3 \\ \\ \\ \\ \\text{(1, 512) x (512, 10) = (1, 10)} \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "The $W$ are our matrices of weights with shape (outputs, inputs). $W_1$ is (10, 512),  $W_2$ is (512, 512) and $W_3$ is (512, 10). Here we take these matrices as shape (input, output) and index individual weights as $W_{3, ij}$ where $i$ is the $i$th neuron in the input layer, and $j$ is the $j$th neuron in the output layer. All vectors are row vectors. This is a little non-standard but this notation makes the derivations below much simpler. The shapes of these vector-matrix multiplications are shown on the right. We go from a length 784 vector $\\mathbf{x}$ to a length 10 vector $\\mathbf{l_3}$ as expected.\n",
    "\n",
    "Here, we already run into some notational difficulty. Our layers are functions, which when we evaluate at some input we get a vector. We then feed this vector into the next function. So our layers are both functions, and when evaluated are vectors. We will use the bold font to indicate the evaluated function. We will drop the bracket notation as it is too verbose.\n",
    "TODO: But in notation, we always use the evaluated vector, not the function symbol.\n",
    "\n",
    "Finally, we take the (1, 10) shape vector of logits and use these to compute our loss function, the cross-entropy loss:\n",
    "\n",
    "$$\n",
    "L(\\mathbf{l_3}, y) = -\\log \\dfrac{ \\exp{ \\mathbf{ l_{3, y} }}}{ \\sum_k \\exp{ \\mathbf{l_{3, k} }}}\n",
    "$$\n",
    "\n",
    "Here $\\mathbf{l_{3, k}}$ is the $k$-indexed element of our vector of logits from layer 3, and $y$ is the index of the correct label for this image. You may recognise the term after the $\\log$ as the [softmax function](https://en.wikipedia.org/wiki/Softmax_function#:~:text=The%20softmax%20function%2C%20also%20known,used%20in%20multinomial%20logistic%20regression) which normalises the logits to probabilities. Therefore, we are computing the probability of the input image $\\mathbf{x}$ being of label $y$ according to our model.\n",
    "\n",
    "Therefore, we are aiming to maximise the log probability associated with the correct label, $y$. This makes intuitive sense, we have an image $\\mathbf{x}$ and are computing a set of probabilities, one for each of the 10 labels. Of course, we want to maximise the probability we assign to the correct label $y$, we we know in this supervised context that it is the true, correct label for this image. Here we will equivalently minimise the negative log probability.\n",
    "\n"
   ],
   "id": "746aa183d90fab3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Predicting a label from an image - the forward pass\n",
    "\n",
    "For this simple network, it is easy to use our model to take an image $\\mathbf{x}$ (input), map it to our output of 10 logits and use these to predict a label $\\hat{y}$. First we apply the network to the input data:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\mathbf{l_3} &= l_3(l_2(l_1(\\mathbf{x}))) \\\\\n",
    "             &= (((\\mathbf{x}W_1) W_2) W_3 \\\\\n",
    "             &=  \\mathbf{x}W_1 W_2 W_3 \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "We then take this output, and the predicted label is the one that maximises the probability\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_{y} \\, \\mathrm{softmax}(\\mathbf{l}_3)_y\n",
    "$$\n",
    "\n",
    "i.e. the predict label is the one that is assigned the highest probability, according to our output layer (we take advantage of that fact that the possible labels, $\\hat{y}$, are defined as indices, so we can use them in index out the layer 3 logits)."
   ],
   "id": "e673dea83c2a38c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Backpropagation in our simple network\n",
    "\n",
    "TODO: go through optimisation here and the whole point of calculating these derivatives is to update the weights\n",
    "\n",
    "Next, we need to know how to update our weight matrices ($W_3$, $W_2$, $W_1$) during training. For example, take $W_{3, 12}$ that connects the first neuron from layer 2 to the second neuron in layer 3. How does a small change in this weight change the output of our loss function? In this case, a small change in $W_{3, 12}$ will result in a small change in the second neuron in layer 3 ($\\mathbf{l_{3, 2}$) which will directly affect the loss function $L(\\mathbf{l_3})$. This 'chain' of dependencies is capture by chain rule:\n",
    "\n",
    "\\begin{aligned}\n",
    "&\\dfrac{\\partial L(\\mathbf{l_3})}{\\partial W_3} = \\dfrac{\\partial L(\\mathbf{l_3})}{\\partial \\mathbf{l_3}}  \\dfrac{\\partial \\mathbf{l_3} }{\\partial W_3}\n",
    "\\end{aligned}\n",
    "\n",
    "i.e. the change in the loss function due to a small change in a weight equals the effect of a small change in the weight on layer 3, and then effect of the corresponding change in layer 3 on the loss function. We can express the derivatives of other weight matrices similarly:\n",
    "\n",
    "\\begin{aligned}\n",
    "&\\dfrac{\\partial L(\\mathbf{l_3})}{\\partial W_2} = \\dfrac{\\partial L(\\mathbf{l_3})}{\\partial \\mathbf{l_3}}  \\dfrac{\\partial \\mathbf{l_2}}{\\partial \\mathbf{l_3}}  \\dfrac{\\partial \\mathbf{l_2} }{\\partial W_3}  \\\\\n",
    "&\\dfrac{\\partial L(\\mathbf{l_3})}{\\partial W_1} = \\dfrac{\\partial L(\\mathbf{l_3})}{\\partial \\mathbf{l_3}}  \\dfrac{\\partial \\mathbf{l_2}}{\\partial \\mathbf{l_3}}  \\dfrac{\\partial \\mathbf{l_1}}{\\partial \\mathbf{l_2}}  \\dfrac{\\partial \\mathbf{l_1} }{\\partial W_1}\n",
    "\\end{aligned}\n",
    "\n",
    "TODO: check notation\n",
    "TODO: CHECK AGAINST https://www.jasonosajima.com/backprop.html\n",
    "\n",
    "Now, this notation is doing some extreme heavy lifting, and hiding a lot of complexity. Take $\\dfrac{\\partial L(\\mathbf{l_3})}{\\partial W_2}$, $L$ is a scalar-valued function (it takes as input a vector of length 10 and returns a scalar). We want to take the deriative of this w.r.t. a matrix - we want to know, how each element of $W_{3, ij}$ affects the loss function $L$. Therefore this will be a matrix, of size (512, 10) (the size of $W_3$). How about $\\dfrac{\\partial \\mathbf{l_2}}{\\partial \\mathbf{l_3}}$? This is the derivative of a vector-valued function (i.e. it outputs a vector, of length 512) wirth repsect to a vector! So we will ask, for each element of $\\mathbf{l_{3, j}}$ how does element $\\mathbf{l_{2, i}}$ affect it? As layer 3 has 10 neurons and layer 2 as 512, we will have a (10, 512) matrix of partial derivatives. And for $\\dfrac{\\partial \\mathbf{l_2} }{\\partial W_3}$ we will ask how a small change in each weight in $W_2$ will affect each dimension in $\\mathbf{l_2}$ - we will have a (512, 512, 512) 3-rank tensor!\n",
    "\n",
    "Fortunately the structure of our network means many of these partial derivatives are zero, and we can simply this a lot. Below, we go through each of these in turn, before implementing this network in Python. Note that for brevity we often write $\\partial L(\\mathbf{l_3})$ as $\\partial L$. TODO: only for $W_3$ at first."
   ],
   "id": "5cd3045544b6321d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### $\\dfrac{\\partial L(\\mathbf{l_3})}{\\partial W_3}$\n",
    "\n",
    "This is the exact derivative we want to compute to update the weights for the third layer, $W_3$. It will be a (512, 10) matrix, with each element been how a small change in that weight affects the loss i.e.\n",
    "\n",
    "$$\n",
    "    \\dfrac{\\partial L }{\\partial W} =\n",
    "    \\begin{bmatrix}\n",
    "        \\dfrac{ \\partial L }{ \\partial W_{1,1} } & \\dfrac{ \\partial L }{ \\partial W_{1,2} } & ... & \\dfrac{ \\partial L }{ \\partial W_{1,10} }\\\\\n",
    "        \\dfrac{ \\partial L }{ \\partial W_{2,1} } & \\ddots & & \\vdots \\\\\n",
    "        \\vdots \\\\\n",
    "        \\dfrac{ \\partial L }{ \\partial W_{512,1} } & \\dots & &   \\dfrac{ \\partial L }{ \\partial W_{512,10} }\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Remember in our convention, the first index of the weight matrix is the input, the second is the output. So for example, $W_{512, 10}$ is the weight from layer 2 neuron 512 to layer 3 unit 10.\n",
    "\n",
    "Add a picture of the neurons?\n",
    "\n",
    "Now that we are clear the matrix of partial derivative takes this form, we will compute its contents using the chain rule $\\dfrac{\\partial L(\\mathbf{l_3})}{\\partial W_3} = \\dfrac{\\partial L(\\mathbf{l_3})}{\\partial \\mathbf{l_3}}  \\dfrac{\\partial \\mathbf{l_3} }{\\partial W_3}$.\n"
   ],
   "id": "beef3ce4c09cf8cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### $\\dfrac{\\partial L(\\mathbf{l_3})}{\\partial \\mathbf{l_3}}$\n",
    "\n",
    "This is the derivative of the scalar-valued loss function with respect to a vector (the 10 units of layer 3). So, it will be a 10-dimension vector of\n",
    "partial derivatives, as below. Recall that we index the $i$th unit of a layer (e.g. the third layer) with the notation $\\mathbf{l_{3, i}}$.\n",
    "We can evaluate the derivative of the cross entropy loss directly: it is $\\text{softmax}(l_{3, i}) - 1$ when $i = y$\n",
    "(i.e. this is the derivative for the logit that matches the index of the correct label, $y$). The derivative for all other units in layer 3,\n",
    "when $i \\neq y$, $\\text{softmax}(l_{3, i})$ when $i \\neq y$. We can represent this as  $\\text{softmax}(l_{3, i}) - \\delta_{i, y}$ where $\\delta$ is the\n",
    "[Kronecker delta](https://en.wikipedia.org/wiki/Kronecker_delta). This is quite curious, see **Appendix 1** for the derivation.\n",
    "\n",
    "To summarise, the partial derivative of the loss w.r.t layer 3 is:\n",
    "\n",
    "$$\n",
    "\\dfrac{ \\partial L}{ \\partial \\mathbf{l_3} } =\n",
    "\\begin{bmatrix}\n",
    "    \\dfrac{\\partial L}{\\partial \\mathbf{l_{3, 1}}} \\\\\n",
    "    \\dfrac{\\partial L}{\\partial \\mathbf{l_{3, 2}}} \\\\\n",
    "    \\vdots\\\\\n",
    "    \\dfrac{\\partial L}{\\partial \\mathbf{l_{3, 10}}}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    \\text{softmax}(l_{3, 1}) - \\delta_{1, y} \\\\\n",
    "    \\text{softmax}(l_{3, 2}) - \\delta_{2, y} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\text{softmax}(l_{3, 10}) - \\delta_{10, y}\n",
    "\\end{bmatrix}\n",
    "$$"
   ],
   "id": "93d04754c3b7f10e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### $\\dfrac{\\partial \\mathbf{l_3} }{\\partial W_3}$\n",
    "\n",
    "Here we have the derivative of a vector with respect to a matrix. We are asking how a small change every weight in $W_3$ (a 512 by 10 matrix) effects each dimension of the vector $\\mathbf{l_3}$. So we have a (512, 10, 10) set of partial derivatives, a rank-3 tensor! How can we even deal with this in our computation?\n",
    "\n",
    "Luckily, the structure of our layered neural network means that this can be reduced. The strategy is to break the problem down to look at individual elements of $\\dfrac{\\partial L(l_3)}{\\partial W_3}$, as in the image above and fill in each term-wise e.g. $\\dfrac{\\partial L(l_3)}{\\partial W_{3, ij}}$ as well as using the total derivative rule (**Appendix B**):\n",
    "\n",
    "\\begin{align}\n",
    "\\dfrac{\\partial L(l_3)}{\\partial W_{3, ij}}\n",
    "&= \\dfrac{\\partial L}{\\partial l_{3}} \\dfrac{\\partial l_{3}}{\\partial W_{3,ij}}  \\\\\n",
    "&= \\sum_k \\dfrac{\\partial L}{\\partial l_{3,k}} \\dfrac{\\partial l_{3,k}}{\\partial W_{3,ij}} \\\\\n",
    "&= \\dfrac{\\partial L}{\\partial l_{3,j}} \\dfrac{\\partial l_{3,j}}{\\partial W_{3,ij}} \\\\\n",
    "&= \\dfrac{\\partial L}{\\partial l_{3,j}} l_{2, i}\n",
    "\\end{align}\n",
    "\n",
    "TODO Explain this in great detail with words. It is a nightmare to follow the indicies but quite clear in terms of neurons, inputs and outputs (show image). The nice thing about this is that the derivative for $\\dfrac{\\partial l_{3,k}}{\\partial W_{3,ij}}$ is $ 0 \\ \\forall \\ j \\neq k$  i.e. a weight that connects from unit $i$ has no effect on the $k$th unit in layer 3 unless it connects to unit $k$ i.e. $j = k$ in layer 3. Therefore, the term is simply by the scalar derivative rule:\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\dfrac{\\partial l_{3,j}}{\\partial W_{3,ij}} = \\dfrac{\\partial \\ l_{2, i} W_{3, ij} }{\\partial W_{3,ij}} =  l_{2, i}\n",
    "\\end{aligned}\n",
    "\n",
    "Also note that this will be the same for every column of the derivative L to W2 (see expansion above). i.e. all weight from l2_i will have the same derivative (it makes sense, as they compeltely depend on the change in l2_i as they go from W_ij to their target j\n",
    "TODO: explain this more intutiively, have an image"
   ],
   "id": "9f99399df4e05b7b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Putting this all together: $\\dfrac{\\partial L(\\mathbf{l_3})}{\\partial \\mathbf{l_3}} = \\dfrac{\\partial L(\\mathbf{l_3})}{\\partial \\mathbf{l_3}} \\dfrac{\\partial \\mathbf{l_3} }{\\partial W_3}$\n",
    "\n",
    "Note here the weights are all W_3 matrix and we omit for brevity. And now we know we can simply the expression to reduce the derivatgive only to\n",
    "respect that the neuron in layer three that our weight actually connects to (as above).\n",
    "\n",
    "$$\n",
    "\\dfrac{ \\partial L}{ \\partial \\mathbf{l_3} } =\n",
    "\\begin{bmatrix}\n",
    "\\dfrac{\\partial L}{ \\partial l_{3, 1} } \\dfrac{\\partial l_{3, 1} }{ \\partial W_{1,1} } \n",
    "& \\dfrac{\\partial L}{ \\partial l_{3, 2} } \\dfrac{\\partial l_{3, 2} }{ \\partial W_{1,2} } \n",
    "& \\dots \n",
    "& \\dfrac{\\partial L}{ \\partial l_{3, 10} } \\dfrac{\\partial l_{3, 10} }{ \\partial W_{1, 10} }  \\\\\n",
    "\\dfrac{\\partial L}{ \\partial l_{3, 1} } \\dfrac{\\partial l_{3, 1} }{ \\partial W_{2,1} } \n",
    "& \\ddots \\\\\n",
    "\\vdots \\\\\n",
    "\\dfrac{\\partial L}{ \\partial l_{3, 1} } \\dfrac{\\partial l_{3, 1} }{ \\partial W_{512,1} } \n",
    "& \\dots & \n",
    "& \\dfrac{\\partial L}{ \\partial l_{3, 10} } \\dfrac{\\partial l_{3, 10} }{ \\partial W_{512,10} }\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\dfrac{\\partial L}{ \\partial l_{3, 1} } l_{2, 1} & \n",
    "\\dfrac{\\partial L}{ \\partial l_{3, 2} } l_{2, 1}\n",
    "& \\dots & & \\\\\n",
    "\\dfrac{\\partial L}{ \\partial l_{3, 1} } l_{2, 2} & \\ddots \\\\\n",
    "\\vdots \\\\\n",
    "\\dfrac{\\partial L}{ \\partial l_{3, 1} } l_{2, 512}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Which we can write (recalling we are using row veectors, so transposing to a to column vector):\n",
    "\n",
    "$$\n",
    "\\mathbf{l_2}^T \\dfrac{\\partial L}{\\partial \\mathbf{l_3}}\n",
    "$$\n",
    "\n",
    "i.e. the outer product of our vector of partial derivatives of the loss with respect to the output layer 3 (as above)\n",
    "and layer two outputs of the network. Amazing how all this complexity goes down to something so simple! return to this.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "5edbec45cda23259"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### $\\dfrac{\\partial \\mathbf{l_2}}{\\partial \\mathbf{l_3}}$\n",
   "id": "683bcba3bae4a6ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "TODO: just drop the (l3) and make it clear somewhere to remember that it is a function of just l3.\n",
    "\n",
    "#### Putting the Layer 2 weights together: $\\dfrac{\\partial L(\\mathbf{l_3})}{\\partial W_2} = \\dfrac{\\partial L(\\mathbf{l_3})}{\\partial \\mathbf{l_3}}  \\dfrac{\\partial \\mathbf{l_2}}{\\partial \\mathbf{l_3}}  \\dfrac{\\partial \\mathbf{l_2} }{\\partial W_3} $\n",
    "\n",
    "Now this gets complicated again, more complicated above. While W2(1,1) say only affecst neuron 1 in layer 2, neuron 1 in layer 2\n",
    "is connected to all 10 neurons in layer 3. So l2_1 affects the loss through all of this. Therefore, we can proceed\n",
    "as above but we need to compute this iteratively. First, we can compute $\\dfrac{\\partial L}{\\partial \\mathbf{l_2}$, noting \n",
    "this is a derivative of a scalar valued loss function with respect to a vector. So we will have a 512 length vector\n",
    "of partial derivatives. We can compute each entry with the total derivative rule:\n",
    "\n",
    "$$\n",
    "\\dfrac{ \\partial L }{ \\partial l_{2, i} } = \\sum_k \\dfrac{ \\partial L }{ \\partial l_{3, k} } \\dfrac{\\partial l_{3, k} }{ \\partial l_{2, i} }\n",
    "$$\n",
    "\n",
    "say units!\n",
    "This doesn't simplify so well. But look at the derivative (its the weight matrix) and look at the computation we want to do. We can\n",
    "recover this vector with:\n",
    "\n",
    "I think dl/dl3 W_1\n",
    "\n",
    "$\\dfrac{\\partial L(\\mathbf{l_3})}{\\partial \\mathbf{l_3}}  \\dfrac{\\partial \\mathbf{l_2}}{\\partial \\mathbf{l_3}}$\n",
    "\n",
    "then again\n",
    "\n",
    "$\\dfrac{\\partial \\mathbf{l_2} }{\\partial W_3}$\n",
    "\n",
    "\n",
    "express as outer product\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "And simialrly for the final layer (check computation by hand, just put here the final result\n",
    "\n",
    "Note how we can reuse many of these computations!"
   ],
   "id": "e15ca1f42d995fdd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Same idea for L2, but add in the  extra term\n",
    "\n",
    "Then list all derivatives\n",
    "\n",
    "and implement.\n",
    "\n",
    "IMAE OF UNITS\n",
    "TODO: indicate W_ij is from unit i to j\n",
    "i.e. IMAGE OF THE MATRIX\n",
    "\n",
    "TODO: we should be writing L(l3, y)!!\n",
    "TODO: make sure that when we index a vector, it is not bold!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "See **Appendix 1** for the derivation of $\\dfrac{\\partial L(l_3)}{\\partial l_3}$, the notation $\\delta{iy}$ is the Kronecker delta. The only way to see this is the derivation.\n",
    "\n",
    "Note that this notation is doing a lot of heavy lifting! This is really key and ofen skipped, things work out nicely for our simple function that that is not always the case. For example, XX is a vector of a scalar valued function to a vector valued functino, XX is a derivative of a vector valued functino to a vector valued fucntion, and XX is the derivative of a vector valued function to a matrix! See **Appendix 2** for a full exploration of this. Simiarlly these are not scalar multiplcations, it depends on the results!\n",
    "\n",
    "TODO: A percenton lesson. Because each perceptron learns a hpyerplane, so indeed we are stackng lots of hyperplans together in a net and summing them! beautiful! and then nonlinear... and deepp...\n",
    "\n",
    "\n"
   ],
   "id": "aad169558bd52f3d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T16:42:06.253063800Z",
     "start_time": "2025-12-08T21:30:38.618261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "class MyNeuralNetwork:\n",
    "    def __init__(self, learning_rate=0.05):\n",
    "\n",
    "        self.a = learning_rate\n",
    "\n",
    "        # Define weight matrix (output dim, input dim) by convention\n",
    "        self.W1 = np.random.uniform(0, 0.05, (512, 28*28))\n",
    "        self.W2 = np.random.uniform(0, 0.05, (512, 512))\n",
    "        self.W3 = np.random.uniform(0, 0.05, (10, 512))\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        return - np.log( np.exp(x[y]) / np.sum(np.exp(x)) )\n",
    "\n",
    "    def update_weights(self, x, y):\n",
    "\n",
    "        # Forward pass\n",
    "        l1 = x @ self.W1.T\n",
    "        l2 = l1 @ self.W2.T\n",
    "        l3 = l2 @ self.W3.T\n",
    "\n",
    "        loss = self.loss(l3, y)\n",
    "        print(f\"Loss: {loss}\")\n",
    "\n",
    "        # 1) check these shapes, see why they must be wrong\n",
    "        # 2) use out products and matrix multiplications. You will never\n",
    "        # be able to do this without paying very careful attention to the matrix shapes!!!\n",
    "        dloss_dl3 = np.exp(l3) / np.sum(np.exp(l3))\n",
    "        dloss_dl3[y] -= 1\n",
    "        dl3_W3 = l2\n",
    "        dl3_l2 = self.W3.T\n",
    "        dl2_W2 = l1\n",
    "        dl2_l1 = self.W2.T\n",
    "        dl1_W1 = x\n",
    "\n",
    "        dloss_dW3 = dloss_dl3 * dl3_W3\n",
    "        dloss_dW2 = dloss_dl3 * dl3_l2 * dl2_W2\n",
    "        dloss_dW1 = dloss_dl3 * dl3_l2 * dl2_l1 * dl1_W1\n",
    "\n",
    "        self.W3 -= self.a * dloss_dW3\n",
    "        self.W2 -= self.a * dloss_dW2\n",
    "        self.W1 -= self.a * dloss_dW1\n",
    "\n",
    "    def train_step(self, x, y):\n",
    "\n",
    "        self.update_weights(x, y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "9e907e85b1133c4e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T16:42:06.253063800Z",
     "start_time": "2025-12-08T21:30:38.634084Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "288b7ee37c9c4480",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Appendix 1**\n",
    "\n",
    "The cross entropy loss is:\n",
    "\n",
    "\\begin{aligned}\n",
    "    L &= -\\log \\dfrac{ \\exp{ l_{3, y} } }{ \\sum_k \\exp{ l_{3, k} } } \\\\\n",
    "    &= -\\bigg[ \\log \\exp{ l_{3, y} } - \\log \\sum_k \\exp{ l_{3, k} } \\bigg] \\\\\n",
    "    &=  \\log \\sum_k \\exp{ l_{3, k} } - l_{3, y}\n",
    "\\end{aligned}\n",
    "\n",
    "(by the log laws). i.e. we take the logit  of layer 3 that matches the correct label $y$, normalise it to a probability\n",
    "with the softmax function and take the negative log.\n",
    "\n",
    "Let's start by taking the derivative with respect to $l_{3, y}$ where this is shorthand for $l_{3, i}$, $i=y$ i.e.\n",
    "the layer 3 logit for the label that is correct for this image. We are asking: how does a small change in this logit effect the loss?\n",
    "\n",
    "\\begin{aligned}\n",
    "\\dfrac{ \\partial L }{ \\partial l_{3, y} } &= \\dfrac{ \\partial }{ \\partial l_{3, y} } \\left( \\log \\sum_k \\exp{ l_{3, k} } - l_{3, y} \\right) \\\\\n",
    "&=  \\dfrac{ \\partial }{ \\partial l_{3, y} } \\log \\sum_k \\exp{ l_{3, k} } - \\dfrac{ \\partial }{ \\partial l_{3, y} }  l_{3, y}  \\\\\n",
    "&= \\dfrac{1}{ \\sum_k \\exp{ l_{3, k} } }  \\dfrac{ \\partial }{ \\partial l_{3, y} } \\sum_k \\exp{ l_{3, k} } - 1\n",
    "\\end{aligned}\n",
    "\n",
    "(by the derivative of $\\log x$ rule). Note that the last term will be $0$ when the the input dimension is not $y$ (because it is treated as a scalar).\n",
    "\n",
    "We see that in the sum, the derivative of $\\exp{ l_{3, i} } $ w.r.t $l_{3, k}$ is $\\exp{ l_{3, i} }$ when $i = k$ and $0$ otherwise (as it is treated as a scalar).\n",
    "So whatever dimension $i$ of $l_3$, we will input to the loss, we get $\\text{softmax}(\\mathbf{l_3})_i$ as the first term. But only when $i = y$ do we get $-1$ in the second term.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "c02e2a692bfa776c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
