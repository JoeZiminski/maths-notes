{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introduction to Neural Networks\n",
    "\n",
    "Neural networks are a very flexible model by which to learn complex, high dimensional functions. For example, if we have a dataset (X, y)\n",
    "where x \\in X are 28 x 28 images, and y \\in Y is an label, taking the form of an integer {1...10}. Our model could learn a function\n",
    "that takes x as an input and outputs the most likely label i.e. f(x) : x \\rightarrow y. Typically we will learn the condiional distribution\n",
    "p(y|x). Here y is a discrete random variable (1..10) and x is an image, so p(y|X) will be a 10-element discrete probability distribution.\n",
    "Note the model does not learn the probability distribution directly, but outputs 'logits' that we normalise with the softmax function\n",
    "to get a probability distribution\n",
    "so we will map x to l \\in R^10 where each entry is\n",
    " represented by the vector $\\mathbf{x}$ and a corresponding set of labels\n",
    "p(y|X).\n",
    "\n",
    "Therefore our model will approximate this function.\n",
    "\n",
    "In this example, we will follow the [Pytorch](https://docs.pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html) 'Getting Started' page to\n",
    "train a simple model. Then, we will get a deeper grib on how these models work by implementing the model ourselves in Numpy, which requires calculating\n",
    "the model update step ourselves.\n",
    "\n",
    "## Coding up a Neural Network in Pytorch\n",
    "\n",
    "This section follows [the pytorch inroducion](https://docs.pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)\n",
    "almost exactly, so the notes will be light here.\n",
    "\n",
    "\n",
    "### The data\n",
    "\n",
    "We will use the MINST dataset, containing 28x28 images and associtesd label. e.g. an Image of a cap, with the label 0.\n",
    "And image of a X, with the label 1. etc. for XXX images.\n",
    "\n",
    "We will split the dataset into training and testing data.\n"
   ],
   "id": "7d06a3a649e5399c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T21:29:57.385627Z",
     "start_time": "2025-12-08T21:29:53.133986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# DataLoader is an iterable around DataSet, which stores samples and their corresponding labels.\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",  # root directory\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n"
   ],
   "id": "4c808e140659e8f8",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T21:29:57.425079Z",
     "start_time": "2025-12-08T21:29:57.385627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n"
   ],
   "id": "932122acd4ca09d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This makes sense, our batch is (batch size, num channels (RGB), image height, image width) and for each image in the batch we have a single label (e.g. X = t-shirt).",
   "id": "e5f981b4ece35e93"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### The Model\n",
    "\n",
    "The very cool thing, we can just define the inputs, outputs, and define an architecture that we will be able to mould into the function of interest. Note the amazing beauty of how flexible this is, because of our set up. We simply define inputs, outputs and a black-box architecture (for now). Then we don't need to think about whats going on inside, we 'shape' this through iterative training of the weights on our known data. Once we have got a good mould we are done!\n"
   ],
   "id": "3652af145a41a46b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T21:40:57.009911Z",
     "start_time": "2025-12-08T21:40:56.999160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Linear layers are:  yᵢ = φ( ∑ⱼ xⱼ Wᵀⱼᵢ + bᵢ ) so together y = φ(xW^T + b)\n",
    "        # where W is a (output_features, input_features) set of weights, b is vector of biases\n",
    "        # so we can easily control output shape. Layers are fully connected.\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "\n"
   ],
   "id": "b0e4b7b8959226d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### What is the Model Learning?\n",
    "\n",
    "Let's just look again what the model is learning. To fully characterise the dataset,  we would have the joint distribution\n",
    "between the data and the labels p(x, y). We can draw this out in the simple case:\n",
    "\n",
    "\n",
    "This also gives us information about the relative frequencies of the labels, y and images. Then we could convert to p(y|X) by dividing by p(x) i.e. Bayes\n",
    "\n",
    "\n",
    "However, this is much more complicated, and the aim of generative models. This is what we need to learn for generative models! but, it is more complex.\n",
    "Can maginalise out\n",
    "\n",
    "In our case, we will take a shortcut and just learn p(y|x). Basically it learns the feature space and how that maps to 28*28 dimension space and partitions it\n",
    "into 10 classes. The decision boundary is a hyper-surface in the space. Of course, this tells us nothing about p(x, y) because XXX. but it is a nice way of p(y|X).\n",
    "\n",
    "A simple drawing!\n",
    "\n",
    "but in reality, it is not a simple plane but a manifold in much hig\n",
    "\n",
    "hmmm, still not really sure what exactly we are learning. DOes this literally funnel an unkonwn x into a already-known x and then map p(y|x)?\n",
    "\n",
    "https://arxiv.org/abs/1311.2901\n",
    "http://neuralnetworksanddeeplearning.com/\n",
    "https://cs231n.github.io/linear-classify/\n",
    "https://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf"
   ],
   "id": "55eee77b6273f905"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T21:29:59.374944Z",
     "start_time": "2025-12-08T21:29:59.166008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "# We can see we have Weights, Biases, Weights, Biases, Weights, Biases (3 layers)\n",
    "print(f\"The model parameters\")\n",
    "for name, param in model.named_parameters():\n",
    "      print(\n",
    "          f\"Name: {name}\\n\"\n",
    "          f\"Type: {type(param)}\\n\"\n",
    "          f\"Size: {param.size()}\\n\"\n",
    "          f\"e.g. {param[:5]}\\n\\n\")"
   ],
   "id": "8970d571e3f7ae62",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model parameters\n",
      "Name: linear_relu_stack.0.weight\n",
      "Type: <class 'torch.nn.parameter.Parameter'>\n",
      "Size: torch.Size([512, 784])\n",
      "e.g. tensor([[ 0.0192,  0.0357, -0.0006,  ..., -0.0333, -0.0235,  0.0033],\n",
      "        [-0.0218,  0.0026,  0.0204,  ...,  0.0252, -0.0259, -0.0091],\n",
      "        [ 0.0164, -0.0108,  0.0284,  ..., -0.0228,  0.0354,  0.0250],\n",
      "        [ 0.0121, -0.0232, -0.0101,  ..., -0.0092,  0.0006,  0.0351],\n",
      "        [-0.0173,  0.0102,  0.0229,  ...,  0.0273, -0.0032,  0.0100]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "\n",
      "Name: linear_relu_stack.0.bias\n",
      "Type: <class 'torch.nn.parameter.Parameter'>\n",
      "Size: torch.Size([512])\n",
      "e.g. tensor([ 0.0227,  0.0245,  0.0095,  0.0304, -0.0260], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "\n",
      "Name: linear_relu_stack.2.weight\n",
      "Type: <class 'torch.nn.parameter.Parameter'>\n",
      "Size: torch.Size([512, 512])\n",
      "e.g. tensor([[-0.0316, -0.0208, -0.0376,  ..., -0.0434,  0.0045, -0.0224],\n",
      "        [ 0.0171, -0.0396,  0.0083,  ..., -0.0402, -0.0002, -0.0139],\n",
      "        [-0.0201, -0.0098, -0.0215,  ..., -0.0074, -0.0322, -0.0273],\n",
      "        [-0.0105,  0.0341, -0.0043,  ..., -0.0437,  0.0024,  0.0096],\n",
      "        [ 0.0003,  0.0159,  0.0422,  ...,  0.0259, -0.0254, -0.0288]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "\n",
      "Name: linear_relu_stack.2.bias\n",
      "Type: <class 'torch.nn.parameter.Parameter'>\n",
      "Size: torch.Size([512])\n",
      "e.g. tensor([ 0.0285, -0.0168, -0.0206,  0.0379,  0.0348], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "\n",
      "Name: linear_relu_stack.4.weight\n",
      "Type: <class 'torch.nn.parameter.Parameter'>\n",
      "Size: torch.Size([10, 512])\n",
      "e.g. tensor([[-0.0252, -0.0008, -0.0320,  ..., -0.0011,  0.0215, -0.0009],\n",
      "        [-0.0426,  0.0246,  0.0369,  ...,  0.0417,  0.0145, -0.0030],\n",
      "        [ 0.0393, -0.0421, -0.0057,  ..., -0.0249,  0.0145,  0.0342],\n",
      "        [-0.0205, -0.0179,  0.0157,  ...,  0.0191, -0.0093, -0.0026],\n",
      "        [ 0.0191, -0.0117,  0.0228,  ...,  0.0283, -0.0263,  0.0300]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "\n",
      "Name: linear_relu_stack.4.bias\n",
      "Type: <class 'torch.nn.parameter.Parameter'>\n",
      "Size: torch.Size([10])\n",
      "e.g. tensor([-0.0314, -0.0343,  0.0079, -0.0357, -0.0442], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training and Evaluating the Model",
   "id": "b26828e71b6375e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T21:30:05.012698Z",
     "start_time": "2025-12-08T21:29:59.393830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train(mode=True)  # put into 'training mode'\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Pred is (64, 10) tuple of predictions for this batch\n",
    "        # y is (64, 1) (classes)\n",
    "        # Cross entropy loss https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()  # perform one step θt <- f(θ_{t-1})\n",
    "        optimizer.zero_grad()  # zero the accumulated gradients, ready for the next step\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss = loss.item()  # Note `loss` is a object, we use `item()` to get the scalar loss\n",
    "            current = (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "train(train_dataloader, model, loss_fn, optimizer)"
   ],
   "id": "da357b6fe6802954",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.297308  [   64/60000]\n",
      "loss: 2.285923  [ 6464/60000]\n",
      "loss: 2.268851  [12864/60000]\n",
      "loss: 2.269377  [19264/60000]\n",
      "loss: 2.253051  [25664/60000]\n",
      "loss: 2.225462  [32064/60000]\n",
      "loss: 2.232326  [38464/60000]\n",
      "loss: 2.198354  [44864/60000]\n",
      "loss: 2.198508  [51264/60000]\n",
      "loss: 2.172440  [57664/60000]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T21:30:38.610102Z",
     "start_time": "2025-12-08T21:30:05.019095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    \"\"\"\"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    model.eval()  # Go from train to eval mode\n",
    "\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():  # this just turns of gradient computation for speed\n",
    "\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            # pred_i = torch.argmax(torch.exp(pred) / torch.sum(torch.exp(pred)), axis=1)\n",
    "            pred_i = pred.argmax(1)  # of course, it doesn't matter if the logits are passed through softmax, which maintains transitivity\n",
    "            correct += (pred_i == y).type(torch.float).sum().item()\n",
    "            test_loss += loss_fn(pred, y)\n",
    "\n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "print(\"Epoch 1\")\n",
    "test(test_dataloader, model, loss_fn)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Epoch {i + 2}\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n"
   ],
   "id": "7d2a9d6f22016a93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Test Error: \n",
      " Accuracy: 43.0%, Avg loss: 2.161597 \n",
      "\n",
      "Epoch 2\n",
      "loss: 2.166717  [   64/60000]\n",
      "loss: 2.156315  [ 6464/60000]\n",
      "loss: 2.102063  [12864/60000]\n",
      "loss: 2.127685  [19264/60000]\n",
      "loss: 2.076803  [25664/60000]\n",
      "loss: 2.019103  [32064/60000]\n",
      "loss: 2.050828  [38464/60000]\n",
      "loss: 1.969575  [44864/60000]\n",
      "loss: 1.976774  [51264/60000]\n",
      "loss: 1.917304  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 1.904784 \n",
      "\n",
      "Epoch 3\n",
      "loss: 1.932660  [   64/60000]\n",
      "loss: 1.901088  [ 6464/60000]\n",
      "loss: 1.786190  [12864/60000]\n",
      "loss: 1.840296  [19264/60000]\n",
      "loss: 1.725365  [25664/60000]\n",
      "loss: 1.674734  [32064/60000]\n",
      "loss: 1.705723  [38464/60000]\n",
      "loss: 1.596955  [44864/60000]\n",
      "loss: 1.619671  [51264/60000]\n",
      "loss: 1.533497  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 1.537534 \n",
      "\n",
      "Epoch 4\n",
      "loss: 1.597141  [   64/60000]\n",
      "loss: 1.561808  [ 6464/60000]\n",
      "loss: 1.412652  [12864/60000]\n",
      "loss: 1.498589  [19264/60000]\n",
      "loss: 1.372249  [25664/60000]\n",
      "loss: 1.364505  [32064/60000]\n",
      "loss: 1.383409  [38464/60000]\n",
      "loss: 1.301641  [44864/60000]\n",
      "loss: 1.334398  [51264/60000]\n",
      "loss: 1.251118  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.267292 \n",
      "\n",
      "Epoch 5\n",
      "loss: 1.339720  [   64/60000]\n",
      "loss: 1.318575  [ 6464/60000]\n",
      "loss: 1.158088  [12864/60000]\n",
      "loss: 1.273363  [19264/60000]\n",
      "loss: 1.141379  [25664/60000]\n",
      "loss: 1.163889  [32064/60000]\n",
      "loss: 1.185737  [38464/60000]\n",
      "loss: 1.119894  [44864/60000]\n",
      "loss: 1.158803  [51264/60000]\n",
      "loss: 1.088780  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.100131 \n",
      "\n",
      "Epoch 6\n",
      "loss: 1.169723  [   64/60000]\n",
      "loss: 1.166807  [ 6464/60000]\n",
      "loss: 0.991699  [12864/60000]\n",
      "loss: 1.133397  [19264/60000]\n",
      "loss: 0.996681  [25664/60000]\n",
      "loss: 1.029001  [32064/60000]\n",
      "loss: 1.064936  [38464/60000]\n",
      "loss: 1.004613  [44864/60000]\n",
      "loss: 1.043977  [51264/60000]\n",
      "loss: 0.988165  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.991927 \n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Add a note here on generative vs discrimiantive. Here we only learn p(y|x) NOT p(x, y)! Here we simply do ML on p(y|x) !!!!\n",
    "\n"
   ],
   "id": "d22ca23278e0e345"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training a Neural Network by Hand - A Simple Example\n",
    "\n",
    "Pytorch...\n",
    "\n",
    "### The Model\n",
    "\n",
    "Let $l_1$, $l_2$ and $l_3$ be vector-valued functions representing our layers. $l_1$ takes our length $784$ row vector $mathbf{x}$ and returns a length $512$ vector. $l_2$ takes as input, and outputs, a length $512$ vector. Finally, $l_3$ takes a length $512$ vector as input and outputs a length $10$ vector. To make the mathematics clear, we will first use a very simple model, that has no bias or non-linear rectifying functions.\n",
    "\n",
    "\\begin{aligned}\n",
    "    &l_1(\\mathbf{x}) = \\mathbf{x}W_1^T \\ \\ \\ \\ \\text{(1, 10) x (10, 512) = (1, 512)} \\\\\n",
    "    &l_2(\\mathbf{l_1}) = \\mathbf{l_1}W_2^T \\ \\ \\ \\ \\text{(1, 512) x (512, 512) = (1, 512)} \\\\\n",
    "    &l_3(\\mathbf{l_2}) = \\mathbf{l_2}W_3^T \\ \\ \\ \\ \\text{(1, 512) x (512, 10) = (1, 10)} \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "The $W$ are our matrices of weights with shape (outputs, inputs). $W_1$ is (512, 10),  $W_1$ is (512, 10) and $W_1$ is (512, 10). The shapes of these vector-matrix multiplications are shown on the right. We go from a length 784 vector to a length 10 vector as expected.\n",
    "\n",
    "Here, we already run into some notational difficulty. Our layers are functions, which when we evaluate at some input we get a vector. We then feed this vector into the next function. So our layers are both functions, and when evaluated are vectors. We will use the bold font to indicate the evaluated function. We will drop the bracket notation as it is too verbose.\n",
    "\n",
    "Finally, we take the (1, 10) shape vector of logits and use these to compute our loss function, the cross-entropy loss:\n",
    "\n",
    "$$\n",
    "L(\\mathbf{l_3}, y) = -\\log \\dfrac{ \\exp{ \\mathbf{ l_{3, y} }}}{ \\sum_k \\exp{ \\mathbf{l_{3, k} }}}\n",
    "$$\n",
    "\n",
    "Here $\\mathbf{l_{3, k}}$ is the $k$-indexed element of our vector of logits from layer 3, and $y$ is the index of the correct label for this image. You may recognise the term after the $\\log$ as the [softmax function](https://en.wikipedia.org/wiki/Softmax_function#:~:text=The%20softmax%20function%2C%20also%20known,used%20in%20multinomial%20logistic%20regression) which normalises the logits to probabilities. Therefore, we are computing the probability of the input image $\\mathbf{x}$ being of label $y$ according to our model.\n",
    "\n",
    "Therefore, we are aiming to maximise the log probability associated with the correct label, $y$. This makes intuitive sense, we have an image $\\mathbf{x}$ and are computing a set of probabilities, one for each of the 10 labels. Of course, we want to maximise the probability we assign to the correct label $y$, we we know in this supervised context that it is the true, correct label for this image. Here we will equivalently minimise the negative log probability.\n",
    "\n"
   ],
   "id": "746aa183d90fab3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Predicting a label from an image - the forward pass\n",
    "\n",
    "For this simple network, it is easy to use our model to take an image $\\mathbf{x}$ (input), map it to our output of 10 logits and use these to predict a label $\\hat{y}$. First we apply the network to the input data:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\mathbf{l_3} $= l_3(l_2(l_1(\\mathbf{x}))) \\\\\n",
    "             %=\n",
    "\\end{aligned}\n",
    "\n",
    "\n"
   ],
   "id": "e673dea83c2a38c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "How can we possibly make sense of these multiplications? (1, 10) X (10, 512) X (10, 512, 512)? Well, the first two work, and fortunately the third decomposes because the majority of the elements are zero. The stratergy is to break the problem down to look at individual elements of $\\dfrac{\\partial L(l_3)}{\\partial W_3}$, as in the image above and fill in each term-wise e.g. $\\dfrac{\\partial L(l_3)}{\\partial W_{3, ij}}$ as well as using the total derivative rule:\n",
    "\n",
    "TODO: NEED TO HANDLE THE TRANSPOSE HERE\n",
    "HUGE IMAGE ONF THE MATRIX\n",
    "\n",
    "Then how we can compute this with the outer product!\n",
    "\n",
    "Same idea for L2, but add in the  extra term\n",
    "\n",
    "Then list all derivatives\n",
    "\n",
    "and implement.\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\dfrac{\\partial L(l_3)}{\\partial W_{3, ij}}\n",
    "&= \\dfrac{\\partial L(l_3)}{\\partial l_{3}} \\dfrac{\\partial l_{3}}{\\partial W_{3,ij}}  \\\\\n",
    "&= \\sum_k \\dfrac{\\partial L(l_3)}{\\partial l_{3,k}} \\dfrac{\\partial l_{3,k}}{\\partial W_{3,ij}}\n",
    "\\end{align}\n",
    "\n",
    "The nice thing about this is that the derivative for $\\dfrac{\\partial l_{3,k}}{\\partial W_{3,ij}}$ is $ 0 \\ \\forall \\ j \\neq k$  i.e. a weight that connects from unit $i$ has no effect on the $k$th unit in layer 3 unless it connects to unit $k$ i.e. $j = k$ in layer 3. Therefore, the term is\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\dfrac{\\partial l_{3,j}}{\\partial W_{3,ij}} = \\dfrac{\\partial \\ l_{2, i} W_{3, ij} }{\\partial W_{3,ij}} =  l_{2, i}\n",
    "\\end{aligned}\n",
    "\n",
    "IMAE OF UNITS\n",
    "TODO: indicate W_ij is from unit i to j\n",
    "i.e. IMAGE OF THE MATRIX\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "See **Appendix 1** for the derivation of $\\dfrac{\\partial L(l_3)}{\\partial l_3}$, the notation $\\delta{iy}$ is the Kronecker delta. The only way to see this is the derivation.\n",
    "\n",
    "Note that this notation is doing a lot of heavy lifting! This is really key and ofen skipped, things work out nicely for our simple function that that is not always the case. For example, XX is a vector of a scalar valued function to a vector valued functino, XX is a derivative of a vector valued functino to a vector valued fucntion, and XX is the derivative of a vector valued function to a matrix! See **Appendix 2** for a full exploration of this. Simiarlly these are not scalar multiplcations, it depends on the results!\n",
    "\n",
    "TODO: A percenton lesson. Because each perceptron learns a hpyerplane, so indeed we are stackng lots of hyperplans together in a net and summing them! beautiful! and then nonlinear... and deepp...\n",
    "\n",
    "\n"
   ],
   "id": "aad169558bd52f3d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T21:30:38.624295Z",
     "start_time": "2025-12-08T21:30:38.618261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "class MyNeuralNetwork:\n",
    "    def __init__(self, learning_rate=0.05):\n",
    "\n",
    "        self.a = learning_rate\n",
    "\n",
    "        # Define weight matrix (output dim, input dim) by convention\n",
    "        self.W1 = np.random.uniform(0, 0.05, (512, 28*28))\n",
    "        self.W2 = np.random.uniform(0, 0.05, (512, 512))\n",
    "        self.W3 = np.random.uniform(0, 0.05, (10, 512))\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        return - np.log( np.exp(x[y]) / np.sum(np.exp(x)) )\n",
    "\n",
    "    def update_weights(self, x, y):\n",
    "\n",
    "        # Forward pass\n",
    "        l1 = x @ self.W1.T\n",
    "        l2 = l1 @ self.W2.T\n",
    "        l3 = l2 @ self.W3.T\n",
    "\n",
    "        loss = self.loss(l3, y)\n",
    "        print(f\"Loss: {loss}\")\n",
    "\n",
    "        # 1) check these shapes, see why they must be wrong\n",
    "        # 2) use out products and matrix multiplications. You will never\n",
    "        # be able to do this without paying very careful attention to the matrix shapes!!!\n",
    "        dloss_dl3 = np.exp(l3) / np.sum(np.exp(l3))\n",
    "        dloss_dl3[y] -= 1\n",
    "        dl3_W3 = l2\n",
    "        dl3_l2 = self.W3.T\n",
    "        dl2_W2 = l1\n",
    "        dl2_l1 = self.W2.T\n",
    "        dl1_W1 = x\n",
    "\n",
    "        dloss_dW3 = dloss_dl3 * dl3_W3\n",
    "        dloss_dW2 = dloss_dl3 * dl3_l2 * dl2_W2\n",
    "        dloss_dW1 = dloss_dl3 * dl3_l2 * dl2_l1 * dl1_W1\n",
    "\n",
    "        self.W3 -= self.a * dloss_dW3\n",
    "        self.W2 -= self.a * dloss_dW2\n",
    "        self.W1 -= self.a * dloss_dW1\n",
    "\n",
    "    def train_step(self, x, y):\n",
    "\n",
    "        self.update_weights(x, y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "9e907e85b1133c4e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T21:30:38.636220Z",
     "start_time": "2025-12-08T21:30:38.634084Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "288b7ee37c9c4480",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
