{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "621d2aefba8f6ec4",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks\n",
    "\n",
    "Neural networks are a very flexible model through which to learn complex, high dimensional functions. First, we will apply the model in Pytorch, and then implement it by hand, using multivariable calculus to derive the parameter updates.\n",
    "\n",
    "In this example, we will apply a neural network to an image classification problem. We have a dataset containing images clothes, with each image having a label describing the item of clothing in the image. Each $28 \\times 28$-pixel image is represented as a length $784$ vector $\\mathbf{x}$. We use bold font to indicate a vector. There are 10 possible items of clothing (e.g. t-shirt, hat) in our dataset, so $y$ is an integer in the range $[0, 10]$. Together, our sample space is pairings of images and labels, $(\\mathbf{x}_i, y_i) \\sim (\\mathcal{X}, \\mathcal{Y})$ (i.e. a single image, label pair index by $i$ can be drawn from the sample space of all pairs of images and labels). s  \n",
    "\n",
    "\n",
    "We want to learn the conditional distribution $p(y| \\mathbf{x})$ i.e. what is the probability of the label $y$ given an image, $\\mathbf{x}$. For example, what is the probability this is an image $\\mathbf{x}$ is of a t-shirt? In this case, we want to learn a function $f(\\mathbf{x}) : \\mathbf{x} \\rightarrow \\mathbf{y}$. It will take a vector of length $784$ and output a vector of length $10$, with each element of the output vector assigning some weight related to the probability of image $\\mathbf{x}$ being a particular label $y$. These unnormalised weights output by the model are called 'logits'.\n",
    "\n",
    "<img src=\"nn-image-1.png\" width=\"400\">\n",
    "\n",
    "In this example, we will first follow the [Pytorch](https://docs.pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html) 'Getting Started' page to train a simple model for this classification task. Then, we will get a deeper grip on how these models work by implementing the model ourselves in Numpy, which requires calculating the derivatives required for model training ourselves.\n",
    "\n",
    "## Coding up a Neural Network in Pytorch\n",
    "\n",
    "This section exactly follows [the Pytorch introduction](https://docs.pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html).\n",
    "\n",
    "### The data\n",
    "\n",
    "We will use the FashionMINST dataset, containing $28 \\times 28$ images of clothing, 10 possible labels. In total there are $60,000$ image sin the training set and $10,000$ images in the test set.\n",
    "\n",
    "<img src=\"FashionMNIST.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c808e140659e8f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T20:16:45.132456Z",
     "start_time": "2025-12-09T20:16:45.099069Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# DataLoader is an iterable around DataSet, which stores samples and their corresponding labels.\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",  # root directory\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932122acd4ca09d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T20:16:45.150936Z",
     "start_time": "2025-12-09T20:16:45.139966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f981b4ece35e93",
   "metadata": {},
   "source": [
    "This makes sense, our batch is (batch size, num channels (RGB), image height, image width) and for each image in the batch we have a single label (e.g. X = t-shirt).\n",
    "\n",
    "When training, Batch Size...\n",
    "\n",
    "Epoch..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3652af145a41a46b",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "The very cool thing, we can just define the inputs, outputs, and define an architecture that we will be able to mould into the function of interest. Note the amazing beauty of how flexible this is, because of our set up. We simply define inputs, outputs and a black-box architecture (for now). Then we don't need to think about whats going on inside, we 'shape' this through iterative training of the weights on our known data. Once we have got a good mould we are done!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e4b7b8959226d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T20:16:45.168023Z",
     "start_time": "2025-12-09T20:16:45.160191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Linear layers are:  yᵢ = φ( ∑ⱼ xⱼ Wᵀⱼᵢ + bᵢ ) so together y = φ(xW^T + b)\n",
    "        # where W is a (output_features, input_features) set of weights, b is vector of biases\n",
    "        # so we can easily control output shape. Layers are fully connected.\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eee77b6273f905",
   "metadata": {},
   "source": [
    "\n",
    "#### What is the Model Learning?\n",
    "\n",
    "Let's just look again what the model is learning. To fully characterise the dataset,  we would have the joint distribution\n",
    "between the data and the labels p(x, y). We can draw this out in the simple case:\n",
    "\n",
    "\n",
    "This also gives us information about the relative frequencies of the labels, y and images. Then we could convert to p(y|X) by dividing by p(x) i.e. Bayes\n",
    "\n",
    "\n",
    "However, this is much more complicated, and the aim of generative models. This is what we need to learn for generative models! but, it is more complex.\n",
    "Can maginalise out\n",
    "\n",
    "In our case, we will take a shortcut and just learn p(y|x). Basically it learns the feature space and how that maps to 28*28 dimension space and partitions it\n",
    "into 10 classes. The decision boundary is a hyper-surface in the space. Of course, this tells us nothing about p(x, y) because XXX. but it is a nice way of p(y|X).\n",
    "\n",
    "A simple drawing!\n",
    "\n",
    "but in reality, it is not a simple plane but a manifold in much hig\n",
    "\n",
    "hmmm, still not really sure what exactly we are learning. DOes this literally funnel an unkonwn x into a already-known x and then map p(y|x)?\n",
    "\n",
    "https://arxiv.org/abs/1311.2901\n",
    "http://neuralnetworksanddeeplearning.com/\n",
    "https://cs231n.github.io/linear-classify/\n",
    "https://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8970d571e3f7ae62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T20:16:45.195384Z",
     "start_time": "2025-12-09T20:16:45.189151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model parameters\n",
      "Name: linear_relu_stack.0.weight\n",
      "Type: <class 'torch.nn.parameter.Parameter'>\n",
      "Size: torch.Size([512, 784])\n",
      "e.g. tensor([[ 0.0242, -0.0248, -0.0166,  ..., -0.0256,  0.0325, -0.0028],\n",
      "        [ 0.0264,  0.0106, -0.0349,  ..., -0.0113, -0.0121,  0.0221],\n",
      "        [ 0.0038, -0.0243,  0.0197,  ..., -0.0243, -0.0306,  0.0113],\n",
      "        [-0.0337, -0.0340, -0.0342,  ...,  0.0348, -0.0187, -0.0099],\n",
      "        [ 0.0065,  0.0300,  0.0117,  ...,  0.0249, -0.0121,  0.0218]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "\n",
      "Name: linear_relu_stack.0.bias\n",
      "Type: <class 'torch.nn.parameter.Parameter'>\n",
      "Size: torch.Size([512])\n",
      "e.g. tensor([-0.0340, -0.0322, -0.0116,  0.0172, -0.0129], grad_fn=<SliceBackward0>)\n",
      "\n",
      "\n",
      "Name: linear_relu_stack.2.weight\n",
      "Type: <class 'torch.nn.parameter.Parameter'>\n",
      "Size: torch.Size([512, 512])\n",
      "e.g. tensor([[ 0.0256, -0.0107, -0.0299,  ..., -0.0403,  0.0318, -0.0249],\n",
      "        [ 0.0342, -0.0205,  0.0428,  ...,  0.0127,  0.0360, -0.0173],\n",
      "        [-0.0149,  0.0350, -0.0221,  ...,  0.0298,  0.0333, -0.0402],\n",
      "        [-0.0007, -0.0317,  0.0151,  ...,  0.0384,  0.0278, -0.0417],\n",
      "        [ 0.0317, -0.0260,  0.0305,  ...,  0.0030,  0.0208, -0.0424]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "\n",
      "Name: linear_relu_stack.2.bias\n",
      "Type: <class 'torch.nn.parameter.Parameter'>\n",
      "Size: torch.Size([512])\n",
      "e.g. tensor([-0.0398, -0.0158,  0.0001,  0.0098, -0.0318], grad_fn=<SliceBackward0>)\n",
      "\n",
      "\n",
      "Name: linear_relu_stack.4.weight\n",
      "Type: <class 'torch.nn.parameter.Parameter'>\n",
      "Size: torch.Size([10, 512])\n",
      "e.g. tensor([[-0.0050,  0.0381,  0.0136,  ...,  0.0361, -0.0427,  0.0113],\n",
      "        [-0.0285, -0.0210,  0.0118,  ..., -0.0344, -0.0051, -0.0302],\n",
      "        [-0.0077, -0.0141, -0.0155,  ...,  0.0079, -0.0294, -0.0201],\n",
      "        [ 0.0030, -0.0211,  0.0398,  ..., -0.0036, -0.0310, -0.0081],\n",
      "        [ 0.0365,  0.0120,  0.0255,  ...,  0.0139, -0.0323,  0.0019]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "\n",
      "Name: linear_relu_stack.4.bias\n",
      "Type: <class 'torch.nn.parameter.Parameter'>\n",
      "Size: torch.Size([10])\n",
      "e.g. tensor([ 0.0121, -0.0431,  0.0244, -0.0143, -0.0158], grad_fn=<SliceBackward0>)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "# We can see we have Weights, Biases, Weights, Biases, Weights, Biases (3 layers)\n",
    "print(f\"The model parameters\")\n",
    "for name, param in model.named_parameters():\n",
    "      print(\n",
    "          f\"Name: {name}\\n\"\n",
    "          f\"Type: {type(param)}\\n\"\n",
    "          f\"Size: {param.size()}\\n\"\n",
    "          f\"e.g. {param[:5]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26828e71b6375e",
   "metadata": {},
   "source": [
    "### Training and Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da357b6fe6802954",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T20:16:49.392600Z",
     "start_time": "2025-12-09T20:16:45.213973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.311416  [   64/60000]\n",
      "loss: 2.297227  [ 6464/60000]\n",
      "loss: 2.284765  [12864/60000]\n",
      "loss: 2.277797  [19264/60000]\n",
      "loss: 2.245504  [25664/60000]\n",
      "loss: 2.223144  [32064/60000]\n",
      "loss: 2.229235  [38464/60000]\n",
      "loss: 2.200769  [44864/60000]\n",
      "loss: 2.204567  [51264/60000]\n",
      "loss: 2.150434  [57664/60000]\n"
     ]
    }
   ],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train(mode=True)  # put into 'training mode'\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Pred is (64, 10) tuple of predictions for this batch\n",
    "        # y is (64, 1) (classes)\n",
    "        # Cross entropy loss https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()  # perform one step θt <- f(θ_{t-1})\n",
    "        optimizer.zero_grad()  # zero the accumulated gradients, ready for the next step\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss = loss.item()  # Note `loss` is a object, we use `item()` to get the scalar loss\n",
    "            current = (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "train(train_dataloader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2a9d6f22016a93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T20:17:12.194829Z",
     "start_time": "2025-12-09T20:16:49.400115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Test Error: \n",
      " Accuracy: 36.9%, Avg loss: 2.159129 \n",
      "\n",
      "Epoch 2\n",
      "loss: 2.175913  [   64/60000]\n",
      "loss: 2.156889  [ 6464/60000]\n",
      "loss: 2.109339  [12864/60000]\n",
      "loss: 2.123337  [19264/60000]\n",
      "loss: 2.051893  [25664/60000]\n",
      "loss: 2.000705  [32064/60000]\n",
      "loss: 2.027452  [38464/60000]\n",
      "loss: 1.956121  [44864/60000]\n",
      "loss: 1.970606  [51264/60000]\n",
      "loss: 1.868530  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 1.889209 \n",
      "\n",
      "Epoch 3\n",
      "loss: 1.925272  [   64/60000]\n",
      "loss: 1.883625  [ 6464/60000]\n",
      "loss: 1.786613  [12864/60000]\n",
      "loss: 1.827395  [19264/60000]\n",
      "loss: 1.697302  [25664/60000]\n",
      "loss: 1.655534  [32064/60000]\n",
      "loss: 1.676771  [38464/60000]\n",
      "loss: 1.589110  [44864/60000]\n",
      "loss: 1.623531  [51264/60000]\n",
      "loss: 1.493784  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.532667 \n",
      "\n",
      "Epoch 4\n",
      "loss: 1.596773  [   64/60000]\n",
      "loss: 1.552969  [ 6464/60000]\n",
      "loss: 1.422226  [12864/60000]\n",
      "loss: 1.491803  [19264/60000]\n",
      "loss: 1.362407  [25664/60000]\n",
      "loss: 1.363337  [32064/60000]\n",
      "loss: 1.370365  [38464/60000]\n",
      "loss: 1.304668  [44864/60000]\n",
      "loss: 1.344313  [51264/60000]\n",
      "loss: 1.227757  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 1.266162 \n",
      "\n",
      "Epoch 5\n",
      "loss: 1.341841  [   64/60000]\n",
      "loss: 1.314810  [ 6464/60000]\n",
      "loss: 1.162503  [12864/60000]\n",
      "loss: 1.266502  [19264/60000]\n",
      "loss: 1.135582  [25664/60000]\n",
      "loss: 1.166634  [32064/60000]\n",
      "loss: 1.176979  [38464/60000]\n",
      "loss: 1.123147  [44864/60000]\n",
      "loss: 1.165496  [51264/60000]\n",
      "loss: 1.070059  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.097651 \n",
      "\n",
      "Epoch 6\n",
      "loss: 1.168139  [   64/60000]\n",
      "loss: 1.162590  [ 6464/60000]\n",
      "loss: 0.991322  [12864/60000]\n",
      "loss: 1.124865  [19264/60000]\n",
      "loss: 0.995096  [25664/60000]\n",
      "loss: 1.032257  [32064/60000]\n",
      "loss: 1.056600  [38464/60000]\n",
      "loss: 1.006710  [44864/60000]\n",
      "loss: 1.047809  [51264/60000]\n",
      "loss: 0.972327  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.988704 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    \"\"\"\"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    model.eval()  # Go from train to eval mode\n",
    "\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():  # this just turns of gradient computation for speed\n",
    "\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            # pred_i = torch.argmax(torch.exp(pred) / torch.sum(torch.exp(pred)), axis=1)\n",
    "            pred_i = pred.argmax(1)  # of course, it doesn't matter if the logits are passed through softmax, which maintains transitivity\n",
    "            correct += (pred_i == y).type(torch.float).sum().item()\n",
    "            test_loss += loss_fn(pred, y)\n",
    "\n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "print(\"Epoch 1\")\n",
    "test(test_dataloader, model, loss_fn)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Epoch {i + 2}\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22ca23278e0e345",
   "metadata": {},
   "source": [
    "Add a note here on generative vs discrimiantive. Here we only learn p(y|x) NOT p(x, y)! Here we simply do ML on p(y|x) !!!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746aa183d90fab3",
   "metadata": {},
   "source": [
    "## Training a Neural Network by Hand - A 'Simple' Example\n",
    "\n",
    "Now, we will implement the same model that we created in Python, but this time 'by-hand' in Numpy. To do this, we will have  to calculate the derivatives of the parameters with respect to the loss function, in order to update them during training. \n",
    "First, we will review a simplified version of the model that contains only weights, but no bias or nonlinear functions.\n",
    "\n",
    "### The Model\n",
    "\n",
    "Let $l^1$, $l^2$ and $l^3$ be vector-valued functions representing the layers $1$, $2$, $3$ of the network. $l^1$ takes our length $784$ row vector $\\mathbf{x}$ and returns a length $512$ vector. $l^2$ takes this length $512$ vector as input and returns another length $512$ veector. Our final layer $l^3$ takes this a length $512$ vector as input and outputs a length $10$ vector. \n",
    "\\begin{aligned}\n",
    "    &l^1(\\mathbf{x}) = \\mathbf{x}W^1 \\ \\ \\ \\ \\text{(1, 784) x (784, 512) = (1, 512)} \\\\\n",
    "    &l^2(\\mathbf{l^1}) = \\mathbf{l^1}W^2 \\ \\ \\ \\ \\text{(1, 512) x (512, 512) = (1, 512)} \\\\\n",
    "    &l^3(\\mathbf{l^2}) = \\mathbf{l^2}W^3 \\ \\ \\ \\ \\text{(1, 512) x (512, 10) = (1, 10)} \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "All vectors are row vectors. The $W$ are our matrices of weights with shape (input, output): $W^1$ is $(10, 512)$,  $W^2$ is $(512, 512)$ and $W^3$ is $(512, 10)$. We can therefore index individual weights as $W^3_{i, o}$ where $i$ is the index of the input unit (i.e. the unit the weight connects from), and $o$ is the index of the output unit (i.e. the unit the weight connects to). \n",
    "\n",
    "You can see by **writing out the matrix multiplication** that this operation matches the information flow through the network.\n",
    "\n",
    "[IMAGE]\n",
    "\n",
    "*(An aside)*: This notation a little non-standard (often vectors are column vectors, and the weight matrix is shape (output, input) but this notation makes the derivations below much simpler. Also the layers are functions $l^3(\\cdot)$ but can be treated as vectors $\\mathbf{l^3}$ when evaluated. We will typically write them as vectors.\n",
    "\n",
    "### The Loss\n",
    "\n",
    "**TODO**: add, define, use the word 'score'\n",
    "\n",
    "We take the $(1, 10)$ shape vector of logits (representing score for each label) output from the final layer, and use these to compute our loss. We will use the cross-entropy loss:\n",
    "\n",
    "$$\n",
    "L(\\mathbf{l^3}, y) = -\\log \\dfrac{ \\exp{ l^3_y }}{ \\sum_k \\exp{ l^3_k }}\n",
    "$$\n",
    "\n",
    "Here $l^3_k$ is the element of our vector of logits with some index $k$, and $y$ is the index of the correct label for this image. You may recognise the term after the $\\log$ as the [softmax function](https://en.wikipedia.org/wiki/Softmax_function#:~:text=The%20softmax%20function%2C%20also%20known,used%20in%20multinomial%20logistic%20regression) which normalises the logits to probabilities. Therefore, we are computing the probability our model assigns the input image $\\mathbf{x}$ being (the correct) label $y$. Clearly, we want this probability to be high ($1$, ideally). [This article](https://cs231n.github.io/linear-classify/) as a nice, deeper discussion of the Cross Entropy Loss.\n",
    "\n",
    "So this loss makes intuitive sense. We have an image $\\mathbf{x}$ and are computing a set of probabilities, one for each of the $10$ labels. We are aiming to maximise the probability associated with the correct label, $y$, as we know in this supervised learning context that this is the actual label for this image. Here all we are doing on top is (equivalently) minimising the negative log probability.\n",
    "\n",
    "\n",
    "### Predicting a label from an image - the forward pass\n",
    "\n",
    "For this simple network, it is reliatively easy to use our model to i) take an image $\\mathbf{x}$ ii)  map it to our output of 10 logits iii) use these to predict a label $\\hat{y}$. First we apply the network to the input data:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\mathbf{l^3} &= l^3(l^2(l^1(\\mathbf{x}))) \\\\\n",
    "             &= (((\\mathbf{x}W^1) W^2) W^3 \\\\\n",
    "             &=  \\mathbf{x}W^1 W^2 W^3 \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "We then take this output, and the predicted label is the one that maximises the probability as copmuted by the softmax function:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_{y} \\, \\mathrm{softmax}(\\mathbf{l^3})_y\n",
    "$$\n",
    "\n",
    "In other words, the predicted label is the one the one the model assigns the highest probability to. The notation can be confusing at first look, but we take advantage of that fact that the labels are defined as indices—so we can use the correct label $y$ to index out the corresponding entry in the vector of probaiblities.\n",
    "\n",
    "### Backpropagation in our simple network\n",
    "\n",
    "We want to find the set of weights that minimise the loss function. To do this, we will traverse this function by travelling along it's negative slope. Mathematically, this means we need to compute the derivative of our loss function with respect to the weights.\n",
    "\n",
    "For example, take $W^3_{1,2}$ that connects the first neuron from layer 2 to the second neuron in layer 3. How does a small change in this weight change the output of our loss function? In this case, a small change in $W^3_{1,2}$ will result in a small change in the second neuron in layer 3 ($l^3_2$) which will directly affect the loss function $L(\\mathbf{l^3})$. This 'chain' of dependencies is capture by chain rule. Here we use [partial derivatives](https://en.wikipedia.org/wiki/Partial_derivative) because we only care about how a function that takes mutliple inputs changes with respect to a single one of those inputs.\n",
    "\n",
    "\\begin{aligned}\n",
    "&\\dfrac{\\partial L(\\mathbf{l^3})}{\\partial W^3_{1,2}} = \\dfrac{\\partial L(\\mathbf{l^3})}{\\partial l^3_2}  \\dfrac{\\partial l^3_2 }{\\partial W^3_{1,2}}\n",
    "\\end{aligned}\n",
    "\n",
    "Often, we will write equations that collect all weights for a layer and their affect on all neurons in a layer. Also, from now on, we will write the loss as $L$ instead of $L(\\mathbf{l^3})$ just for brevity, but it is useful to remember it a function of out output layer. Together:\n",
    "\n",
    "\\begin{aligned}\n",
    "&\\dfrac{\\partial L}{\\partial W^3} = \\dfrac{\\partial L}{\\partial \\mathbf{l^3}}   \\dfrac{\\partial \\mathbf{l^3} }{\\partial W^3} \n",
    "\\end{aligned}\n",
    "\n",
    "and for the other weight matrices:\n",
    "\n",
    "\\begin{aligned}\n",
    "&\\dfrac{\\partial L}{\\partial W^2} = \\dfrac{\\partial L}{\\partial \\mathbf{l^3}}  \\dfrac{\\partial \\mathbf{l^3}}{\\partial \\mathbf{l^2}}  \\dfrac{\\partial \\mathbf{l^2} }{\\partial W^3}  \\\\\n",
    "&\\dfrac{\\partial L}{\\partial W^1} = \\dfrac{\\partial L}{\\partial \\mathbf{l^3}}  \\dfrac{\\partial \\mathbf{l^3}}{\\partial \\mathbf{l^2}}  \\dfrac{\\partial \\mathbf{l^2}}{\\partial \\mathbf{l^1}}  \\dfrac{\\partial \\mathbf{l^1} }{\\partial W^1}\n",
    "\\end{aligned}\n",
    "\n",
    "TODO: CHECK AGAINST https://www.jasonosajima.com/backprop.html\n",
    "\n",
    "e.g. for how the loss changes with a change in weights in layer 2, we look at: i) how changing the weights in layer 2 affects the output layer 2 ii) how a change in layer 2 affects the output of layer 3 ii) how a change in layer 3 affects the loss.\n",
    "\n",
    "#### Understanding each term with [matrix calculus](https://en.wikipedia.org/wiki/Matrix_calculus)\n",
    "\n",
    "This notation is doing a lot of heavy lifting and hiding complexity. Take $\\dfrac{\\partial L}{\\partial W^2}$, $L$ is a scalar-valued function (it takes as an input a vector of length 10, our output of layer 3, and returns a scalar). We want to take the deriative of this with respect to a matrix—how each element of $W^3_{i,o}$ affects the loss. Therefore this derivative will be a $(512, 10)$  matrix (the size of $W^3$). \n",
    "\n",
    "How about $\\dfrac{\\partial \\mathbf{l^2}}{\\partial \\mathbf{l^3}}$? This is the derivative of a vector-valued function (it outputs a vector, of length $512$) with repsect to a vector! This is the Jacobian, which tracks how element $l^2$ affects each element in  $l^3$. As layer 3 has $10$ neurons and layer 2 has $512$, we will have a $(10, 512)$ matrix of partial derivatives.\n",
    "\n",
    "$\\dfrac{\\partial \\mathbf{l^2} }{\\partial W^3}$ asks how a small change in each weight in $W^2$ will affect each dimension in $\\mathbf{l^2}$. We will have a $(512, 512, 512)$, rank-3 tensor!\n",
    "\n",
    "#### Computing the derivatives with respect to $W^3$\n",
    "\n",
    "Fortunately the structure of our network means many of these partial derivatives are zero, and we can simply this a lot. Below, we go through each of these in turn, before implementing this network in Python. First we will look in detail at computing the derivative of the loss with respect to the weights connected to the final layer:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial W^3} = \\dfrac{\\partial L}{\\partial \\mathbf{l^3}}   \\dfrac{\\partial \\mathbf{l^3} }{\\partial W^3} \n",
    "$$\n",
    "\n",
    "##### $\\dfrac{\\partial L}{\\partial W^3}$\n",
    "\n",
    "This is the exact derivative we want to compute to update the weights for the third layer, $W^3$. It will be a $(512, 10)$ matrix, with each element being how a small change in that weight affects the loss e.g. (we drop the superscript for brevity):\n",
    "\n",
    "$$\n",
    "    \\dfrac{\\partial L }{\\partial W} =\n",
    "    \\begin{bmatrix}\n",
    "        \\dfrac{ \\partial L }{ \\partial W_{1,1} } & \\dfrac{ \\partial L }{ \\partial W_{1,2} } & ... & \\dfrac{ \\partial L }{ \\partial W_{1,10} }\\\\\n",
    "        \\dfrac{ \\partial L }{ \\partial W_{2,1} } & \\ddots & & \\vdots \\\\\n",
    "        \\vdots \\\\\n",
    "        \\dfrac{ \\partial L }{ \\partial W_{512,1} } & \\dots & &   \\dfrac{ \\partial L }{ \\partial W_{512,10} }\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Next we will explore how to compute the elements of this matrix.\n",
    "\n",
    "##### $\\dfrac{\\partial L}{\\partial \\mathbf{l^3}}$\n",
    "\n",
    "This is the derivative of a scalar-valued function with respect to a vector. In our case, it will be a 10-dimension vector of partial derivatives, where each entry captures the loss changes with respect to a change in that unit of layer 3. \n",
    "\n",
    "We can evaluate the derivative of the cross entropy loss directly:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial }{\\partial l^3_k} - \\log \\dfrac{e^{l^3_y}}{\\sum_k e^{l^3_k}} = \\text{softmax}(l^3_k) - \\delta_{k, y}\n",
    "$$\n",
    "\n",
    " \n",
    "Where $\\delta$ is the [Kronecker delta](https://en.wikipedia.org/wiki/Kronecker_delta) that equals $1$ when $k=y$ and $0$ otherwise. See **Appendix 1** for the derivation. In full this looks like:\n",
    "\n",
    "$$\n",
    "\\dfrac{ \\partial L}{ \\partial \\mathbf{l^3} } =\n",
    "\\begin{bmatrix}\n",
    "    \\dfrac{\\partial L}{\\partial l^3_1} \\\\\n",
    "    \\dfrac{\\partial L}{\\partial l^3_2} \\\\\n",
    "    \\vdots\\\\\n",
    "    \\dfrac{\\partial L}{\\partial l^3_{10}}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    \\text{softmax}(l^3_1) - \\delta_{1, y} \\\\\n",
    "    \\text{softmax}(l^3_2) - \\delta_{2, y} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\text{softmax}(l^3_{10}) - \\delta_{10, y}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Together, this says that our vector of derivatives is $\\text{softmax}(l^3_i)$ for all output logits in layer 3 that are for the incorrect labels, and $\\text{softmax}(l^3_i) - 1$ for the index of the true label $y$. \n",
    "\n",
    "\n",
    "##### $\\dfrac{\\partial \\mathbf{l^3} }{\\partial W^3}$\n",
    "\n",
    "As mentioned above, this is a 3-rank tensor with the shape $(512, 10, 10)$ (how each element of $l^3_n$ changes with each weight $W^3_{i,o}$). How can we even deal with this in our computation?\n",
    "\n",
    "Luckily, the structure of our layered neural network means that this can be significantly reduced, as most of the elements are zero. The strategy is to break the problem down to look at individual elements of $\\dfrac{\\partial L}{\\partial W^3_{i,o}}$ and fill in each term-wise using the total derivative rule (**Appendix 2**):\n",
    "\n",
    "\\begin{align}\n",
    "\\dfrac{\\partial L}{\\partial W^3_{i,o}}\n",
    "&= \\dfrac{\\partial L}{\\partial l^3} \\dfrac{\\partial l^3}{\\partial W^3_{i,o}}  \\\\\n",
    "&= \\sum_k \\dfrac{\\partial L}{\\partial l^3_k} \\dfrac{\\partial l^3_k}{\\partial W^3_{i,o}} \\\\\n",
    "&= \\dfrac{\\partial L}{\\partial l^3_o} \\dfrac{\\partial l^3_o}{\\partial W^3_{i,o}} \\\\\n",
    "&= \\dfrac{\\partial L}{\\partial l^3_o} l^2_i\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Obviously, the notation can get a little tricky to follow here. In words, to find out how the loss chanegs with respect to a single weight $W^3_{i,o}$ we will see how changing this weight changes layer 3, and then how changing layer 3 changes the loss. To compute this, we will take the sum over how each weight $W^3_{i,o}$ affects the unit $k$ in layer 3, $l^3_k$, then how a change in this unit affects the loss. This weight only connects to one unit, $o$ and so the effect of changing that weight on all other units is zero. \n",
    "\n",
    "Finally, we can take the usual scalar derivative here $\\dfrac{\\partial l^3_o}{\\partial W^3_{i,o}} = \\dfrac{\\partial \\ l^2_i W^3_{i,o}}{\\partial W^3_{i,o}} = l^2_i$. This makes intuitive sense, the change in $l^3_o$ when we change $W^3_{i,o}$ is exactly the value of the input unit, $l^2_i$.\n",
    "\n",
    "\n",
    "##### Putting this all together: $\\dfrac{\\partial L}{\\partial W^3} = \\dfrac{\\partial L}{\\partial \\mathbf{l^3}} \\dfrac{\\partial \\mathbf{l^3} }{\\partial W^3}$\n",
    "\n",
    "Now we can put the element-wise approach and all derivatives computed above together in one matrix. All the weights are all $W^3$ matrix and we omit the superscript for brevity:\n",
    "\n",
    "$$\n",
    "\\dfrac{ \\partial L}{ \\partial W^3} =\n",
    "\\begin{bmatrix}\n",
    "\\dfrac{\\partial L}{ \\partial l^3_1 } \\dfrac{\\partial l^3_1 }{ \\partial W_{1,1} } \n",
    "& \\dfrac{\\partial L}{ \\partial l^3_2 } \\dfrac{\\partial l^3_2 }{ \\partial W_{1,2} } \n",
    "& \\dots \n",
    "& \\dfrac{\\partial L}{ \\partial l^3_{10} } \\dfrac{\\partial l^3_{10} }{ \\partial W_{1, 10} }  \\\\\n",
    "\\dfrac{\\partial L}{ \\partial l^3_1 } \\dfrac{\\partial l^3_1 }{ \\partial W_{2,1} } \n",
    "& \\ddots \\\\\n",
    "\\vdots \\\\\n",
    "\\dfrac{\\partial L}{ \\partial l^3_1 } \\dfrac{\\partial l^3_1 }{ \\partial W_{512,1} } \n",
    "& \\dots & \n",
    "& \\dfrac{\\partial L}{ \\partial l^3_{10} } \\dfrac{\\partial l^3_{10} }{ \\partial W_{512,10} }\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "[\\text{sm}(l^3_1) - \\delta_{1, y}]l^2_1 & \n",
    "[\\text{sm}(l^3_2) - \\delta_{2, y}] l^2_1\n",
    "& \\dots \n",
    "& [\\text{sm}(l^3_{10}) - \\delta_{10, y}] l^2_1 & \\\\\n",
    "[\\text{sm}(l^3_1) - \\delta_{1, y}] l^2_2 & \\ddots & & \\vdots \\\\\n",
    "\\vdots \\\\\n",
    "[\\text{sm}(l^3_1) - \\delta_{1, y}] l^2_{512} & \\dots & \n",
    "& [\\text{sm}(l^3_{10}) - \\delta_{10, y}] l^2_{512} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "i.e. for each weight, we see the effect of changing that weight on the layer 3 unit it is connected to, multiplied by the \n",
    "effect of changing that layer 3 unit on the loss.\n",
    "\n",
    "We can write all of this as the product of two vectors (recall all vectors are row vectors here, so the transpose is to a column vector):\n",
    "\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial W^3} = (\\mathbf{l^2})^T \\dfrac{\\partial L}{\\partial \\mathbf{l^3}}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Computing the derivatives with respect to $W^2$\n",
    "\n",
    "For the most part, computing the derivative of the loss with respect to the layer 2 weights uses all of the same ideas and tricks as above. There is one element of added complexity—changing an element $W^2_{i,o}$ affects only one element of layer 2, $l^2_o$. But this layer is connected to *every* unit in layer 3, and we need to account for all of these changes. Luckily, we can iteratively compute the first terms to simpify them, so when we are ready to deal with the weights we don't need to think about all of this complexity. Recall we are trying to compute:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial W^2} = \\dfrac{\\partial L}{\\partial \\mathbf{l^3}}  \\dfrac{\\partial \\mathbf{l^3}}{\\partial \\mathbf{l^2}}  \\dfrac{\\partial \\mathbf{l^2} }{\\partial W^3}\n",
    "$$\n",
    "\n",
    "We have seen similar terms to all of these above, except for the middle term on the right hand side, the Jacobian.\n",
    "\n",
    "\n",
    "##### $\\dfrac{\\partial \\mathbf{l^2}}{\\partial \\mathbf{l^3}}$\n",
    "\n",
    "This is the Jacobian. It is $(512, 10)$ and contains the derivative of each element in $\\mathbf{l^2}$ with respect to each element in $\\mathbf{l^3}$:\n",
    "\n",
    "TODO: CHECK WHAT WAY AROUND THIS SHOULD BE, ALSO CHECK DIMS OF ABOUVE ARE (512, 512, 10) AND IT MATCHES THIS\n",
    "\n",
    "\\begin{aligned}\n",
    "\\dfrac{\\partial \\mathbf{l^3}}{\\partial \\mathbf{l^2}} = \n",
    "\\begin{bmatrix}\n",
    "    \\dfrac{\\partial l^3_1}{\\partial l^2_1} & \\dfrac{\\partial l^3_2}{\\partial l^2_1} & \\dots & \\dfrac{\\partial l^3_{10}}{\\partial l^2_1} \\\\\n",
    "    \\dfrac{\\partial l^3_1}{\\partial l^2_2} & \\ddots & & \\vdots \\\\\n",
    "    \\vdots & & \\\\\n",
    "    \\dfrac{\\partial l^3_1}{\\partial l^2_{512}} & \\dots & & \\dfrac{\\partial l^3_{10}}{\\partial l^2_{512}} \n",
    "\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "\n",
    "If we try and calculate the right hand side of the above equation, we are trying to multiply a $(512,10)$ matrix $\\dfrac{\\partial \\mathbf{l^3}}{\\partial \\mathbf{l^2}}$ with a $(510, 510, 10)$ 3-rank tensor $\\dfrac{\\partial \\mathbf{l^2} }{\\partial W^3}$. Luckily, we will can go from the left and collapse the Jacobian, very similar to how we collapsed the weight matrix above. As long as we compute derivatives with respect to the scalar loss, they will keep the same of the vector or matrix we are taking the derivative with respect to.\n",
    "\n",
    "##### Putting the Layer 2 weights calculation together: \n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial W^2} = \\dfrac{\\partial L}{\\partial \\mathbf{l^3}}  \\dfrac{\\partial \\mathbf{l^3}}{\\partial \\mathbf{l^2}}  \\dfrac{\\partial \\mathbf{l^2} }{\\partial W^3}\n",
    "$$\n",
    "\n",
    "First, we will calculate $\\dfrac{\\partial L}{\\partial \\mathbf{l^3}}  \\dfrac{\\partial \\mathbf{l^3}}{\\partial \\mathbf{l^2}}$, that we know will be a length $512$ vector, using the total derivative rule as above:\n",
    "\n",
    "$$\n",
    "\\dfrac{ \\partial L }{ \\partial l^2_i } = \\sum_o \\dfrac{ \\partial L }{ \\partial l^3_o } \\dfrac{\\partial l^3_o }{ \\partial l^2_i }\n",
    "$$\n",
    "\n",
    "\n",
    "Note this is slightly more complex than the $W^3$ case. Because every  unit in layer 2 is connected to every unit in layer 3, the derivatives don't go to zero. We will actually need to compute every derivative. However, we can see each term in the sum only requires application of scalar derivative rules, e.g.:\n",
    "\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial l^3_o }{ \\partial l^2_i } = \\dfrac{\\partial l^2_i W^3_{i, o} }{ \\partial l^2_i } = W^3_{i, o}\n",
    "$$\n",
    " XXX\n",
    "\n",
    "So together, the full vector of partial derivatives is:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\begin{bmatrix}\n",
    "    \\sum_o \\dfrac{ \\partial L }{ \\partial l^3_o } \\dfrac{\\partial l^3_o }{ \\partial l^2_1 } &\n",
    "    \\sum_o \\dfrac{ \\partial L }{ \\partial l^3_o } \\dfrac{\\partial l^3_o }{ \\partial l^2_2 } &\n",
    "    \\cdots &\n",
    "    \\sum_o \\dfrac{ \\partial L }{ \\partial l^3_o } \\dfrac{\\partial l^3_o }{ \\partial l^2_{512} }\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "\n",
    "And so we can compute this with the matrix vector calculation:\n",
    "\n",
    "$$\n",
    "\\dfrac{ \\partial L }{ \\partial \\mathbf{l^2} } = \\dfrac{ \\partial L}{ \\partial \\mathbf{l^3}} (W^3)^T\n",
    "$$ \n",
    "\n",
    "\n",
    "which is a $(1, 10)$ vector multiplying a $(10, 512)$ matrix. \n",
    "\n",
    "Now, we can finally compute $\\dfrac{ \\partial L}{\\partial W^2} = \\dfrac{\\partial L}{\\partial \\mathbf{l^2} } \\dfrac{\\partial \\mathbf{l^2}}{\\partial W^2}$ which we can do again with the total derivaive! And we do it in exactly the same way we computed $\\dfrac{ \\partial L}{\\partial W^3} = \\dfrac{\\partial L}{\\partial \\mathbf{l^3} } \\dfrac{\\partial \\mathbf{l^3}}{\\partial W^3}$ above. In this case it is the outer product of the vectors:\n",
    "\n",
    "\n",
    "$$\n",
    "\\dfrac{ \\partial L }{ \\partial W^2 } = (\\mathbf{l^1})^T \\dfrac{ \\partial L }{ \\partial \\mathbf{l^2} } \n",
    "$$\n",
    "\n",
    "\n",
    "i.e. the outer product of a $(512, 1)$ vector by a $(1, 512)$ vector to give us a $(512, 512)$ matrix of derivatives. For example, the first row of $\\dfrac{ \\partial L }{ \\partial W^2 }$ is how the loss changes with respect to a change in the weights that connect layer 1 unit $1$ to every unit in layer 2 unit. A change in the weight will change layer 2 unit $o$ by the value of layer 1 unit $1$ - this is the same for every weight in that row. This then is multiplied by how a small change in that unit $o$ in layer 2 effects the loss.\n",
    "\n",
    "**It's worth reflecting on this. We have such complexity here. And it reduces really nicely.**\n",
    "\n",
    "##### Computing the partial derivatives of $W^1$\n",
    "\n",
    "Since we have done through all of the hard work of really inspecting what is going on under the hood, we can really simply the notation going forward. This compact notation really highlights the 'chain' aspect of the chain rule:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial W^1} = \\dfrac{\\partial L}{\\partial \\mathbf{l^3}}  \\dfrac{\\partial \\mathbf{l^3}}{\\partial \\mathbf{l^2}}  \\dfrac{\\partial \\mathbf{l^2}}{\\partial \\mathbf{l^1}}  \\dfrac{\\partial \\mathbf{l^1} }{\\partial W^1}\n",
    "$$\n",
    "\n",
    "We are using the total derivative rule to condense all intermediate calculations, just like above:\n",
    "\n",
    "First:\n",
    "\n",
    "$$\n",
    "\\mathbf{g_1} = \\dfrac{\\partial L}{\\partial \\mathbf{l^2}} = \\dfrac{\\partial L}{\\partial \\mathbf{l^3}}  \\dfrac{\\partial \\mathbf{l^3}}{\\partial \\mathbf{l^2}} = \\dfrac{\\partial L}{\\partial \\mathbf{l^3}} (W^3)^T\n",
    "$$\n",
    "\n",
    "$(1, 512) = (1, 10) \\times (10, 512)$\n",
    "\n",
    "$$\n",
    "\\mathbf{g_2} = \\dfrac{\\partial L}{\\partial \\mathbf{l^1}} =  \\dfrac{\\partial L}{\\partial \\mathbf{l^2}} \\dfrac{\\partial \\mathbf{l^2}}{\\partial \\mathbf{l^1}} = \\mathbf{g_1}(W^2)^T\n",
    "$$\n",
    "\n",
    "$(1, 512) = (1, 512) \\times (512, 512)$ \n",
    "\n",
    "Finally we have the outer product calculation:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial W^1} = \\dfrac{\\partial L}{\\partial \\mathbf{l^1}} \\dfrac{\\partial \\mathbf{l^1}}{\\partial W^1} = (\\mathbf{x})^T \\mathbf{g_2}\n",
    "$$\n",
    "\n",
    "$(512, 512) = (512, 1) \\times (1, 512)$\n",
    "\n",
    "It's awesome to see how changing the derivatives, backwards from the final layer, greatly simplifies computing the derivatives for our complex, fully connected network.\n",
    "\n",
    "It is also very easy to compte, because we already have our weight vectors and we compute the output of each layer as part of our forward pass!\n",
    "\n",
    "## The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e907e85b1133c4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T20:17:12.207717Z",
     "start_time": "2025-12-09T20:17:12.202352Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "run = False\n",
    "\n",
    "class MyBasicNetwork:\n",
    "    def __init__(self, learning_rate=0.02):\n",
    "\n",
    "        self.a = learning_rate\n",
    "\n",
    "        # Define weight matrix (output dim, input dim) by convention\n",
    "        # Use zero-mean Xavier init (good for sigmoid, it has little\n",
    "        # effect here as we don't use activation functions,\n",
    "        # but useful for comparison.)\n",
    "        self.W1 = np.random.randn(784, 512) * np.sqrt(1 / 784)\n",
    "        self.W2 = np.random.randn(512, 512) * np.sqrt(1 / 512)\n",
    "        self.W3 = np.random.randn(512, 10) * np.sqrt(1 / 512)\n",
    "\n",
    "    def loss(self, l3, y):\n",
    "        p = self.softmax(l3)[0][y]\n",
    "        return -np.log( p + 1e-15)\n",
    "\n",
    "    def softmax(self, vec):\n",
    "        C = np.max(vec)\n",
    "        return np.exp(vec - C) / np.sum(np.exp(vec - C))\n",
    "\n",
    "    def predict(self, x):\n",
    "        # forward pass through the network\n",
    "        x = x.reshape(1, x.size)\n",
    "\n",
    "        l1 = x @ self.W1\n",
    "        l2 = l1 @ self.W2\n",
    "        l3 = l2 @ self.W3\n",
    "\n",
    "        pred = np.argmax(self.softmax(l3))\n",
    "\n",
    "        return pred, l1, l2, l3\n",
    "\n",
    "    def update_weights(self, x, y, verbose=False):\n",
    "\n",
    "        _, l1, l2, l3 = self.predict(x)\n",
    "\n",
    "        loss = self.loss(l3, y)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Loss: {loss}\")\n",
    "\n",
    "        # Compute the derivatives\n",
    "        dloss_dl3 = self.softmax(l3)  # double check this\n",
    "        dloss_dl3[0][y] -= 1\n",
    "\n",
    "        dloss_dW3 = l2.T @ dloss_dl3       # (512, 10) = (512, 1) x (1, 10)\n",
    "\n",
    "        dloss_dl2 = dloss_dl3 @ self.W3.T  # (1, 512) = (1, 10) x (10, 512)\n",
    "        dloss_dW2 = l1.T @ dloss_dl2       # (512, 512) = (512, 1) x (1, 512)\n",
    "\n",
    "        dloss_dl1 = dloss_dl2 @ self.W2.T  # (1, 512) = (1, 512) x (512, 512)\n",
    "        dloss_dW1 = x.T @ dloss_dl1        # (784, 512) = (781, 1) x (1, 512)\n",
    "\n",
    "        self.W3 -= self.a * dloss_dW3\n",
    "        self.W2 -= self.a * dloss_dW2\n",
    "        self.W1 -= self.a * dloss_dW1\n",
    "\n",
    "# We won't run this here because it is very slow,\n",
    "# but it gives an accuracy of ~73%\n",
    "if run:\n",
    "    # Initialise and train the model (no batching)\n",
    "    model = MyBasicNetwork()\n",
    "\n",
    "\n",
    "    for i, (X, y) in enumerate(training_data):\n",
    "\n",
    "        x = np.asarray(X[0, :, :])\n",
    "        y = int(y)\n",
    "\n",
    "        model.update_weights(x, y)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Training iteration: sample: {i}\")\n",
    "\n",
    "    # Check the model accuracy\n",
    "    results = np.empty(len(test_data))\n",
    "\n",
    "    for i, (X, y) in enumerate(test_data):\n",
    "\n",
    "        x = np.asarray(X[0, :, :])\n",
    "\n",
    "        results[i] = model.predict(x)[0] == y\n",
    "\n",
    "    print(f\"Percent Correct: {np.mean(results) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b35543",
   "metadata": {},
   "source": [
    "## A Better Model\n",
    "\n",
    "We will add some simple changes to improve the performance of the model. In our simple version about, we essentially had $\\mathbf{l^3} = \\mathbf{x}W$ where $W$ is a $(784, 10)$ matrix. One way to intepret this is a matrix encoding 10 linear hyperplanes in a $784$-dimension space that separate points according to their label.\n",
    "\n",
    "A simple change we will make is to include a bias term $\\mathbf{b}$. We can interpret this as adding an offset to each hyperplane, meaning our data does not need to be centered at zero.\n",
    "\n",
    "More improtantly, we will also add an activation function around each unit. Once the activation at each unit is computed, we run it through a nonlinear function. Immediately, this means we loose the 'linear hyperplane' interpretation. This change now means the SEP BOUNDARY between groups do not need to be linear hyperplanes, but can be much more flexible and bendy. wNow, we will introduce an activation function which will make our network nonlinear. **LINK TO CS PAGE.**\n",
    "\n",
    "Our network is now:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    l^1 &= \\phi(\\mathbf{x}W^1 + \\mathbf{b^1}) \\\\\n",
    "    l^2 &= \\phi(\\mathbf{l^1}W^2 + \\mathbf{b^2}) \\\\\n",
    "    l^3 &= \\mathbf{l^2}W^3 + \\mathbf{b^3} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\phi(\\cdot)$ is our nonlinear activation function. $\\mathbf{b^i}$ is a $(1, n)$ vector of biases, one for each of $n$ units layer $i$. A note on the notation, the output of $\\mathbf{l^{n-1}}W^n+\\mathbf{b^n}$ in each layer is still a vector (e.g. $(1, 10)$) and we apply the nonlinear activation function element-wise to this vector.\n",
    "\n",
    "Note we use the sigmoid function because it has interesting derivaives, but in general ReLu is preferred in larger, modern networks due to the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem). Note we don't apply the sigmoid function to the last layer, as we feed this directly to the cross-entropy loss which is already doing a mapping with the softmax function.\n",
    "\n",
    "Now, we will take the derivatives of the loss with respect to the weights *and* biases. \n",
    "\n",
    "### Computing the derivatives\n",
    "\n",
    "#### Derivatives of the sigmoid activation function\n",
    "\n",
    "The sigmoid function maps the interval $(-\\infty, \\infty)$ to $(0, 1)$. Some intuition. \n",
    "\n",
    "$$\n",
    "\\phi(z(t)) = \\dfrac{1}{1 + e^{-z(t)}}\n",
    "$$\n",
    "\n",
    "Let $z$ is some arbitrary function of the variable $t$. Then the derivative by the chain rule is:\n",
    "\n",
    "$$\n",
    "\\dfrac{d}{dt} (1 + e^{-z(t)})^{-1} =  \\dfrac{ e^{-z(t)} }{ (1 + e^{-z(t)})^2 } \\dfrac{d}{dt} z(t)\n",
    "$$\n",
    "\n",
    "In our case, this will just include computing these extra terms of our layers next tot he derivative.\n",
    "\n",
    "#### $W^3$ and $\\mathbf{b^3}$\n",
    "\n",
    "As we don't apply the sigmoid to the last layer, this is exactly the same as the simple example:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial W^3} = (\\mathbf{l^2})^T \\dfrac{\\partial L}{\\partial \\mathbf{l^3}}\n",
    "$$\n",
    "\n",
    "Note that the derivative of layer 3 with respect to the bias, $l^3_o = l^2_i W^3_{i, o} + b^3_o$ is $1$. This makes intuitive sense, when we make a small $\\delta b^3_o$ change, it just changes the output of layer 3 exactly by this small change $\\delta b^3_o$, because it is simply added to the output. Therefore:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial \\mathbf{b^3}} = \\dfrac{\\partial L}{\\partial \\mathbf{l^3}}\n",
    "$$\n",
    "\n",
    "\n",
    "#### $W^2$ and $\\mathbf{b^2}$\n",
    "\n",
    "Now things get tasty. Unfortunately, the notation gets even more complex, the easiest way to see what is going on is to follow along with a pen and paper, writing out each steps and converting to matrices as you go.\n",
    "\n",
    "Recall, the same as above, a change in the loss with a change in the weights of layer 2 is:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial W^2} = \\dfrac{\\partial L}{\\partial \\mathbf{l^3}}  \\dfrac{\\partial \\mathbf{l^3}}{\\partial \\mathbf{l^2}}  \\dfrac{\\partial \\mathbf{l^2} }{\\partial W^2}\n",
    "$$\n",
    "\n",
    "\n",
    "##### $\\dfrac{ \\partial L }{ \\partial \\mathbf{l^2}}$\n",
    "\n",
    "Because layer 3 does not have an activation function (and the derivative of the new bias term goes to zero) this is identical to the simple case above:\n",
    "\n",
    "$$\n",
    "\\dfrac{ \\partial L }{ \\partial \\mathbf{l^2}} = \\dfrac{ \\partial L }{ \\partial \\mathbf{l^3}} (W^3)^T\n",
    "$$\n",
    "\n",
    "\n",
    "##### $\\dfrac{ \\partial L }{ \\partial W^2 }$\n",
    "\n",
    "Now we are ready to compute the derivative of the loss with respect to the weight matrix of layer 2, and the bias vector. First starting with the weights, looking at each weight in isolation:\n",
    "\n",
    "$$\n",
    "\\dfrac{ \\partial L }{\\partial W^2_{i, o}} = \\dfrac{ \\partial L }{ \\partial l^2_o } \\dfrac{ \\partial l^2_o }{ \\partial W^2_{i, o}}\n",
    "$$\n",
    "\n",
    "We calculated the first term above, so looking at the second term:\n",
    "\n",
    "$$\n",
    "\\dfrac{ \\partial l^2_o }{ \\partial W^2_{i, o}} = \\dfrac{ \\partial }{ \\partial W^2_{i, o}} \\phi( \\sum_n l^1_n W^2_{n, o} + b^2_o)\n",
    "$$\n",
    "\n",
    "Recalling the derivative of the term inside $\\phi$ with respect to the weight $W^2_{i,o}$ is $l^1_i$ as it is $0$ when $n \\neq i$. Letting $\\hat{l^2_o} = \\sum_i l^1_i W^2_{i, o} + b^2_o$:\n",
    "\n",
    "$$\n",
    "\\dfrac{ \\partial l^2_o }{ \\partial W^2_{i,o}} = \\dfrac{ e^{-\\hat{l^2_o}} }{ (1 + e^{-\\hat{l^2_o}})^2 } \\ l^1_i\n",
    "$$\n",
    "\n",
    "We can implement this in matrix form. $\\hat{l^2}$ is a $(1, 10)$ vector and so the element-wise multiplication is what we need here (officilally called the [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) with notation $\\circ$. \n",
    "\n",
    "So in fact, the derivative looks very similar to before we had the activation function, except now we have this extra term related to its derivative:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\dfrac{ \\partial L }{\\partial W^2} = (\\mathbf{l^1})^T   \\bigg(  \\dfrac{ e^{-\\mathbf{\\hat{l^2}}} }{ (1 + e^{-\\mathbf{\\hat{l^2}}})^2 }   \\circ \\dfrac{ \\partial L }{ \\partial \\mathbf{l^2} } \\bigg)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(512, 512) = (512, 1) \\times (1, 512) \\circ (1, 512)\n",
    "$$\n",
    "\n",
    "and if we take that derivative with respect to $b_o$ then the term goes to $1$ and dissapears:\n",
    "\n",
    "$$\n",
    "    \\dfrac{ \\partial L }{\\partial \\mathbf{b^2}} =  \\dfrac{ e^{-\\mathbf{\\hat{l^2}}} }{ (1 + e^{-\\mathbf{\\hat{l^2}}})^2 } \\circ \\dfrac{ \\partial L }{ \\partial \\mathbf{l^2} }\n",
    "$$\n",
    "\n",
    "\n",
    "#### $W^1$ and $\\mathbf{b^1}$\n",
    "\n",
    "The derivation for layer 1 follows all the same ideas that we explored for layer 2.\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L }{ \\partial W^1} = \\dfrac{\\partial L }{ \\partial \\mathbf{l^3} } \\dfrac{\\partial \\mathbf{l^3} }{ \\partial \\mathbf{l^2} } \\dfrac{\\partial \\mathbf{l^2} }{ \\partial \\mathbf{l^1} } \\dfrac{\\partial \\mathbf{l^1} }{ \\partial \\mathbf{W^1} }\n",
    "$$\n",
    "\n",
    "We just need to compute $\\dfrac{\\partial L }{ \\partial \\mathbf{l^1} }$ again, now we have the nonlinear activation function to deal with.\n",
    "\n",
    "#### $\\dfrac{\\partial L}{ \\partial \\mathbf{l^1} }$ \n",
    "\n",
    "This will be a $(1, 512)$ vector where each element is how the loss changes with a small change in the corresponding unit of layer 1. For each unit in layer 1, we can compute this with the total derivative rule, by looking at the effect of each a small change in a layer 1 unit (input, $i$) on every unit in layer 2 (output, $o$) that it is connected to:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\dfrac{\\partial L}{\\partial l^1_i }  &= \\sum_o \\dfrac{\\partial L}{\\partial l^2_o } \\dfrac{\\partial l^2_o}{\\partial l^1_i } \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and note the output of a single layer 2 unit is:\n",
    "\n",
    "$$\n",
    "l^2_o = \\phi \\left( \\sum_i l^1_i W^2_{i, o} + b^2_o \\right)\n",
    "$$\n",
    "\n",
    "For the derivative of the term inside the activation function, this is exactly the same as the simple case. We take the derivative of $\\sum_i l^1_i W^2_{i, o} + b^2_o$ with respect to a specific input unit $l^1_\\hat{i}$ then all terms are zero except for the case where $i = \\hat{i}$. Therefore, the derivative with respect to $l^1_\\hat{i}$ is $W^2_{\\hat{i}, o}$. \n",
    "\n",
    "Here, we will introduce the notation $\\mathbf{\\hat{l^2}} = \\mathbf{l^1}W^2 + \\mathbf{b^2}$, i.e. $\\mathbf{\\hat{l^2}}$ is the output of layer 2 before we put it through the activation function. \n",
    "\n",
    "Putting this together with the derivative of the activation function, as above:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\dfrac{\\partial L}{\\partial l^1_i } &= \\sum_o \\dfrac{\\partial L}{\\partial l^2_o } \\dfrac{\\partial l^2_o}{\\partial l^1_i }  \\\\\n",
    " &= \\sum_o \\dfrac{\\partial L}{\\partial l^2_o }  \\dfrac{\\partial }{\\partial l^1_i } \\phi({\\hat{l^2_o}} ) \\\\\n",
    "&= \\sum_o \\dfrac{\\partial L}{\\partial l^2_o } \\dfrac{ e^{-\\hat{l^2_o}} }{ (1 + e^{-\\hat{l^2_o}})^2 } W^2_{i, o}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Intuitively, this makes sense. XXX. \n",
    "\n",
    "So we can implement this in vector form as:\n",
    "\n",
    "$$\n",
    "\\dfrac{ \\partial L }{ \\partial \\mathbf{l^1} } = \\bigg( \\dfrac{ \\partial L }{ \\partial \\mathbf{l^2} }  \\circ  \\dfrac{ e^{-\\mathbf{\\hat{l^2}}} }{ (1 + e^{-\\mathbf{\\hat{l^2}}})^2 } \\bigg) (W^2)^T\n",
    "$$\n",
    "\n",
    "The easiest way to see this vector form implementation is to write out all the terms with pen and paper and follow them through. MAKE CLEAR THE EPONENTIAL TERM IS A VECTOR AND WE APPLY THE EXPON INDIVIDUALLY.\n",
    "\n",
    "#### So putting it all together for $W^1$ and $\\mathbf{b^1}$\n",
    "\n",
    "Again, we use $\\hat{\\mathbf{l^1}} = \\mathbf{x}W^1 + \\mathbf{b^1}$ for the output of layer 1 before inputting to the activation function. \n",
    "\n",
    "So as in the simple case, we will go through these piece-by-piece:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\mathbf{g_1} &= \\dfrac{\\partial L }{ \\partial \\mathbf{l^2} } = \\dfrac{\\partial L }{ \\partial \\mathbf{l^3} } \\dfrac{\\partial \\mathbf{l^3} }{ \\partial \\mathbf{l^2} }  \\\\\n",
    "    &=  \\dfrac{ \\partial L }{ \\partial \\mathbf{l^3} } (W^3)^T\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$(1, 512) = (1, 10) \\circ (1, 10) \\times (10, 512)$ \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{g_2} &= \\dfrac{ \\partial L }{ \\partial \\mathbf{l^1 }} \\\\ \n",
    "&= \\mathbf{g_1} \\dfrac{\\partial \\mathbf{l^2} }{ \\partial \\mathbf{l^1} } \\\\\n",
    "&= \\bigg( \\mathbf{g_1} \\circ \\dfrac{ e^{-\\hat{\\mathbf{l^2}} } }{ (1 + e^{-\\hat{\\mathbf{l^2}}})^2 } \\bigg) (W^2)^T\n",
    "\\end{aligned} \n",
    "$$\n",
    "\n",
    "$(1, 512) = (1, 512) \\circ (1, 512) \\times (512, 512)$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\dfrac{\\partial L}{ W^1} &= \\mathbf{g_2} \\dfrac{\\partial \\mathbf{l^1} }{ \\partial W^1 }\\\\\n",
    "&= \\mathbf{x}^T \\bigg( \\dfrac{ e^{-\\hat{\\mathbf{l^1}} } }{ (1 + e^{-\\hat{\\mathbf{l^1}} })^2 } \\circ \\mathbf{g_2} \\bigg)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(784, 512) = (784, 1) \\times (1, 512) * (1, 512)\n",
    "$$\n",
    "\n",
    "When taking the derivative with respect to the bias, all steps are the same except the last step $\\mathbf{x}$ is $1$ (to see this, take the derivative of $l^1_o = x_i W^1_{i, o} + b^1_o$ with respect to $b_0$ or $W^1_{i,o}$):\n",
    "\n",
    "$$\n",
    "\\dfrac{ \\partial L}{ \\partial \\mathbf{b^1}} = \\dfrac{ e^{-\\hat{\\mathbf{l^1}} } }{ (1 + e^{-\\hat{\\mathbf{l^1}} })^2 } \\circ \\mathbf{g_2}\n",
    "$$\n",
    "\n",
    "TODO: add total derivative appendix\n",
    "TODO: add some pictures of neurons\n",
    "TODO: check all writing\n",
    "\n",
    "In sum, thank god for autodiff!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fdbd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run = False\n",
    "\n",
    "class MyBetterNetwork:\n",
    "    def __init__(self, learning_rate=0.02):\n",
    "\n",
    "        self.a = learning_rate\n",
    "\n",
    "        # Define weight matrix (output dim, input dim) by convention\n",
    "        # Use zero-mean Xavier init (good for sigmoid)\n",
    "        # This makes a huge differences vs uniform.\n",
    "        self.W1 = np.random.randn(784, 512) * np.sqrt(1 / 784)\n",
    "        self.W2 = np.random.randn(512, 512) * np.sqrt(1 / 512)\n",
    "        self.W3 = np.random.randn(512, 10) * np.sqrt(1 / 512)\n",
    "\n",
    "        self.b1 = np.zeros((1, 512))\n",
    "        self.b2 = np.zeros((1, 512))\n",
    "        self.b3 = np.zeros((1, 10))\n",
    "\n",
    "    def loss(self, l3, y):\n",
    "        p = self.softmax(l3)[0][y]\n",
    "        return -np.log( p + 1e-15)\n",
    "\n",
    "    def softmax(self, vec):\n",
    "        C = np.max(vec)\n",
    "        return np.exp(vec - C) / np.sum(np.exp(vec - C))\n",
    "\n",
    "    def predict(self, x):\n",
    "        # forward pass through the network\n",
    "        x = x.reshape(1, x.size)\n",
    "\n",
    "        l1_hat = x @ self.W1 + self.b1\n",
    "        l1 = self.phi(l1_hat)\n",
    "\n",
    "        l2_hat = l1 @ self.W2 + self.b2\n",
    "        l2 = self.phi(l2_hat)\n",
    "\n",
    "        l3 = l2 @ self.W3 + self.b3\n",
    "\n",
    "        pred = np.argmax(self.softmax(l3))\n",
    "\n",
    "        return pred, l1_hat, l1, l2_hat, l2, l3\n",
    "\n",
    "    def phi(self, vec):\n",
    "        return 1 / (1 + np.exp(-vec))\n",
    "\n",
    "    def dphi_dvec(self, vec):\n",
    "        return np.exp(-vec) / (1 + np.exp(-vec))**2\n",
    "\n",
    "    def update_weights(self, x, y, verbose=False):\n",
    "\n",
    "        x = x.reshape(1, x.size)\n",
    "\n",
    "        _, l1_hat, l1, l2_hat, l2, l3 = self.predict(x)\n",
    "\n",
    "        loss = self.loss(l3, y)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Loss: {loss}\")\n",
    "\n",
    "        # Compute the derivatives\n",
    "        dloss_dl3 = self.softmax(l3)  # double check this\n",
    "        dloss_dl3[0][y] -= 1\n",
    "\n",
    "        dloss_dW3 = l2.T @ dloss_dl3\n",
    "        dloss_db3 = dloss_dl3\n",
    "\n",
    "        dloss_dl2 = dloss_dl3 @ self.W3.T                               # (1, 512) = (1, 10) x (10, 512)\n",
    "        dloss_dW2 = l1.T @ (self.dphi_dvec(l2_hat) * dloss_dl2)         # (512, 512) = (512, 1) x (1, 512) * (1, 512)\n",
    "        dloss_db2 = self.dphi_dvec(l2_hat) * dloss_dl2                  # (1, 512) = (512, 1) x (1, 512)\n",
    "\n",
    "        dloss_dl1 = (dloss_dl2 * self.dphi_dvec(l2_hat)) @ self.W2.T    # (1, 512) = (1, 512) * (1, 512) x (512, 512)\n",
    "        dloss_dW1 = x.T @ (self.dphi_dvec(l1_hat) * dloss_dl1)          # (784, 512) = (784, 1) x (1, 512) * (1, 512)\n",
    "        dloss_db1 = self.dphi_dvec(l1_hat) * dloss_dl1                  # (1, 512) = (1, 512) * (1, 512)\n",
    "\n",
    "        self.W3 -= self.a * dloss_dW3\n",
    "        self.W2 -= self.a * dloss_dW2\n",
    "        self.W1 -= self.a * dloss_dW1\n",
    "\n",
    "        self.b3 -= self.a * dloss_db3\n",
    "        self.b2 -= self.a * dloss_db2\n",
    "        self.b1 -= self.a * dloss_db1\n",
    "\n",
    "# Initialise and train the model (no batching)\n",
    "model = MyBetterNetwork()\n",
    "\n",
    "for i, (X, y) in enumerate(training_data):\n",
    "\n",
    "    x = np.asarray(X[0, :, :])\n",
    "    y = int(y)\n",
    "\n",
    "    model.update_weights(x, y, verbose=False)\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Training iteration: sample: {i}\")\n",
    "\n",
    "# We won't run this here because it is very slow,\n",
    "# but it gives an accuracy of ~83%\n",
    "# Check the model accuracy\n",
    "if run:\n",
    "    results = np.empty(len(test_data))\n",
    "\n",
    "    for i, (X, y) in enumerate(test_data):\n",
    "\n",
    "        x = np.asarray(X[0, :, :])\n",
    "\n",
    "        results[i] = model.predict(x)[0] == y\n",
    "\n",
    "    print(f\"Percent Correct: {np.mean(results) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02e2a692bfa776c",
   "metadata": {},
   "source": [
    "**Appendix 1**\n",
    "\n",
    "The cross entropy loss is:\n",
    "\n",
    "\\begin{aligned}\n",
    "    L &= -\\log \\dfrac{ \\exp{ l_{3, y} } }{ \\sum_k \\exp{ l_{3, k} } } \\\\\n",
    "    &= -\\bigg[ \\log \\exp{ l_{3, y} } - \\log \\sum_k \\exp{ l_{3, k} } \\bigg] \\\\\n",
    "    &=  \\log \\sum_k \\exp{ l_{3, k} } - l_{3, y}\n",
    "\\end{aligned}\n",
    "\n",
    "(by the log laws). i.e. we take the logit  of layer 3 that matches the correct label $y$, normalise it to a probability\n",
    "with the softmax function and take the negative log.\n",
    "\n",
    "Let's start by taking the derivative with respect to $l_{3, y}$ where this is shorthand for $l_{3, i}$, $i=y$ i.e.\n",
    "the layer 3 logit for the label that is correct for this image. We are asking: how does a small change in this logit effect the loss?\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\dfrac{ \\partial L }{ \\partial l_{3, y} } &= \\dfrac{ \\partial }{ \\partial l_{3, y} } \\left( \\log \\sum_k \\exp{ l_{3, k} } - l_{3, y} \\right)\n",
    "&=  \\dfrac{ \\partial }{ \\partial l_{3, y} } \\log \\sum_k \\exp{ l_{3, k} } - \\dfrac{ \\partial }{ \\partial l_{3, y} }  l_{3, y}  \\\\\n",
    "&= \\dfrac{1}{ \\sum_k \\exp{ l_{3, k} } }  \\dfrac{ \\partial }{ \\partial l_{3, y} } \\sum_k \\exp{ l_{3, k} } - 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "(by the derivative of $\\log x$ rule). Note that the last term will be $0$ when the the input dimension is not $y$ (because it is treated as a scalar).\n",
    "\n",
    "We see that in the sum, the derivative of $\\exp{ l_{3, i} } $ w.r.t $l_{3, k}$ is $\\exp{ l_{3, i} }$ when $i = k$ and $0$ otherwise (as it is treated as a scalar).\n",
    "So whatever dimension $i$ of $l_3$, we will input to the loss, we get $\\text{softmax}(\\mathbf{l_3})_i$ as the first term. But only when $i = y$ do we get $-1$ in the second term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e20936",
   "metadata": {},
   "source": [
    "**Appendix 2**\n",
    "\n",
    "In multivariate calculus, the total derivative rule captures how a function changes with respect to a variable that affects the function in multiple ways (through multiple intermediary functions).\n",
    "\n",
    "For example, let's say we have the variable $t$ and let:\n",
    "\n",
    "$$\n",
    "w = f(x(t), y(t))\n",
    "$$\n",
    "\n",
    "The total derivative rule tells us how the output, $w$ changes with a small change in $t$. Intuitively, it is the sum of how $\\delta t$ changes $w$ through $x(t)$ and how $\\delta t$ changes $w$ through $y(t)$:\n",
    "\n",
    "$$\n",
    "\\dfrac{dw}{dt} = \\dfrac{\\partial w}{ \\partial x(t)} \\dfrac{\\partial x(t)}{ \\partial t} + \\dfrac{\\partial w}{ \\partial y(t)} \\dfrac{\\partial y(t)}{ \\partial t} \n",
    "$$\n",
    "\n",
    "Note this is exactly analogous to our set up with the layers and weights. A small change in a unit in layer 2 will effect the loss through every unit in layer 3. Let's look at unit 1 from layer 2 ($l^2_1$) as an example. Here layer 3 units are a function taking $l^2_1$, which is like $t$ in the above example, as an input:\n",
    "\n",
    "$$\n",
    "\\text{loss} = L( \\ l^3_1(l^2_1), \\ l^3_2(l^2_1), \\ ..., \\ l^3_{10}(l^2_1) \\ )\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{ \\partial l^2_1 } = \\dfrac{\\partial L}{ \\partial l^3_1 } \\dfrac{ \\partial l^3_1}{ \\partial l^2_1 } + \\dfrac{\\partial L}{ \\partial l^3_2 } \\dfrac{\\partial l^3_2 }{ \\partial l^2_1 } + ... + \\dfrac{\\partial L}{ \\partial l^3_{10} } \\dfrac{\\partial l^3_{10} }{ \\partial l^2_1 }\n",
    "$$\n",
    "\n",
    "Which is exactly what we do to deal with these derivatives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
