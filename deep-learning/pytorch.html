<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Introduction to Neural Networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="pytorch_files/libs/clipboard/clipboard.min.js"></script>
<script src="pytorch_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="pytorch_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="pytorch_files/libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="pytorch_files/libs/quarto-html/popper.min.js"></script>
<script src="pytorch_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="pytorch_files/libs/quarto-html/anchor.min.js"></script>
<link href="pytorch_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="pytorch_files/libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="pytorch_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="pytorch_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="pytorch_files/libs/bootstrap/bootstrap-9e3ffae467580fdb927a41352e75a2e0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Introduction to Neural Networks</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Neural networks are a very flexible model through which to learn complex, high dimensional functions. First, we will apply the model in Pytorch, and then implement it by hand, using multivariable calculus to derive the parameter updates.</p>
<p>In this example, we will apply a neural network to an image classification problem. We have a dataset containing images clothes, with each image having a label describing the item of clothing in the image. Each <span class="math inline">\(28 \times 28\)</span>-pixel image is represented as a length <span class="math inline">\(784\)</span> vector <span class="math inline">\(\mathbf{x}\)</span>. We use bold font to indicate a vector. There are 10 possible items of clothing (e.g.&nbsp;t-shirt, hat) in our dataset, so <span class="math inline">\(y\)</span> is an integer in the range <span class="math inline">\([0, 10]\)</span>. Together, our sample space is pairings of images and labels, <span class="math inline">\((\mathbf{x}_i, y_i) \sim (\mathcal{X}, \mathcal{Y})\)</span> (i.e.&nbsp;a single image, label pair index by <span class="math inline">\(i\)</span> can be drawn from the sample space of all pairs of images and labels). s</p>
<p>We want to learn the conditional distribution <span class="math inline">\(p(y| \mathbf{x})\)</span> i.e.&nbsp;what is the probability of the label <span class="math inline">\(y\)</span> given an image, <span class="math inline">\(\mathbf{x}\)</span>. For example, what is the probability this is an image <span class="math inline">\(\mathbf{x}\)</span> is of a t-shirt? In this case, we want to learn a function <span class="math inline">\(f(\mathbf{x}) : \mathbf{x} \rightarrow \mathbf{y}\)</span>. It will take a vector of length <span class="math inline">\(784\)</span> and output a vector of length <span class="math inline">\(10\)</span>, with each element of the output vector assigning some weight related to the probability of image <span class="math inline">\(\mathbf{x}\)</span> being a particular label <span class="math inline">\(y\)</span>. These unnormalised weights output by the model are called ‘logits’.</p>
<p><img src="nn-image-1.png" width="400"></p>
<p>In this example, we will first follow the <a href="https://docs.pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html">Pytorch</a> ‘Getting Started’ page to train a simple model for this classification task. Then, we will get a deeper grip on how these models work by implementing the model ourselves in Numpy, which requires calculating the derivatives required for model training ourselves.</p>
<section id="coding-up-a-neural-network-in-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="coding-up-a-neural-network-in-pytorch">Coding up a Neural Network in Pytorch</h2>
<p>This section exactly follows <a href="https://docs.pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html">the Pytorch introduction</a>.</p>
<section id="the-data" class="level3">
<h3 class="anchored" data-anchor-id="the-data">The data</h3>
<p>We will use the FashionMINST dataset, containing <span class="math inline">\(28 \times 28\)</span> images of clothing, 10 possible labels. In total there are <span class="math inline">\(60,000\)</span> image sin the training set and <span class="math inline">\(10,000\)</span> images in the test set.</p>
<p><img src="FashionMNIST.png" width="500"></p>
<div id="4c808e140659e8f8" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2025-12-09T20:16:45.132456Z&quot;,&quot;start_time&quot;:&quot;2025-12-09T20:16:45.099069Z&quot;}}">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># DataLoader is an iterable around DataSet, which stores samples and their corresponding labels.</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> datasets</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms <span class="im">import</span> ToTensor</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>training_data <span class="op">=</span> datasets.FashionMNIST(</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span><span class="st">"data"</span>,  <span class="co"># root directory</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    train<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    transform<span class="op">=</span>ToTensor()</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>test_data <span class="op">=</span> datasets.FashionMNIST(</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span><span class="st">"data"</span>,</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    train<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    transform<span class="op">=</span>ToTensor()</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="932122acd4ca09d1" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2025-12-09T20:16:45.150936Z&quot;,&quot;start_time&quot;:&quot;2025-12-09T20:16:45.139966Z&quot;}}">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create data loaders</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader(training_data, batch_size<span class="op">=</span>batch_size)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>test_dataloader <span class="op">=</span> DataLoader(test_data, batch_size<span class="op">=</span>batch_size)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> X, y <span class="kw">in</span> test_dataloader:</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Shape of X [N, C, H, W]: </span><span class="sc">{</span>X<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Shape of y: </span><span class="sc">{</span>y<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> </span><span class="sc">{</span>y<span class="sc">.</span>dtype<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">break</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])
Shape of y: torch.Size([64]) torch.int64</code></pre>
</div>
</div>
<p>This makes sense, our batch is (batch size, num channels (RGB), image height, image width) and for each image in the batch we have a single label (e.g.&nbsp;X = t-shirt).</p>
<p>When training, Batch Size…</p>
<p>Epoch…</p>
</section>
<section id="the-model" class="level3">
<h3 class="anchored" data-anchor-id="the-model">The Model</h3>
<p>The very cool thing, we can just define the inputs, outputs, and define an architecture that we will be able to mould into the function of interest. Note the amazing beauty of how flexible this is, because of our set up. We simply define inputs, outputs and a black-box architecture (for now). Then we don’t need to think about whats going on inside, we ‘shape’ this through iterative training of the weights on our known data. Once we have got a good mould we are done!</p>
<div id="b0e4b7b8959226d1" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2025-12-09T20:16:45.168023Z&quot;,&quot;start_time&quot;:&quot;2025-12-09T20:16:45.160191Z&quot;}}">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Using </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NeuralNetwork(nn.Module):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Linear layers are:  yᵢ = φ( ∑ⱼ xⱼ Wᵀⱼᵢ + bᵢ ) so together y = φ(xW^T + b)</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># where W is a (output_features, input_features) set of weights, b is vector of biases</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># so we can easily control output shape. Layers are fully connected.</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_relu_stack <span class="op">=</span> nn.Sequential(</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>, <span class="dv">512</span>),</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, <span class="dv">512</span>),</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, <span class="dv">10</span>)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.linear_relu_stack(x)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> NeuralNetwork().to(device)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Using cpu
NeuralNetwork(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=784, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=10, bias=True)
  )
)</code></pre>
</div>
</div>
<section id="what-is-the-model-learning" class="level4">
<h4 class="anchored" data-anchor-id="what-is-the-model-learning">What is the Model Learning?</h4>
<p>Let’s just look again what the model is learning. To fully characterise the dataset, we would have the joint distribution between the data and the labels p(x, y). We can draw this out in the simple case:</p>
<p>This also gives us information about the relative frequencies of the labels, y and images. Then we could convert to p(y|X) by dividing by p(x) i.e.&nbsp;Bayes</p>
<p>However, this is much more complicated, and the aim of generative models. This is what we need to learn for generative models! but, it is more complex. Can maginalise out</p>
<p>In our case, we will take a shortcut and just learn p(y|x). Basically it learns the feature space and how that maps to 28*28 dimension space and partitions it into 10 classes. The decision boundary is a hyper-surface in the space. Of course, this tells us nothing about p(x, y) because XXX. but it is a nice way of p(y|X).</p>
<p>A simple drawing!</p>
<p>but in reality, it is not a simple plane but a manifold in much hig</p>
<p>hmmm, still not really sure what exactly we are learning. DOes this literally funnel an unkonwn x into a already-known x and then map p(y|x)?</p>
<p>https://arxiv.org/abs/1311.2901 http://neuralnetworksanddeeplearning.com/ https://cs231n.github.io/linear-classify/ https://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf</p>
<div id="8970d571e3f7ae62" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2025-12-09T20:16:45.195384Z&quot;,&quot;start_time&quot;:&quot;2025-12-09T20:16:45.189151Z&quot;}}">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># We can see we have Weights, Biases, Weights, Biases, Weights, Biases (3 layers)</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The model parameters"</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>      <span class="bu">print</span>(</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f"Name: </span><span class="sc">{</span>name<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f"Type: </span><span class="sc">{</span><span class="bu">type</span>(param)<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f"Size: </span><span class="sc">{</span>param<span class="sc">.</span>size()<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f"e.g. </span><span class="sc">{</span>param[:<span class="dv">5</span>]<span class="sc">}</span><span class="ch">\n\n</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>The model parameters
Name: linear_relu_stack.0.weight
Type: &lt;class 'torch.nn.parameter.Parameter'&gt;
Size: torch.Size([512, 784])
e.g. tensor([[ 0.0242, -0.0248, -0.0166,  ..., -0.0256,  0.0325, -0.0028],
        [ 0.0264,  0.0106, -0.0349,  ..., -0.0113, -0.0121,  0.0221],
        [ 0.0038, -0.0243,  0.0197,  ..., -0.0243, -0.0306,  0.0113],
        [-0.0337, -0.0340, -0.0342,  ...,  0.0348, -0.0187, -0.0099],
        [ 0.0065,  0.0300,  0.0117,  ...,  0.0249, -0.0121,  0.0218]],
       grad_fn=&lt;SliceBackward0&gt;)


Name: linear_relu_stack.0.bias
Type: &lt;class 'torch.nn.parameter.Parameter'&gt;
Size: torch.Size([512])
e.g. tensor([-0.0340, -0.0322, -0.0116,  0.0172, -0.0129], grad_fn=&lt;SliceBackward0&gt;)


Name: linear_relu_stack.2.weight
Type: &lt;class 'torch.nn.parameter.Parameter'&gt;
Size: torch.Size([512, 512])
e.g. tensor([[ 0.0256, -0.0107, -0.0299,  ..., -0.0403,  0.0318, -0.0249],
        [ 0.0342, -0.0205,  0.0428,  ...,  0.0127,  0.0360, -0.0173],
        [-0.0149,  0.0350, -0.0221,  ...,  0.0298,  0.0333, -0.0402],
        [-0.0007, -0.0317,  0.0151,  ...,  0.0384,  0.0278, -0.0417],
        [ 0.0317, -0.0260,  0.0305,  ...,  0.0030,  0.0208, -0.0424]],
       grad_fn=&lt;SliceBackward0&gt;)


Name: linear_relu_stack.2.bias
Type: &lt;class 'torch.nn.parameter.Parameter'&gt;
Size: torch.Size([512])
e.g. tensor([-0.0398, -0.0158,  0.0001,  0.0098, -0.0318], grad_fn=&lt;SliceBackward0&gt;)


Name: linear_relu_stack.4.weight
Type: &lt;class 'torch.nn.parameter.Parameter'&gt;
Size: torch.Size([10, 512])
e.g. tensor([[-0.0050,  0.0381,  0.0136,  ...,  0.0361, -0.0427,  0.0113],
        [-0.0285, -0.0210,  0.0118,  ..., -0.0344, -0.0051, -0.0302],
        [-0.0077, -0.0141, -0.0155,  ...,  0.0079, -0.0294, -0.0201],
        [ 0.0030, -0.0211,  0.0398,  ..., -0.0036, -0.0310, -0.0081],
        [ 0.0365,  0.0120,  0.0255,  ...,  0.0139, -0.0323,  0.0019]],
       grad_fn=&lt;SliceBackward0&gt;)


Name: linear_relu_stack.4.bias
Type: &lt;class 'torch.nn.parameter.Parameter'&gt;
Size: torch.Size([10])
e.g. tensor([ 0.0121, -0.0431,  0.0244, -0.0143, -0.0158], grad_fn=&lt;SliceBackward0&gt;)

</code></pre>
</div>
</div>
</section>
</section>
<section id="training-and-evaluating-the-model" class="level3">
<h3 class="anchored" data-anchor-id="training-and-evaluating-the-model">Training and Evaluating the Model</h3>
<div id="da357b6fe6802954" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2025-12-09T20:16:49.392600Z&quot;,&quot;start_time&quot;:&quot;2025-12-09T20:16:45.213973Z&quot;}}">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(dataloader, model, loss_fn, optimizer):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    size <span class="op">=</span> <span class="bu">len</span>(dataloader.dataset)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    model.train(mode<span class="op">=</span><span class="va">True</span>)  <span class="co"># put into 'training mode'</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch, (X, y) <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        X, y <span class="op">=</span> X.to(device), y.to(device)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pred is (64, 10) tuple of predictions for this batch</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># y is (64, 1) (classes)</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Cross entropy loss https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> model(X)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(pred, y)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>        optimizer.step()  <span class="co"># perform one step θt &lt;- f(θ_{t-1})</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()  <span class="co"># zero the accumulated gradients, ready for the next step</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> batch <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> loss.item()  <span class="co"># Note `loss` is a object, we use `item()` to get the scalar loss</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>            current <span class="op">=</span> (batch <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> <span class="bu">len</span>(X)</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"loss: </span><span class="sc">{</span>loss<span class="sc">:&gt;7f}</span><span class="ss">  [</span><span class="sc">{</span>current<span class="sc">:&gt;5d}</span><span class="ss">/</span><span class="sc">{</span>size<span class="sc">:&gt;5d}</span><span class="ss">]"</span>)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>train(train_dataloader, model, loss_fn, optimizer)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>loss: 2.311416  [   64/60000]
loss: 2.297227  [ 6464/60000]
loss: 2.284765  [12864/60000]
loss: 2.277797  [19264/60000]
loss: 2.245504  [25664/60000]
loss: 2.223144  [32064/60000]
loss: 2.229235  [38464/60000]
loss: 2.200769  [44864/60000]
loss: 2.204567  [51264/60000]
loss: 2.150434  [57664/60000]</code></pre>
</div>
</div>
<div id="7d2a9d6f22016a93" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2025-12-09T20:17:12.194829Z&quot;,&quot;start_time&quot;:&quot;2025-12-09T20:16:49.400115Z&quot;}}">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test(dataloader, model, loss_fn):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">""""""</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    size <span class="op">=</span> <span class="bu">len</span>(dataloader.dataset)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    num_batches <span class="op">=</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()  <span class="co"># Go from train to eval mode</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    test_loss, correct <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():  <span class="co"># this just turns of gradient computation for speed</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> batch, (X, y) <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>            X, y <span class="op">=</span> X.to(device), y.to(device)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>            pred <span class="op">=</span> model(X)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># pred_i = torch.argmax(torch.exp(pred) / torch.sum(torch.exp(pred)), axis=1)</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>            pred_i <span class="op">=</span> pred.argmax(<span class="dv">1</span>)  <span class="co"># of course, it doesn't matter if the logits are passed through softmax, which maintains transitivity</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>            correct <span class="op">+=</span> (pred_i <span class="op">==</span> y).<span class="bu">type</span>(torch.<span class="bu">float</span>).<span class="bu">sum</span>().item()</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>            test_loss <span class="op">+=</span> loss_fn(pred, y)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>        test_loss <span class="op">/=</span> num_batches</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>        correct <span class="op">/=</span> size</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Test Error: </span><span class="ch">\n</span><span class="ss"> Accuracy: </span><span class="sc">{</span>(<span class="dv">100</span><span class="op">*</span>correct)<span class="sc">:&gt;0.1f}</span><span class="ss">%, Avg loss: </span><span class="sc">{</span>test_loss<span class="sc">:&gt;8f}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Epoch 1"</span>)</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>test(test_dataloader, model, loss_fn)</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i <span class="op">+</span> <span class="dv">2</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>    train(train_dataloader, model, loss_fn, optimizer)</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>    test(test_dataloader, model, loss_fn)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1
Test Error: 
 Accuracy: 36.9%, Avg loss: 2.159129 

Epoch 2
loss: 2.175913  [   64/60000]
loss: 2.156889  [ 6464/60000]
loss: 2.109339  [12864/60000]
loss: 2.123337  [19264/60000]
loss: 2.051893  [25664/60000]
loss: 2.000705  [32064/60000]
loss: 2.027452  [38464/60000]
loss: 1.956121  [44864/60000]
loss: 1.970606  [51264/60000]
loss: 1.868530  [57664/60000]
Test Error: 
 Accuracy: 49.2%, Avg loss: 1.889209 

Epoch 3
loss: 1.925272  [   64/60000]
loss: 1.883625  [ 6464/60000]
loss: 1.786613  [12864/60000]
loss: 1.827395  [19264/60000]
loss: 1.697302  [25664/60000]
loss: 1.655534  [32064/60000]
loss: 1.676771  [38464/60000]
loss: 1.589110  [44864/60000]
loss: 1.623531  [51264/60000]
loss: 1.493784  [57664/60000]
Test Error: 
 Accuracy: 62.1%, Avg loss: 1.532667 

Epoch 4
loss: 1.596773  [   64/60000]
loss: 1.552969  [ 6464/60000]
loss: 1.422226  [12864/60000]
loss: 1.491803  [19264/60000]
loss: 1.362407  [25664/60000]
loss: 1.363337  [32064/60000]
loss: 1.370365  [38464/60000]
loss: 1.304668  [44864/60000]
loss: 1.344313  [51264/60000]
loss: 1.227757  [57664/60000]
Test Error: 
 Accuracy: 63.6%, Avg loss: 1.266162 

Epoch 5
loss: 1.341841  [   64/60000]
loss: 1.314810  [ 6464/60000]
loss: 1.162503  [12864/60000]
loss: 1.266502  [19264/60000]
loss: 1.135582  [25664/60000]
loss: 1.166634  [32064/60000]
loss: 1.176979  [38464/60000]
loss: 1.123147  [44864/60000]
loss: 1.165496  [51264/60000]
loss: 1.070059  [57664/60000]
Test Error: 
 Accuracy: 64.6%, Avg loss: 1.097651 

Epoch 6
loss: 1.168139  [   64/60000]
loss: 1.162590  [ 6464/60000]
loss: 0.991322  [12864/60000]
loss: 1.124865  [19264/60000]
loss: 0.995096  [25664/60000]
loss: 1.032257  [32064/60000]
loss: 1.056600  [38464/60000]
loss: 1.006710  [44864/60000]
loss: 1.047809  [51264/60000]
loss: 0.972327  [57664/60000]
Test Error: 
 Accuracy: 65.7%, Avg loss: 0.988704 
</code></pre>
</div>
</div>
<p>Add a note here on generative vs discrimiantive. Here we only learn p(y|x) NOT p(x, y)! Here we simply do ML on p(y|x) !!!!</p>
</section>
</section>
<section id="training-a-neural-network-by-hand---a-simple-example" class="level2">
<h2 class="anchored" data-anchor-id="training-a-neural-network-by-hand---a-simple-example">Training a Neural Network by Hand - A ‘Simple’ Example</h2>
<p>Now, we will implement the same model that we created in Python, but this time ‘by-hand’ in Numpy. To do this, we will have to calculate the derivatives of the parameters with respect to the loss function, in order to update them during training. First, we will review a simplified version of the model that contains only weights, but no bias or nonlinear functions.</p>
<section id="the-model-1" class="level3">
<h3 class="anchored" data-anchor-id="the-model-1">The Model</h3>
Let <span class="math inline">\(l^1\)</span>, <span class="math inline">\(l^2\)</span> and <span class="math inline">\(l^3\)</span> be vector-valued functions representing the layers <span class="math inline">\(1\)</span>, <span class="math inline">\(2\)</span>, <span class="math inline">\(3\)</span> of the network. <span class="math inline">\(l^1\)</span> takes our length <span class="math inline">\(784\)</span> row vector <span class="math inline">\(\mathbf{x}\)</span> and returns a length <span class="math inline">\(512\)</span> vector. <span class="math inline">\(l^2\)</span> takes this length <span class="math inline">\(512\)</span> vector as input and returns another length <span class="math inline">\(512\)</span> veector. Our final layer <span class="math inline">\(l^3\)</span> takes this a length <span class="math inline">\(512\)</span> vector as input and outputs a length <span class="math inline">\(10\)</span> vector.
<span class="math display">\[\begin{aligned}
    &amp;l^1(\mathbf{x}) = \mathbf{x}W^1 \ \ \ \ \text{(1, 784) x (784, 512) = (1, 512)} \\
    &amp;l^2(\mathbf{l^1}) = \mathbf{l^1}W^2 \ \ \ \ \text{(1, 512) x (512, 512) = (1, 512)} \\
    &amp;l^3(\mathbf{l^2}) = \mathbf{l^2}W^3 \ \ \ \ \text{(1, 512) x (512, 10) = (1, 10)} \\
\end{aligned}\]</span>
<p>All vectors are row vectors. The <span class="math inline">\(W\)</span> are our matrices of weights with shape (input, output): <span class="math inline">\(W^1\)</span> is <span class="math inline">\((10, 512)\)</span>, <span class="math inline">\(W^2\)</span> is <span class="math inline">\((512, 512)\)</span> and <span class="math inline">\(W^3\)</span> is <span class="math inline">\((512, 10)\)</span>. We can therefore index individual weights as <span class="math inline">\(W^3_{i, o}\)</span> where <span class="math inline">\(i\)</span> is the index of the input unit (i.e.&nbsp;the unit the weight connects from), and <span class="math inline">\(o\)</span> is the index of the output unit (i.e.&nbsp;the unit the weight connects to).</p>
<p>You can see by <strong>writing out the matrix multiplication</strong> that this operation matches the information flow through the network.</p>
<p>[IMAGE]</p>
<p><em>(An aside)</em>: This notation a little non-standard (often vectors are column vectors, and the weight matrix is shape (output, input) but this notation makes the derivations below much simpler. Also the layers are functions <span class="math inline">\(l^3(\cdot)\)</span> but can be treated as vectors <span class="math inline">\(\mathbf{l^3}\)</span> when evaluated. We will typically write them as vectors.</p>
</section>
<section id="the-loss" class="level3">
<h3 class="anchored" data-anchor-id="the-loss">The Loss</h3>
<p><strong>TODO</strong>: add, define, use the word ‘score’</p>
<p>We take the <span class="math inline">\((1, 10)\)</span> shape vector of logits (representing score for each label) output from the final layer, and use these to compute our loss. We will use the cross-entropy loss:</p>
<p><span class="math display">\[
L(\mathbf{l^3}, y) = -\log \dfrac{ \exp{ l^3_y }}{ \sum_k \exp{ l^3_k }}
\]</span></p>
<p>Here <span class="math inline">\(l^3_k\)</span> is the element of our vector of logits with some index <span class="math inline">\(k\)</span>, and <span class="math inline">\(y\)</span> is the index of the correct label for this image. You may recognise the term after the <span class="math inline">\(\log\)</span> as the <a href="https://en.wikipedia.org/wiki/Softmax_function#:~:text=The%20softmax%20function%2C%20also%20known,used%20in%20multinomial%20logistic%20regression">softmax function</a> which normalises the logits to probabilities. Therefore, we are computing the probability our model assigns the input image <span class="math inline">\(\mathbf{x}\)</span> being (the correct) label <span class="math inline">\(y\)</span>. Clearly, we want this probability to be high (<span class="math inline">\(1\)</span>, ideally). <a href="https://cs231n.github.io/linear-classify/">This article</a> as a nice, deeper discussion of the Cross Entropy Loss.</p>
<p>So this loss makes intuitive sense. We have an image <span class="math inline">\(\mathbf{x}\)</span> and are computing a set of probabilities, one for each of the <span class="math inline">\(10\)</span> labels. We are aiming to maximise the probability associated with the correct label, <span class="math inline">\(y\)</span>, as we know in this supervised learning context that this is the actual label for this image. Here all we are doing on top is (equivalently) minimising the negative log probability.</p>
</section>
<section id="predicting-a-label-from-an-image---the-forward-pass" class="level3">
<h3 class="anchored" data-anchor-id="predicting-a-label-from-an-image---the-forward-pass">Predicting a label from an image - the forward pass</h3>
<p>For this simple network, it is reliatively easy to use our model to i) take an image <span class="math inline">\(\mathbf{x}\)</span> ii) map it to our output of 10 logits iii) use these to predict a label <span class="math inline">\(\hat{y}\)</span>. First we apply the network to the input data:</p>
<span class="math display">\[\begin{aligned}
\mathbf{l^3} &amp;= l^3(l^2(l^1(\mathbf{x}))) \\
             &amp;= (((\mathbf{x}W^1) W^2) W^3 \\
             &amp;=  \mathbf{x}W^1 W^2 W^3 \\
\end{aligned}\]</span>
<p>We then take this output, and the predicted label is the one that maximises the probability as copmuted by the softmax function:</p>
<p><span class="math display">\[
\hat{y} = \arg\max_{y} \, \mathrm{softmax}(\mathbf{l^3})_y
\]</span></p>
<p>In other words, the predicted label is the one the one the model assigns the highest probability to. The notation can be confusing at first look, but we take advantage of that fact that the labels are defined as indices—so we can use the correct label <span class="math inline">\(y\)</span> to index out the corresponding entry in the vector of probaiblities.</p>
</section>
<section id="backpropagation-in-our-simple-network" class="level3">
<h3 class="anchored" data-anchor-id="backpropagation-in-our-simple-network">Backpropagation in our simple network</h3>
<p>We want to find the set of weights that minimise the loss function. To do this, we will traverse this function by travelling along it’s negative slope. Mathematically, this means we need to compute the derivative of our loss function with respect to the weights.</p>
<p>For example, take <span class="math inline">\(W^3_{1,2}\)</span> that connects the first neuron from layer 2 to the second neuron in layer 3. How does a small change in this weight change the output of our loss function? In this case, a small change in <span class="math inline">\(W^3_{1,2}\)</span> will result in a small change in the second neuron in layer 3 (<span class="math inline">\(l^3_2\)</span>) which will directly affect the loss function <span class="math inline">\(L(\mathbf{l^3})\)</span>. This ‘chain’ of dependencies is capture by chain rule. Here we use <a href="https://en.wikipedia.org/wiki/Partial_derivative">partial derivatives</a> because we only care about how a function that takes mutliple inputs changes with respect to a single one of those inputs.</p>
<span class="math display">\[\begin{aligned}
&amp;\dfrac{\partial L(\mathbf{l^3})}{\partial W^3_{1,2}} = \dfrac{\partial L(\mathbf{l^3})}{\partial l^3_2}  \dfrac{\partial l^3_2 }{\partial W^3_{1,2}}
\end{aligned}\]</span>
<p>Often, we will write equations that collect all weights for a layer and their affect on all neurons in a layer. Also, from now on, we will write the loss as <span class="math inline">\(L\)</span> instead of <span class="math inline">\(L(\mathbf{l^3})\)</span> just for brevity, but it is useful to remember it a function of out output layer. Together:</p>
<span class="math display">\[\begin{aligned}
&amp;\dfrac{\partial L}{\partial W^3} = \dfrac{\partial L}{\partial \mathbf{l^3}}   \dfrac{\partial \mathbf{l^3} }{\partial W^3}
\end{aligned}\]</span>
<p>and for the other weight matrices:</p>
<span class="math display">\[\begin{aligned}
&amp;\dfrac{\partial L}{\partial W^2} = \dfrac{\partial L}{\partial \mathbf{l^3}}  \dfrac{\partial \mathbf{l^3}}{\partial \mathbf{l^2}}  \dfrac{\partial \mathbf{l^2} }{\partial W^3}  \\
&amp;\dfrac{\partial L}{\partial W^1} = \dfrac{\partial L}{\partial \mathbf{l^3}}  \dfrac{\partial \mathbf{l^3}}{\partial \mathbf{l^2}}  \dfrac{\partial \mathbf{l^2}}{\partial \mathbf{l^1}}  \dfrac{\partial \mathbf{l^1} }{\partial W^1}
\end{aligned}\]</span>
<p>TODO: CHECK AGAINST https://www.jasonosajima.com/backprop.html</p>
<p>e.g.&nbsp;for how the loss changes with a change in weights in layer 2, we look at: i) how changing the weights in layer 2 affects the output layer 2 ii) how a change in layer 2 affects the output of layer 3 ii) how a change in layer 3 affects the loss.</p>
<section id="understanding-each-term-with-matrix-calculus" class="level4">
<h4 class="anchored" data-anchor-id="understanding-each-term-with-matrix-calculus">Understanding each term with <a href="https://en.wikipedia.org/wiki/Matrix_calculus">matrix calculus</a></h4>
<p>This notation is doing a lot of heavy lifting and hiding complexity. Take <span class="math inline">\(\dfrac{\partial L}{\partial W^2}\)</span>, <span class="math inline">\(L\)</span> is a scalar-valued function (it takes as an input a vector of length 10, our output of layer 3, and returns a scalar). We want to take the deriative of this with respect to a matrix—how each element of <span class="math inline">\(W^3_{i,o}\)</span> affects the loss. Therefore this derivative will be a <span class="math inline">\((512, 10)\)</span> matrix (the size of <span class="math inline">\(W^3\)</span>).</p>
<p>How about <span class="math inline">\(\dfrac{\partial \mathbf{l^2}}{\partial \mathbf{l^3}}\)</span>? This is the derivative of a vector-valued function (it outputs a vector, of length <span class="math inline">\(512\)</span>) with repsect to a vector! This is the Jacobian, which tracks how element <span class="math inline">\(l^2\)</span> affects each element in <span class="math inline">\(l^3\)</span>. As layer 3 has <span class="math inline">\(10\)</span> neurons and layer 2 has <span class="math inline">\(512\)</span>, we will have a <span class="math inline">\((10, 512)\)</span> matrix of partial derivatives.</p>
<p><span class="math inline">\(\dfrac{\partial \mathbf{l^2} }{\partial W^3}\)</span> asks how a small change in each weight in <span class="math inline">\(W^2\)</span> will affect each dimension in <span class="math inline">\(\mathbf{l^2}\)</span>. We will have a <span class="math inline">\((512, 512, 512)\)</span>, rank-3 tensor!</p>
</section>
<section id="computing-the-derivatives-with-respect-to-w3" class="level4">
<h4 class="anchored" data-anchor-id="computing-the-derivatives-with-respect-to-w3">Computing the derivatives with respect to <span class="math inline">\(W^3\)</span></h4>
<p>Fortunately the structure of our network means many of these partial derivatives are zero, and we can simply this a lot. Below, we go through each of these in turn, before implementing this network in Python. First we will look in detail at computing the derivative of the loss with respect to the weights connected to the final layer:</p>
<p><span class="math display">\[
\dfrac{\partial L}{\partial W^3} = \dfrac{\partial L}{\partial \mathbf{l^3}}   \dfrac{\partial \mathbf{l^3} }{\partial W^3}
\]</span></p>
<section id="dfracpartial-lpartial-w3" class="level5">
<h5 class="anchored" data-anchor-id="dfracpartial-lpartial-w3"><span class="math inline">\(\dfrac{\partial L}{\partial W^3}\)</span></h5>
<p>This is the exact derivative we want to compute to update the weights for the third layer, <span class="math inline">\(W^3\)</span>. It will be a <span class="math inline">\((512, 10)\)</span> matrix, with each element being how a small change in that weight affects the loss e.g.&nbsp;(we drop the superscript for brevity):</p>
<p><span class="math display">\[
    \dfrac{\partial L }{\partial W} =
    \begin{bmatrix}
        \dfrac{ \partial L }{ \partial W_{1,1} } &amp; \dfrac{ \partial L }{ \partial W_{1,2} } &amp; ... &amp; \dfrac{ \partial L }{ \partial W_{1,10} }\\
        \dfrac{ \partial L }{ \partial W_{2,1} } &amp; \ddots &amp; &amp; \vdots \\
        \vdots \\
        \dfrac{ \partial L }{ \partial W_{512,1} } &amp; \dots &amp; &amp;   \dfrac{ \partial L }{ \partial W_{512,10} }
    \end{bmatrix}
\]</span></p>
<p>Next we will explore how to compute the elements of this matrix.</p>
</section>
<section id="dfracpartial-lpartial-mathbfl3" class="level5">
<h5 class="anchored" data-anchor-id="dfracpartial-lpartial-mathbfl3"><span class="math inline">\(\dfrac{\partial L}{\partial \mathbf{l^3}}\)</span></h5>
<p>This is the derivative of a scalar-valued function with respect to a vector. In our case, it will be a 10-dimension vector of partial derivatives, where each entry captures the loss changes with respect to a change in that unit of layer 3.</p>
<p>We can evaluate the derivative of the cross entropy loss directly:</p>
<p><span class="math display">\[
\dfrac{\partial }{\partial l^3_k} - \log \dfrac{e^{l^3_y}}{\sum_k e^{l^3_k}} = \text{softmax}(l^3_k) - \delta_{k, y}
\]</span></p>
<p>Where <span class="math inline">\(\delta\)</span> is the <a href="https://en.wikipedia.org/wiki/Kronecker_delta">Kronecker delta</a> that equals <span class="math inline">\(1\)</span> when <span class="math inline">\(k=y\)</span> and <span class="math inline">\(0\)</span> otherwise. See <strong>Appendix 1</strong> for the derivation. In full this looks like:</p>
<p><span class="math display">\[
\dfrac{ \partial L}{ \partial \mathbf{l^3} } =
\begin{bmatrix}
    \dfrac{\partial L}{\partial l^3_1} \\
    \dfrac{\partial L}{\partial l^3_2} \\
    \vdots\\
    \dfrac{\partial L}{\partial l^3_{10}}
\end{bmatrix}
=
\begin{bmatrix}
    \text{softmax}(l^3_1) - \delta_{1, y} \\
    \text{softmax}(l^3_2) - \delta_{2, y} \\
    \vdots \\
    \text{softmax}(l^3_{10}) - \delta_{10, y}
\end{bmatrix}
\]</span></p>
<p>Together, this says that our vector of derivatives is <span class="math inline">\(\text{softmax}(l^3_i)\)</span> for all output logits in layer 3 that are for the incorrect labels, and <span class="math inline">\(\text{softmax}(l^3_i) - 1\)</span> for the index of the true label <span class="math inline">\(y\)</span>.</p>
</section>
<section id="dfracpartial-mathbfl3-partial-w3" class="level5">
<h5 class="anchored" data-anchor-id="dfracpartial-mathbfl3-partial-w3"><span class="math inline">\(\dfrac{\partial \mathbf{l^3} }{\partial W^3}\)</span></h5>
<p>As mentioned above, this is a 3-rank tensor with the shape <span class="math inline">\((512, 10, 10)\)</span> (how each element of <span class="math inline">\(l^3_n\)</span> changes with each weight <span class="math inline">\(W^3_{i,o}\)</span>). How can we even deal with this in our computation?</p>
<p>Luckily, the structure of our layered neural network means that this can be significantly reduced, as most of the elements are zero. The strategy is to break the problem down to look at individual elements of <span class="math inline">\(\dfrac{\partial L}{\partial W^3_{i,o}}\)</span> and fill in each term-wise using the total derivative rule (<strong>Appendix 2</strong>):</p>
<p><span class="math display">\[\begin{align}
\dfrac{\partial L}{\partial W^3_{i,o}}
&amp;= \dfrac{\partial L}{\partial l^3} \dfrac{\partial l^3}{\partial W^3_{i,o}}  \\
&amp;= \sum_k \dfrac{\partial L}{\partial l^3_k} \dfrac{\partial l^3_k}{\partial W^3_{i,o}} \\
&amp;= \dfrac{\partial L}{\partial l^3_o} \dfrac{\partial l^3_o}{\partial W^3_{i,o}} \\
&amp;= \dfrac{\partial L}{\partial l^3_o} l^2_i
\end{align}\]</span></p>
<p>Obviously, the notation can get a little tricky to follow here. In words, to find out how the loss chanegs with respect to a single weight <span class="math inline">\(W^3_{i,o}\)</span> we will see how changing this weight changes layer 3, and then how changing layer 3 changes the loss. To compute this, we will take the sum over how each weight <span class="math inline">\(W^3_{i,o}\)</span> affects the unit <span class="math inline">\(k\)</span> in layer 3, <span class="math inline">\(l^3_k\)</span>, then how a change in this unit affects the loss. This weight only connects to one unit, <span class="math inline">\(o\)</span> and so the effect of changing that weight on all other units is zero.</p>
<p>Finally, we can take the usual scalar derivative here <span class="math inline">\(\dfrac{\partial l^3_o}{\partial W^3_{i,o}} = \dfrac{\partial \ l^2_i W^3_{i,o}}{\partial W^3_{i,o}} = l^2_i\)</span>. This makes intuitive sense, the change in <span class="math inline">\(l^3_o\)</span> when we change <span class="math inline">\(W^3_{i,o}\)</span> is exactly the value of the input unit, <span class="math inline">\(l^2_i\)</span>.</p>
</section>
<section id="putting-this-all-together-dfracpartial-lpartial-w3-dfracpartial-lpartial-mathbfl3-dfracpartial-mathbfl3-partial-w3" class="level5">
<h5 class="anchored" data-anchor-id="putting-this-all-together-dfracpartial-lpartial-w3-dfracpartial-lpartial-mathbfl3-dfracpartial-mathbfl3-partial-w3">Putting this all together: <span class="math inline">\(\dfrac{\partial L}{\partial W^3} = \dfrac{\partial L}{\partial \mathbf{l^3}} \dfrac{\partial \mathbf{l^3} }{\partial W^3}\)</span></h5>
<p>Now we can put the element-wise approach and all derivatives computed above together in one matrix. All the weights are all <span class="math inline">\(W^3\)</span> matrix and we omit the superscript for brevity:</p>
<p><span class="math display">\[
\dfrac{ \partial L}{ \partial W^3} =
\begin{bmatrix}
\dfrac{\partial L}{ \partial l^3_1 } \dfrac{\partial l^3_1 }{ \partial W_{1,1} }
&amp; \dfrac{\partial L}{ \partial l^3_2 } \dfrac{\partial l^3_2 }{ \partial W_{1,2} }
&amp; \dots
&amp; \dfrac{\partial L}{ \partial l^3_{10} } \dfrac{\partial l^3_{10} }{ \partial W_{1, 10} }  \\
\dfrac{\partial L}{ \partial l^3_1 } \dfrac{\partial l^3_1 }{ \partial W_{2,1} }
&amp; \ddots \\
\vdots \\
\dfrac{\partial L}{ \partial l^3_1 } \dfrac{\partial l^3_1 }{ \partial W_{512,1} }
&amp; \dots &amp;
&amp; \dfrac{\partial L}{ \partial l^3_{10} } \dfrac{\partial l^3_{10} }{ \partial W_{512,10} }
\end{bmatrix}
=
\begin{bmatrix}
[\text{sm}(l^3_1) - \delta_{1, y}]l^2_1 &amp;
[\text{sm}(l^3_2) - \delta_{2, y}] l^2_1
&amp; \dots
&amp; [\text{sm}(l^3_{10}) - \delta_{10, y}] l^2_1 &amp; \\
[\text{sm}(l^3_1) - \delta_{1, y}] l^2_2 &amp; \ddots &amp; &amp; \vdots \\
\vdots \\
[\text{sm}(l^3_1) - \delta_{1, y}] l^2_{512} &amp; \dots &amp;
&amp; [\text{sm}(l^3_{10}) - \delta_{10, y}] l^2_{512}
\end{bmatrix}
\]</span></p>
<p>i.e.&nbsp;for each weight, we see the effect of changing that weight on the layer 3 unit it is connected to, multiplied by the effect of changing that layer 3 unit on the loss.</p>
<p>We can write all of this as the product of two vectors (recall all vectors are row vectors here, so the transpose is to a column vector):</p>
<p><span class="math display">\[
\dfrac{\partial L}{\partial W^3} = (\mathbf{l^2})^T \dfrac{\partial L}{\partial \mathbf{l^3}}
\]</span></p>
</section>
</section>
<section id="computing-the-derivatives-with-respect-to-w2" class="level4">
<h4 class="anchored" data-anchor-id="computing-the-derivatives-with-respect-to-w2">Computing the derivatives with respect to <span class="math inline">\(W^2\)</span></h4>
<p>For the most part, computing the derivative of the loss with respect to the layer 2 weights uses all of the same ideas and tricks as above. There is one element of added complexity—changing an element <span class="math inline">\(W^2_{i,o}\)</span> affects only one element of layer 2, <span class="math inline">\(l^2_o\)</span>. But this layer is connected to <em>every</em> unit in layer 3, and we need to account for all of these changes. Luckily, we can iteratively compute the first terms to simpify them, so when we are ready to deal with the weights we don’t need to think about all of this complexity. Recall we are trying to compute:</p>
<p><span class="math display">\[
\dfrac{\partial L}{\partial W^2} = \dfrac{\partial L}{\partial \mathbf{l^3}}  \dfrac{\partial \mathbf{l^3}}{\partial \mathbf{l^2}}  \dfrac{\partial \mathbf{l^2} }{\partial W^3}
\]</span></p>
<p>We have seen similar terms to all of these above, except for the middle term on the right hand side, the Jacobian.</p>
<section id="dfracpartial-mathbfl2partial-mathbfl3" class="level5">
<h5 class="anchored" data-anchor-id="dfracpartial-mathbfl2partial-mathbfl3"><span class="math inline">\(\dfrac{\partial \mathbf{l^2}}{\partial \mathbf{l^3}}\)</span></h5>
<p>This is the Jacobian. It is <span class="math inline">\((512, 10)\)</span> and contains the derivative of each element in <span class="math inline">\(\mathbf{l^2}\)</span> with respect to each element in <span class="math inline">\(\mathbf{l^3}\)</span>:</p>
<p>TODO: CHECK WHAT WAY AROUND THIS SHOULD BE, ALSO CHECK DIMS OF ABOUVE ARE (512, 512, 10) AND IT MATCHES THIS</p>
<span class="math display">\[\begin{aligned}
\dfrac{\partial \mathbf{l^3}}{\partial \mathbf{l^2}} =
\begin{bmatrix}
    \dfrac{\partial l^3_1}{\partial l^2_1} &amp; \dfrac{\partial l^3_2}{\partial l^2_1} &amp; \dots &amp; \dfrac{\partial l^3_{10}}{\partial l^2_1} \\
    \dfrac{\partial l^3_1}{\partial l^2_2} &amp; \ddots &amp; &amp; \vdots \\
    \vdots &amp; &amp; \\
    \dfrac{\partial l^3_1}{\partial l^2_{512}} &amp; \dots &amp; &amp; \dfrac{\partial l^3_{10}}{\partial l^2_{512}}
\end{bmatrix}
\end{aligned}\]</span>
<p>If we try and calculate the right hand side of the above equation, we are trying to multiply a <span class="math inline">\((512,10)\)</span> matrix <span class="math inline">\(\dfrac{\partial \mathbf{l^3}}{\partial \mathbf{l^2}}\)</span> with a <span class="math inline">\((510, 510, 10)\)</span> 3-rank tensor <span class="math inline">\(\dfrac{\partial \mathbf{l^2} }{\partial W^3}\)</span>. Luckily, we will can go from the left and collapse the Jacobian, very similar to how we collapsed the weight matrix above. As long as we compute derivatives with respect to the scalar loss, they will keep the same of the vector or matrix we are taking the derivative with respect to.</p>
</section>
<section id="putting-the-layer-2-weights-calculation-together" class="level5">
<h5 class="anchored" data-anchor-id="putting-the-layer-2-weights-calculation-together">Putting the Layer 2 weights calculation together:</h5>
<p><span class="math display">\[
\dfrac{\partial L}{\partial W^2} = \dfrac{\partial L}{\partial \mathbf{l^3}}  \dfrac{\partial \mathbf{l^3}}{\partial \mathbf{l^2}}  \dfrac{\partial \mathbf{l^2} }{\partial W^3}
\]</span></p>
<p>First, we will calculate <span class="math inline">\(\dfrac{\partial L}{\partial \mathbf{l^3}}  \dfrac{\partial \mathbf{l^3}}{\partial \mathbf{l^2}}\)</span>, that we know will be a length <span class="math inline">\(512\)</span> vector, using the total derivative rule as above:</p>
<p><span class="math display">\[
\dfrac{ \partial L }{ \partial l^2_i } = \sum_o \dfrac{ \partial L }{ \partial l^3_o } \dfrac{\partial l^3_o }{ \partial l^2_i }
\]</span></p>
<p>Note this is slightly more complex than the <span class="math inline">\(W^3\)</span> case. Because every unit in layer 2 is connected to every unit in layer 3, the derivatives don’t go to zero. We will actually need to compute every derivative. However, we can see each term in the sum only requires application of scalar derivative rules, e.g.:</p>
<p><span class="math display">\[
\dfrac{\partial l^3_o }{ \partial l^2_i } = \dfrac{\partial l^2_i W^3_{i, o} }{ \partial l^2_i } = W^3_{i, o}
\]</span> XXX</p>
<p>So together, the full vector of partial derivatives is:</p>
<span class="math display">\[\begin{aligned}
\begin{bmatrix}
    \sum_o \dfrac{ \partial L }{ \partial l^3_o } \dfrac{\partial l^3_o }{ \partial l^2_1 } &amp;
    \sum_o \dfrac{ \partial L }{ \partial l^3_o } \dfrac{\partial l^3_o }{ \partial l^2_2 } &amp;
    \cdots &amp;
    \sum_o \dfrac{ \partial L }{ \partial l^3_o } \dfrac{\partial l^3_o }{ \partial l^2_{512} }
\end{bmatrix}
\end{aligned}\]</span>
<p>And so we can compute this with the matrix vector calculation:</p>
<p><span class="math display">\[
\dfrac{ \partial L }{ \partial \mathbf{l^2} } = \dfrac{ \partial L}{ \partial \mathbf{l^3}} (W^3)^T
\]</span></p>
<p>which is a <span class="math inline">\((1, 10)\)</span> vector multiplying a <span class="math inline">\((10, 512)\)</span> matrix.</p>
<p>Now, we can finally compute <span class="math inline">\(\dfrac{ \partial L}{\partial W^2} = \dfrac{\partial L}{\partial \mathbf{l^2} } \dfrac{\partial \mathbf{l^2}}{\partial W^2}\)</span> which we can do again with the total derivaive! And we do it in exactly the same way we computed <span class="math inline">\(\dfrac{ \partial L}{\partial W^3} = \dfrac{\partial L}{\partial \mathbf{l^3} } \dfrac{\partial \mathbf{l^3}}{\partial W^3}\)</span> above. In this case it is the outer product of the vectors:</p>
<p><span class="math display">\[
\dfrac{ \partial L }{ \partial W^2 } = (\mathbf{l^1})^T \dfrac{ \partial L }{ \partial \mathbf{l^2} }
\]</span></p>
<p>i.e.&nbsp;the outer product of a <span class="math inline">\((512, 1)\)</span> vector by a <span class="math inline">\((1, 512)\)</span> vector to give us a <span class="math inline">\((512, 512)\)</span> matrix of derivatives. For example, the first row of <span class="math inline">\(\dfrac{ \partial L }{ \partial W^2 }\)</span> is how the loss changes with respect to a change in the weights that connect layer 1 unit <span class="math inline">\(1\)</span> to every unit in layer 2 unit. A change in the weight will change layer 2 unit <span class="math inline">\(o\)</span> by the value of layer 1 unit <span class="math inline">\(1\)</span> - this is the same for every weight in that row. This then is multiplied by how a small change in that unit <span class="math inline">\(o\)</span> in layer 2 effects the loss.</p>
<p><strong>It’s worth reflecting on this. We have such complexity here. And it reduces really nicely.</strong></p>
</section>
<section id="computing-the-partial-derivatives-of-w1" class="level5">
<h5 class="anchored" data-anchor-id="computing-the-partial-derivatives-of-w1">Computing the partial derivatives of <span class="math inline">\(W^1\)</span></h5>
<p>Since we have done through all of the hard work of really inspecting what is going on under the hood, we can really simply the notation going forward. This compact notation really highlights the ‘chain’ aspect of the chain rule:</p>
<p><span class="math display">\[
\dfrac{\partial L}{\partial W^1} = \dfrac{\partial L}{\partial \mathbf{l^3}}  \dfrac{\partial \mathbf{l^3}}{\partial \mathbf{l^2}}  \dfrac{\partial \mathbf{l^2}}{\partial \mathbf{l^1}}  \dfrac{\partial \mathbf{l^1} }{\partial W^1}
\]</span></p>
<p>We are using the total derivative rule to condense all intermediate calculations, just like above:</p>
<p>First:</p>
<p><span class="math display">\[
\mathbf{g_1} = \dfrac{\partial L}{\partial \mathbf{l^2}} = \dfrac{\partial L}{\partial \mathbf{l^3}}  \dfrac{\partial \mathbf{l^3}}{\partial \mathbf{l^2}} = \dfrac{\partial L}{\partial \mathbf{l^3}} (W^3)^T
\]</span></p>
<p><span class="math inline">\((1, 512) = (1, 10) \times (10, 512)\)</span></p>
<p><span class="math display">\[
\mathbf{g_2} = \dfrac{\partial L}{\partial \mathbf{l^1}} =  \dfrac{\partial L}{\partial \mathbf{l^2}} \dfrac{\partial \mathbf{l^2}}{\partial \mathbf{l^1}} = \mathbf{g_1}(W^2)^T
\]</span></p>
<p><span class="math inline">\((1, 512) = (1, 512) \times (512, 512)\)</span></p>
<p>Finally we have the outer product calculation:</p>
<p><span class="math display">\[
\dfrac{\partial L}{\partial W^1} = \dfrac{\partial L}{\partial \mathbf{l^1}} \dfrac{\partial \mathbf{l^1}}{\partial W^1} = (\mathbf{x})^T \mathbf{g_2}
\]</span></p>
<p><span class="math inline">\((512, 512) = (512, 1) \times (1, 512)\)</span></p>
<p>It’s awesome to see how changing the derivatives, backwards from the final layer, greatly simplifies computing the derivatives for our complex, fully connected network.</p>
<p>It is also very easy to compte, because we already have our weight vectors and we compute the output of each layer as part of our forward pass!</p>
</section>
</section>
</section>
</section>
<section id="the-code" class="level2">
<h2 class="anchored" data-anchor-id="the-code">The Code</h2>
<div id="9e907e85b1133c4e" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2025-12-09T20:17:12.207717Z&quot;,&quot;start_time&quot;:&quot;2025-12-09T20:17:12.202352Z&quot;}}">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>run <span class="op">=</span> <span class="va">False</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MyBasicNetwork:</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, learning_rate<span class="op">=</span><span class="fl">0.02</span>):</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.a <span class="op">=</span> learning_rate</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Define weight matrix (output dim, input dim) by convention</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use zero-mean Xavier init (good for sigmoid, it has little</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># effect here as we don't use activation functions,</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># but useful for comparison.)</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W1 <span class="op">=</span> np.random.randn(<span class="dv">784</span>, <span class="dv">512</span>) <span class="op">*</span> np.sqrt(<span class="dv">1</span> <span class="op">/</span> <span class="dv">784</span>)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W2 <span class="op">=</span> np.random.randn(<span class="dv">512</span>, <span class="dv">512</span>) <span class="op">*</span> np.sqrt(<span class="dv">1</span> <span class="op">/</span> <span class="dv">512</span>)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W3 <span class="op">=</span> np.random.randn(<span class="dv">512</span>, <span class="dv">10</span>) <span class="op">*</span> np.sqrt(<span class="dv">1</span> <span class="op">/</span> <span class="dv">512</span>)</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss(<span class="va">self</span>, l3, y):</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> <span class="va">self</span>.softmax(l3)[<span class="dv">0</span>][y]</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="op">-</span>np.log( p <span class="op">+</span> <span class="fl">1e-15</span>)</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> softmax(<span class="va">self</span>, vec):</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>        C <span class="op">=</span> np.<span class="bu">max</span>(vec)</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.exp(vec <span class="op">-</span> C) <span class="op">/</span> np.<span class="bu">sum</span>(np.exp(vec <span class="op">-</span> C))</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># forward pass through the network</span></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.reshape(<span class="dv">1</span>, x.size)</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>        l1 <span class="op">=</span> x <span class="op">@</span> <span class="va">self</span>.W1</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>        l2 <span class="op">=</span> l1 <span class="op">@</span> <span class="va">self</span>.W2</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>        l3 <span class="op">=</span> l2 <span class="op">@</span> <span class="va">self</span>.W3</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> np.argmax(<span class="va">self</span>.softmax(l3))</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pred, l1, l2, l3</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_weights(<span class="va">self</span>, x, y, verbose<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>        _, l1, l2, l3 <span class="op">=</span> <span class="va">self</span>.predict(x)</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="va">self</span>.loss(l3, y)</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> verbose:</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Loss: </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the derivatives</span></span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a>        dloss_dl3 <span class="op">=</span> <span class="va">self</span>.softmax(l3)  <span class="co"># double check this</span></span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a>        dloss_dl3[<span class="dv">0</span>][y] <span class="op">-=</span> <span class="dv">1</span></span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a>        dloss_dW3 <span class="op">=</span> l2.T <span class="op">@</span> dloss_dl3       <span class="co"># (512, 10) = (512, 1) x (1, 10)</span></span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a>        dloss_dl2 <span class="op">=</span> dloss_dl3 <span class="op">@</span> <span class="va">self</span>.W3.T  <span class="co"># (1, 512) = (1, 10) x (10, 512)</span></span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a>        dloss_dW2 <span class="op">=</span> l1.T <span class="op">@</span> dloss_dl2       <span class="co"># (512, 512) = (512, 1) x (1, 512)</span></span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a>        dloss_dl1 <span class="op">=</span> dloss_dl2 <span class="op">@</span> <span class="va">self</span>.W2.T  <span class="co"># (1, 512) = (1, 512) x (512, 512)</span></span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a>        dloss_dW1 <span class="op">=</span> x.T <span class="op">@</span> dloss_dl1        <span class="co"># (784, 512) = (781, 1) x (1, 512)</span></span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W3 <span class="op">-=</span> <span class="va">self</span>.a <span class="op">*</span> dloss_dW3</span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W2 <span class="op">-=</span> <span class="va">self</span>.a <span class="op">*</span> dloss_dW2</span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W1 <span class="op">-=</span> <span class="va">self</span>.a <span class="op">*</span> dloss_dW1</span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a><span class="co"># We won't run this here because it is very slow,</span></span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a><span class="co"># but it gives an accuracy of ~73%</span></span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> run:</span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialise and train the model (no batching)</span></span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> MyBasicNetwork()</span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (X, y) <span class="kw">in</span> <span class="bu">enumerate</span>(training_data):</span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> np.asarray(X[<span class="dv">0</span>, :, :])</span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> <span class="bu">int</span>(y)</span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-75"><a href="#cb12-75" aria-hidden="true" tabindex="-1"></a>        model.update_weights(x, y)</span>
<span id="cb12-76"><a href="#cb12-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-77"><a href="#cb12-77" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb12-78"><a href="#cb12-78" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Training iteration: sample: </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-79"><a href="#cb12-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-80"><a href="#cb12-80" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check the model accuracy</span></span>
<span id="cb12-81"><a href="#cb12-81" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> np.empty(<span class="bu">len</span>(test_data))</span>
<span id="cb12-82"><a href="#cb12-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-83"><a href="#cb12-83" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (X, y) <span class="kw">in</span> <span class="bu">enumerate</span>(test_data):</span>
<span id="cb12-84"><a href="#cb12-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-85"><a href="#cb12-85" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> np.asarray(X[<span class="dv">0</span>, :, :])</span>
<span id="cb12-86"><a href="#cb12-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-87"><a href="#cb12-87" aria-hidden="true" tabindex="-1"></a>        results[i] <span class="op">=</span> model.predict(x)[<span class="dv">0</span>] <span class="op">==</span> y</span>
<span id="cb12-88"><a href="#cb12-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-89"><a href="#cb12-89" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Percent Correct: </span><span class="sc">{</span>np<span class="sc">.</span>mean(results) <span class="op">*</span> <span class="dv">100</span><span class="sc">}</span><span class="ss">%"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="a-better-model" class="level2">
<h2 class="anchored" data-anchor-id="a-better-model">A Better Model</h2>
<p>We will add some simple changes to improve the performance of the model. In our simple version about, we essentially had <span class="math inline">\(\mathbf{l^3} = \mathbf{x}W\)</span> where <span class="math inline">\(W\)</span> is a <span class="math inline">\((784, 10)\)</span> matrix. One way to intepret this is a matrix encoding 10 linear hyperplanes in a <span class="math inline">\(784\)</span>-dimension space that separate points according to their label.</p>
<p>A simple change we will make is to include a bias term <span class="math inline">\(\mathbf{b}\)</span>. We can interpret this as adding an offset to each hyperplane, meaning our data does not need to be centered at zero.</p>
<p>More improtantly, we will also add an activation function around each unit. Once the activation at each unit is computed, we run it through a nonlinear function. Immediately, this means we loose the ‘linear hyperplane’ interpretation. This change now means the SEP BOUNDARY between groups do not need to be linear hyperplanes, but can be much more flexible and bendy. wNow, we will introduce an activation function which will make our network nonlinear. <strong>LINK TO CS PAGE.</strong></p>
<p>Our network is now:</p>
<p><span class="math display">\[
\begin{aligned}
    l^1 &amp;= \phi(\mathbf{x}W^1 + \mathbf{b^1}) \\
    l^2 &amp;= \phi(\mathbf{l^1}W^2 + \mathbf{b^2}) \\
    l^3 &amp;= \mathbf{l^2}W^3 + \mathbf{b^3} \\
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\phi(\cdot)\)</span> is our nonlinear activation function. <span class="math inline">\(\mathbf{b^i}\)</span> is a <span class="math inline">\((1, n)\)</span> vector of biases, one for each of <span class="math inline">\(n\)</span> units layer <span class="math inline">\(i\)</span>. A note on the notation, the output of <span class="math inline">\(\mathbf{l^{n-1}}W^n+\mathbf{b^n}\)</span> in each layer is still a vector (e.g.&nbsp;<span class="math inline">\((1, 10)\)</span>) and we apply the nonlinear activation function element-wise to this vector.</p>
<p>Note we use the sigmoid function because it has interesting derivaives, but in general ReLu is preferred in larger, modern networks due to the <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">vanishing gradient problem</a>. Note we don’t apply the sigmoid function to the last layer, as we feed this directly to the cross-entropy loss which is already doing a mapping with the softmax function.</p>
<p>Now, we will take the derivatives of the loss with respect to the weights <em>and</em> biases.</p>
<section id="computing-the-derivatives" class="level3">
<h3 class="anchored" data-anchor-id="computing-the-derivatives">Computing the derivatives</h3>
<section id="derivatives-of-the-sigmoid-activation-function" class="level4">
<h4 class="anchored" data-anchor-id="derivatives-of-the-sigmoid-activation-function">Derivatives of the sigmoid activation function</h4>
<p>The sigmoid function maps the interval <span class="math inline">\((-\infty, \infty)\)</span> to <span class="math inline">\((0, 1)\)</span>. Some intuition.</p>
<p><span class="math display">\[
\phi(z(t)) = \dfrac{1}{1 + e^{-z(t)}}
\]</span></p>
<p>Let <span class="math inline">\(z\)</span> is some arbitrary function of the variable <span class="math inline">\(t\)</span>. Then the derivative by the chain rule is:</p>
<p><span class="math display">\[
\dfrac{d}{dt} (1 + e^{-z(t)})^{-1} =  \dfrac{ e^{-z(t)} }{ (1 + e^{-z(t)})^2 } \dfrac{d}{dt} z(t)
\]</span></p>
<p>In our case, this will just include computing these extra terms of our layers next tot he derivative.</p>
</section>
<section id="w3-and-mathbfb3" class="level4">
<h4 class="anchored" data-anchor-id="w3-and-mathbfb3"><span class="math inline">\(W^3\)</span> and <span class="math inline">\(\mathbf{b^3}\)</span></h4>
<p>As we don’t apply the sigmoid to the last layer, this is exactly the same as the simple example:</p>
<p><span class="math display">\[
\dfrac{\partial L}{\partial W^3} = (\mathbf{l^2})^T \dfrac{\partial L}{\partial \mathbf{l^3}}
\]</span></p>
<p>Note that the derivative of layer 3 with respect to the bias, <span class="math inline">\(l^3_o = l^2_i W^3_{i, o} + b^3_o\)</span> is <span class="math inline">\(1\)</span>. This makes intuitive sense, when we make a small <span class="math inline">\(\delta b^3_o\)</span> change, it just changes the output of layer 3 exactly by this small change <span class="math inline">\(\delta b^3_o\)</span>, because it is simply added to the output. Therefore:</p>
<p><span class="math display">\[
\dfrac{\partial L}{\partial \mathbf{b^3}} = \dfrac{\partial L}{\partial \mathbf{l^3}}
\]</span></p>
</section>
<section id="w2-and-mathbfb2" class="level4">
<h4 class="anchored" data-anchor-id="w2-and-mathbfb2"><span class="math inline">\(W^2\)</span> and <span class="math inline">\(\mathbf{b^2}\)</span></h4>
<p>Now things get tasty. Unfortunately, the notation gets even more complex, the easiest way to see what is going on is to follow along with a pen and paper, writing out each steps and converting to matrices as you go.</p>
<p>Recall, the same as above, a change in the loss with a change in the weights of layer 2 is:</p>
<p><span class="math display">\[
\dfrac{\partial L}{\partial W^2} = \dfrac{\partial L}{\partial \mathbf{l^3}}  \dfrac{\partial \mathbf{l^3}}{\partial \mathbf{l^2}}  \dfrac{\partial \mathbf{l^2} }{\partial W^2}
\]</span></p>
<section id="dfrac-partial-l-partial-mathbfl2" class="level5">
<h5 class="anchored" data-anchor-id="dfrac-partial-l-partial-mathbfl2"><span class="math inline">\(\dfrac{ \partial L }{ \partial \mathbf{l^2}}\)</span></h5>
<p>Because layer 3 does not have an activation function (and the derivative of the new bias term goes to zero) this is identical to the simple case above:</p>
<p><span class="math display">\[
\dfrac{ \partial L }{ \partial \mathbf{l^2}} = \dfrac{ \partial L }{ \partial \mathbf{l^3}} (W^3)^T
\]</span></p>
</section>
<section id="dfrac-partial-l-partial-w2" class="level5">
<h5 class="anchored" data-anchor-id="dfrac-partial-l-partial-w2"><span class="math inline">\(\dfrac{ \partial L }{ \partial W^2 }\)</span></h5>
<p>Now we are ready to compute the derivative of the loss with respect to the weight matrix of layer 2, and the bias vector. First starting with the weights, looking at each weight in isolation:</p>
<p><span class="math display">\[
\dfrac{ \partial L }{\partial W^2_{i, o}} = \dfrac{ \partial L }{ \partial l^2_o } \dfrac{ \partial l^2_o }{ \partial W^2_{i, o}}
\]</span></p>
<p>We calculated the first term above, so looking at the second term:</p>
<p><span class="math display">\[
\dfrac{ \partial l^2_o }{ \partial W^2_{i, o}} = \dfrac{ \partial }{ \partial W^2_{i, o}} \phi( \sum_n l^1_n W^2_{n, o} + b^2_o)
\]</span></p>
<p>Recalling the derivative of the term inside <span class="math inline">\(\phi\)</span> with respect to the weight <span class="math inline">\(W^2_{i,o}\)</span> is <span class="math inline">\(l^1_i\)</span> as it is <span class="math inline">\(0\)</span> when <span class="math inline">\(n \neq i\)</span>. Letting <span class="math inline">\(\hat{l^2_o} = \sum_i l^1_i W^2_{i, o} + b^2_o\)</span>:</p>
<p><span class="math display">\[
\dfrac{ \partial l^2_o }{ \partial W^2_{i,o}} = \dfrac{ e^{-\hat{l^2_o}} }{ (1 + e^{-\hat{l^2_o}})^2 } \ l^1_i
\]</span></p>
<p>We can implement this in matrix form. <span class="math inline">\(\hat{l^2}\)</span> is a <span class="math inline">\((1, 10)\)</span> vector and so the element-wise multiplication is what we need here (officilally called the <a href="https://en.wikipedia.org/wiki/Hadamard_product_(matrices)">Hadamard product</a> with notation <span class="math inline">\(\circ\)</span>.</p>
<p>So in fact, the derivative looks very similar to before we had the activation function, except now we have this extra term related to its derivative:</p>
<p><span class="math display">\[
\begin{aligned}
    \dfrac{ \partial L }{\partial W^2} = (\mathbf{l^1})^T   \bigg(  \dfrac{ e^{-\mathbf{\hat{l^2}}} }{ (1 + e^{-\mathbf{\hat{l^2}}})^2 }   \circ \dfrac{ \partial L }{ \partial \mathbf{l^2} } \bigg)
\end{aligned}
\]</span></p>
<p><span class="math display">\[
(512, 512) = (512, 1) \times (1, 512) \circ (1, 512)
\]</span></p>
<p>and if we take that derivative with respect to <span class="math inline">\(b_o\)</span> then the term goes to <span class="math inline">\(1\)</span> and dissapears:</p>
<p><span class="math display">\[
    \dfrac{ \partial L }{\partial \mathbf{b^2}} =  \dfrac{ e^{-\mathbf{\hat{l^2}}} }{ (1 + e^{-\mathbf{\hat{l^2}}})^2 } \circ \dfrac{ \partial L }{ \partial \mathbf{l^2} }
\]</span></p>
</section>
</section>
<section id="w1-and-mathbfb1" class="level4">
<h4 class="anchored" data-anchor-id="w1-and-mathbfb1"><span class="math inline">\(W^1\)</span> and <span class="math inline">\(\mathbf{b^1}\)</span></h4>
<p>The derivation for layer 1 follows all the same ideas that we explored for layer 2.</p>
<p><span class="math display">\[
\dfrac{\partial L }{ \partial W^1} = \dfrac{\partial L }{ \partial \mathbf{l^3} } \dfrac{\partial \mathbf{l^3} }{ \partial \mathbf{l^2} } \dfrac{\partial \mathbf{l^2} }{ \partial \mathbf{l^1} } \dfrac{\partial \mathbf{l^1} }{ \partial \mathbf{W^1} }
\]</span></p>
<p>We just need to compute <span class="math inline">\(\dfrac{\partial L }{ \partial \mathbf{l^1} }\)</span> again, now we have the nonlinear activation function to deal with.</p>
</section>
<section id="dfracpartial-l-partial-mathbfl1" class="level4">
<h4 class="anchored" data-anchor-id="dfracpartial-l-partial-mathbfl1"><span class="math inline">\(\dfrac{\partial L}{ \partial \mathbf{l^1} }\)</span></h4>
<p>This will be a <span class="math inline">\((1, 512)\)</span> vector where each element is how the loss changes with a small change in the corresponding unit of layer 1. For each unit in layer 1, we can compute this with the total derivative rule, by looking at the effect of each a small change in a layer 1 unit (input, <span class="math inline">\(i\)</span>) on every unit in layer 2 (output, <span class="math inline">\(o\)</span>) that it is connected to:</p>
<p><span class="math display">\[
\begin{aligned}
\dfrac{\partial L}{\partial l^1_i }  &amp;= \sum_o \dfrac{\partial L}{\partial l^2_o } \dfrac{\partial l^2_o}{\partial l^1_i }
\end{aligned}
\]</span></p>
<p>and note the output of a single layer 2 unit is:</p>
<p><span class="math display">\[
l^2_o = \phi \left( \sum_i l^1_i W^2_{i, o} + b^2_o \right)
\]</span></p>
<p>For the derivative of the term inside the activation function, this is exactly the same as the simple case. We take the derivative of <span class="math inline">\(\sum_i l^1_i W^2_{i, o} + b^2_o\)</span> with respect to a specific input unit <span class="math inline">\(l^1_\hat{i}\)</span> then all terms are zero except for the case where <span class="math inline">\(i = \hat{i}\)</span>. Therefore, the derivative with respect to <span class="math inline">\(l^1_\hat{i}\)</span> is <span class="math inline">\(W^2_{\hat{i}, o}\)</span>.</p>
<p>Here, we will introduce the notation <span class="math inline">\(\mathbf{\hat{l^2}} = \mathbf{l^1}W^2 + \mathbf{b^2}\)</span>, i.e.&nbsp;<span class="math inline">\(\mathbf{\hat{l^2}}\)</span> is the output of layer 2 before we put it through the activation function.</p>
<p>Putting this together with the derivative of the activation function, as above:</p>
<p><span class="math display">\[
\begin{aligned}
\dfrac{\partial L}{\partial l^1_i } &amp;= \sum_o \dfrac{\partial L}{\partial l^2_o } \dfrac{\partial l^2_o}{\partial l^1_i }  \\
&amp;= \sum_o \dfrac{\partial L}{\partial l^2_o }  \dfrac{\partial }{\partial l^1_i } \phi({\hat{l^2_o}} ) \\
&amp;= \sum_o \dfrac{\partial L}{\partial l^2_o } \dfrac{ e^{-\hat{l^2_o}} }{ (1 + e^{-\hat{l^2_o}})^2 } W^2_{i, o}
\end{aligned}
\]</span></p>
<p>Intuitively, this makes sense. XXX.</p>
<p>So we can implement this in vector form as:</p>
<p><span class="math display">\[
\dfrac{ \partial L }{ \partial \mathbf{l^1} } = \bigg( \dfrac{ \partial L }{ \partial \mathbf{l^2} }  \circ  \dfrac{ e^{-\mathbf{\hat{l^2}}} }{ (1 + e^{-\mathbf{\hat{l^2}}})^2 } \bigg) (W^2)^T
\]</span></p>
<p>The easiest way to see this vector form implementation is to write out all the terms with pen and paper and follow them through. MAKE CLEAR THE EPONENTIAL TERM IS A VECTOR AND WE APPLY THE EXPON INDIVIDUALLY.</p>
</section>
<section id="so-putting-it-all-together-for-w1-and-mathbfb1" class="level4">
<h4 class="anchored" data-anchor-id="so-putting-it-all-together-for-w1-and-mathbfb1">So putting it all together for <span class="math inline">\(W^1\)</span> and <span class="math inline">\(\mathbf{b^1}\)</span></h4>
<p>Again, we use <span class="math inline">\(\hat{\mathbf{l^1}} = \mathbf{x}W^1 + \mathbf{b^1}\)</span> for the output of layer 1 before inputting to the activation function.</p>
<p>So as in the simple case, we will go through these piece-by-piece:</p>
<p><span class="math display">\[
\begin{aligned}
    \mathbf{g_1} &amp;= \dfrac{\partial L }{ \partial \mathbf{l^2} } = \dfrac{\partial L }{ \partial \mathbf{l^3} } \dfrac{\partial \mathbf{l^3} }{ \partial \mathbf{l^2} }  \\
    &amp;=  \dfrac{ \partial L }{ \partial \mathbf{l^3} } (W^3)^T
\end{aligned}
\]</span></p>
<p><span class="math inline">\((1, 512) = (1, 10) \circ (1, 10) \times (10, 512)\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{g_2} &amp;= \dfrac{ \partial L }{ \partial \mathbf{l^1 }} \\
&amp;= \mathbf{g_1} \dfrac{\partial \mathbf{l^2} }{ \partial \mathbf{l^1} } \\
&amp;= \bigg( \mathbf{g_1} \circ \dfrac{ e^{-\hat{\mathbf{l^2}} } }{ (1 + e^{-\hat{\mathbf{l^2}}})^2 } \bigg) (W^2)^T
\end{aligned}
\]</span></p>
<p><span class="math inline">\((1, 512) = (1, 512) \circ (1, 512) \times (512, 512)\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\dfrac{\partial L}{ W^1} &amp;= \mathbf{g_2} \dfrac{\partial \mathbf{l^1} }{ \partial W^1 }\\
&amp;= \mathbf{x}^T \bigg( \dfrac{ e^{-\hat{\mathbf{l^1}} } }{ (1 + e^{-\hat{\mathbf{l^1}} })^2 } \circ \mathbf{g_2} \bigg)
\end{aligned}
\]</span></p>
<p><span class="math display">\[
(784, 512) = (784, 1) \times (1, 512) * (1, 512)
\]</span></p>
<p>When taking the derivative with respect to the bias, all steps are the same except the last step <span class="math inline">\(\mathbf{x}\)</span> is <span class="math inline">\(1\)</span> (to see this, take the derivative of <span class="math inline">\(l^1_o = x_i W^1_{i, o} + b^1_o\)</span> with respect to <span class="math inline">\(b_0\)</span> or <span class="math inline">\(W^1_{i,o}\)</span>):</p>
<p><span class="math display">\[
\dfrac{ \partial L}{ \partial \mathbf{b^1}} = \dfrac{ e^{-\hat{\mathbf{l^1}} } }{ (1 + e^{-\hat{\mathbf{l^1}} })^2 } \circ \mathbf{g_2}
\]</span></p>
<p>TODO: add total derivative appendix TODO: add some pictures of neurons TODO: check all writing</p>
<p>In sum, thank god for autodiff!</p>
<div id="a1fdbd87" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>run <span class="op">=</span> <span class="va">False</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MyBetterNetwork:</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, learning_rate<span class="op">=</span><span class="fl">0.02</span>):</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.a <span class="op">=</span> learning_rate</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Define weight matrix (output dim, input dim) by convention</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use zero-mean Xavier init (good for sigmoid)</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This makes a huge differences vs uniform.</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W1 <span class="op">=</span> np.random.randn(<span class="dv">784</span>, <span class="dv">512</span>) <span class="op">*</span> np.sqrt(<span class="dv">1</span> <span class="op">/</span> <span class="dv">784</span>)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W2 <span class="op">=</span> np.random.randn(<span class="dv">512</span>, <span class="dv">512</span>) <span class="op">*</span> np.sqrt(<span class="dv">1</span> <span class="op">/</span> <span class="dv">512</span>)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W3 <span class="op">=</span> np.random.randn(<span class="dv">512</span>, <span class="dv">10</span>) <span class="op">*</span> np.sqrt(<span class="dv">1</span> <span class="op">/</span> <span class="dv">512</span>)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b1 <span class="op">=</span> np.zeros((<span class="dv">1</span>, <span class="dv">512</span>))</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b2 <span class="op">=</span> np.zeros((<span class="dv">1</span>, <span class="dv">512</span>))</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b3 <span class="op">=</span> np.zeros((<span class="dv">1</span>, <span class="dv">10</span>))</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss(<span class="va">self</span>, l3, y):</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> <span class="va">self</span>.softmax(l3)[<span class="dv">0</span>][y]</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="op">-</span>np.log( p <span class="op">+</span> <span class="fl">1e-15</span>)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> softmax(<span class="va">self</span>, vec):</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>        C <span class="op">=</span> np.<span class="bu">max</span>(vec)</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.exp(vec <span class="op">-</span> C) <span class="op">/</span> np.<span class="bu">sum</span>(np.exp(vec <span class="op">-</span> C))</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># forward pass through the network</span></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.reshape(<span class="dv">1</span>, x.size)</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>        l1_hat <span class="op">=</span> x <span class="op">@</span> <span class="va">self</span>.W1 <span class="op">+</span> <span class="va">self</span>.b1</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>        l1 <span class="op">=</span> <span class="va">self</span>.phi(l1_hat)</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>        l2_hat <span class="op">=</span> l1 <span class="op">@</span> <span class="va">self</span>.W2 <span class="op">+</span> <span class="va">self</span>.b2</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>        l2 <span class="op">=</span> <span class="va">self</span>.phi(l2_hat)</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>        l3 <span class="op">=</span> l2 <span class="op">@</span> <span class="va">self</span>.W3 <span class="op">+</span> <span class="va">self</span>.b3</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> np.argmax(<span class="va">self</span>.softmax(l3))</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pred, l1_hat, l1, l2_hat, l2, l3</span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> phi(<span class="va">self</span>, vec):</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>vec))</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> dphi_dvec(<span class="va">self</span>, vec):</span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.exp(<span class="op">-</span>vec) <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>vec))<span class="op">**</span><span class="dv">2</span></span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_weights(<span class="va">self</span>, x, y, verbose<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.reshape(<span class="dv">1</span>, x.size)</span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>        _, l1_hat, l1, l2_hat, l2, l3 <span class="op">=</span> <span class="va">self</span>.predict(x)</span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="va">self</span>.loss(l3, y)</span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> verbose:</span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Loss: </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the derivatives</span></span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a>        dloss_dl3 <span class="op">=</span> <span class="va">self</span>.softmax(l3)  <span class="co"># double check this</span></span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a>        dloss_dl3[<span class="dv">0</span>][y] <span class="op">-=</span> <span class="dv">1</span></span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a>        dloss_dW3 <span class="op">=</span> l2.T <span class="op">@</span> dloss_dl3</span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a>        dloss_db3 <span class="op">=</span> dloss_dl3</span>
<span id="cb13-67"><a href="#cb13-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-68"><a href="#cb13-68" aria-hidden="true" tabindex="-1"></a>        dloss_dl2 <span class="op">=</span> dloss_dl3 <span class="op">@</span> <span class="va">self</span>.W3.T                               <span class="co"># (1, 512) = (1, 10) x (10, 512)</span></span>
<span id="cb13-69"><a href="#cb13-69" aria-hidden="true" tabindex="-1"></a>        dloss_dW2 <span class="op">=</span> l1.T <span class="op">@</span> (<span class="va">self</span>.dphi_dvec(l2_hat) <span class="op">*</span> dloss_dl2)         <span class="co"># (512, 512) = (512, 1) x (1, 512) * (1, 512)</span></span>
<span id="cb13-70"><a href="#cb13-70" aria-hidden="true" tabindex="-1"></a>        dloss_db2 <span class="op">=</span> <span class="va">self</span>.dphi_dvec(l2_hat) <span class="op">*</span> dloss_dl2                  <span class="co"># (1, 512) = (512, 1) x (1, 512)</span></span>
<span id="cb13-71"><a href="#cb13-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-72"><a href="#cb13-72" aria-hidden="true" tabindex="-1"></a>        dloss_dl1 <span class="op">=</span> (dloss_dl2 <span class="op">*</span> <span class="va">self</span>.dphi_dvec(l2_hat)) <span class="op">@</span> <span class="va">self</span>.W2.T    <span class="co"># (1, 512) = (1, 512) * (1, 512) x (512, 512)</span></span>
<span id="cb13-73"><a href="#cb13-73" aria-hidden="true" tabindex="-1"></a>        dloss_dW1 <span class="op">=</span> x.T <span class="op">@</span> (<span class="va">self</span>.dphi_dvec(l1_hat) <span class="op">*</span> dloss_dl1)          <span class="co"># (784, 512) = (784, 1) x (1, 512) * (1, 512)</span></span>
<span id="cb13-74"><a href="#cb13-74" aria-hidden="true" tabindex="-1"></a>        dloss_db1 <span class="op">=</span> <span class="va">self</span>.dphi_dvec(l1_hat) <span class="op">*</span> dloss_dl1                  <span class="co"># (1, 512) = (1, 512) * (1, 512)</span></span>
<span id="cb13-75"><a href="#cb13-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-76"><a href="#cb13-76" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W3 <span class="op">-=</span> <span class="va">self</span>.a <span class="op">*</span> dloss_dW3</span>
<span id="cb13-77"><a href="#cb13-77" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W2 <span class="op">-=</span> <span class="va">self</span>.a <span class="op">*</span> dloss_dW2</span>
<span id="cb13-78"><a href="#cb13-78" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W1 <span class="op">-=</span> <span class="va">self</span>.a <span class="op">*</span> dloss_dW1</span>
<span id="cb13-79"><a href="#cb13-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-80"><a href="#cb13-80" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b3 <span class="op">-=</span> <span class="va">self</span>.a <span class="op">*</span> dloss_db3</span>
<span id="cb13-81"><a href="#cb13-81" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b2 <span class="op">-=</span> <span class="va">self</span>.a <span class="op">*</span> dloss_db2</span>
<span id="cb13-82"><a href="#cb13-82" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b1 <span class="op">-=</span> <span class="va">self</span>.a <span class="op">*</span> dloss_db1</span>
<span id="cb13-83"><a href="#cb13-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-84"><a href="#cb13-84" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialise and train the model (no batching)</span></span>
<span id="cb13-85"><a href="#cb13-85" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> MyBetterNetwork()</span>
<span id="cb13-86"><a href="#cb13-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-87"><a href="#cb13-87" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (X, y) <span class="kw">in</span> <span class="bu">enumerate</span>(training_data):</span>
<span id="cb13-88"><a href="#cb13-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-89"><a href="#cb13-89" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.asarray(X[<span class="dv">0</span>, :, :])</span>
<span id="cb13-90"><a href="#cb13-90" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> <span class="bu">int</span>(y)</span>
<span id="cb13-91"><a href="#cb13-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-92"><a href="#cb13-92" aria-hidden="true" tabindex="-1"></a>    model.update_weights(x, y, verbose<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb13-93"><a href="#cb13-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-94"><a href="#cb13-94" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb13-95"><a href="#cb13-95" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Training iteration: sample: </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-96"><a href="#cb13-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-97"><a href="#cb13-97" aria-hidden="true" tabindex="-1"></a><span class="co"># We won't run this here because it is very slow,</span></span>
<span id="cb13-98"><a href="#cb13-98" aria-hidden="true" tabindex="-1"></a><span class="co"># but it gives an accuracy of ~83%</span></span>
<span id="cb13-99"><a href="#cb13-99" aria-hidden="true" tabindex="-1"></a><span class="co"># Check the model accuracy</span></span>
<span id="cb13-100"><a href="#cb13-100" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> run:</span>
<span id="cb13-101"><a href="#cb13-101" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> np.empty(<span class="bu">len</span>(test_data))</span>
<span id="cb13-102"><a href="#cb13-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-103"><a href="#cb13-103" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (X, y) <span class="kw">in</span> <span class="bu">enumerate</span>(test_data):</span>
<span id="cb13-104"><a href="#cb13-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-105"><a href="#cb13-105" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> np.asarray(X[<span class="dv">0</span>, :, :])</span>
<span id="cb13-106"><a href="#cb13-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-107"><a href="#cb13-107" aria-hidden="true" tabindex="-1"></a>        results[i] <span class="op">=</span> model.predict(x)[<span class="dv">0</span>] <span class="op">==</span> y</span>
<span id="cb13-108"><a href="#cb13-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-109"><a href="#cb13-109" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Percent Correct: </span><span class="sc">{</span>np<span class="sc">.</span>mean(results) <span class="op">*</span> <span class="dv">100</span><span class="sc">}</span><span class="ss">%"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p><strong>Appendix 1</strong></p>
<p>The cross entropy loss is:</p>
<span class="math display">\[\begin{aligned}
    L &amp;= -\log \dfrac{ \exp{ l_{3, y} } }{ \sum_k \exp{ l_{3, k} } } \\
    &amp;= -\bigg[ \log \exp{ l_{3, y} } - \log \sum_k \exp{ l_{3, k} } \bigg] \\
    &amp;=  \log \sum_k \exp{ l_{3, k} } - l_{3, y}
\end{aligned}\]</span>
<p>(by the log laws). i.e.&nbsp;we take the logit of layer 3 that matches the correct label <span class="math inline">\(y\)</span>, normalise it to a probability with the softmax function and take the negative log.</p>
<p>Let’s start by taking the derivative with respect to <span class="math inline">\(l_{3, y}\)</span> where this is shorthand for <span class="math inline">\(l_{3, i}\)</span>, <span class="math inline">\(i=y\)</span> i.e. the layer 3 logit for the label that is correct for this image. We are asking: how does a small change in this logit effect the loss?</p>
<p><span class="math display">\[
\begin{aligned}
\dfrac{ \partial L }{ \partial l_{3, y} } &amp;= \dfrac{ \partial }{ \partial l_{3, y} } \left( \log \sum_k \exp{ l_{3, k} } - l_{3, y} \right)
&amp;=  \dfrac{ \partial }{ \partial l_{3, y} } \log \sum_k \exp{ l_{3, k} } - \dfrac{ \partial }{ \partial l_{3, y} }  l_{3, y}  \\
&amp;= \dfrac{1}{ \sum_k \exp{ l_{3, k} } }  \dfrac{ \partial }{ \partial l_{3, y} } \sum_k \exp{ l_{3, k} } - 1
\end{aligned}
\]</span></p>
<p>(by the derivative of <span class="math inline">\(\log x\)</span> rule). Note that the last term will be <span class="math inline">\(0\)</span> when the the input dimension is not <span class="math inline">\(y\)</span> (because it is treated as a scalar).</p>
<p>We see that in the sum, the derivative of $ $ w.r.t <span class="math inline">\(l_{3, k}\)</span> is <span class="math inline">\(\exp{ l_{3, i} }\)</span> when <span class="math inline">\(i = k\)</span> and <span class="math inline">\(0\)</span> otherwise (as it is treated as a scalar). So whatever dimension <span class="math inline">\(i\)</span> of <span class="math inline">\(l_3\)</span>, we will input to the loss, we get <span class="math inline">\(\text{softmax}(\mathbf{l_3})_i\)</span> as the first term. But only when <span class="math inline">\(i = y\)</span> do we get <span class="math inline">\(-1\)</span> in the second term.</p>
<p><strong>Appendix 2</strong></p>
<p>In multivariate calculus, the total derivative rule captures how a function changes with respect to a variable that affects the function in multiple ways (through multiple intermediary functions).</p>
<p>For example, let’s say we have the variable <span class="math inline">\(t\)</span> and let:</p>
<p><span class="math display">\[
w = f(x(t), y(t))
\]</span></p>
<p>The total derivative rule tells us how the output, <span class="math inline">\(w\)</span> changes with a small change in <span class="math inline">\(t\)</span>. Intuitively, it is the sum of how <span class="math inline">\(\delta t\)</span> changes <span class="math inline">\(w\)</span> through <span class="math inline">\(x(t)\)</span> and how <span class="math inline">\(\delta t\)</span> changes <span class="math inline">\(w\)</span> through <span class="math inline">\(y(t)\)</span>:</p>
<p><span class="math display">\[
\dfrac{dw}{dt} = \dfrac{\partial w}{ \partial x(t)} \dfrac{\partial x(t)}{ \partial t} + \dfrac{\partial w}{ \partial y(t)} \dfrac{\partial y(t)}{ \partial t}
\]</span></p>
<p>Note this is exactly analogous to our set up with the layers and weights. A small change in a unit in layer 2 will effect the loss through every unit in layer 3. Let’s look at unit 1 from layer 2 (<span class="math inline">\(l^2_1\)</span>) as an example. Here layer 3 units are a function taking <span class="math inline">\(l^2_1\)</span>, which is like <span class="math inline">\(t\)</span> in the above example, as an input:</p>
<p><span class="math display">\[
\text{loss} = L( \ l^3_1(l^2_1), \ l^3_2(l^2_1), \ ..., \ l^3_{10}(l^2_1) \ )
\]</span></p>
<p><span class="math display">\[
\dfrac{\partial L}{ \partial l^2_1 } = \dfrac{\partial L}{ \partial l^3_1 } \dfrac{ \partial l^3_1}{ \partial l^2_1 } + \dfrac{\partial L}{ \partial l^3_2 } \dfrac{\partial l^3_2 }{ \partial l^2_1 } + ... + \dfrac{\partial L}{ \partial l^3_{10} } \dfrac{\partial l^3_{10} }{ \partial l^2_1 }
\]</span></p>
<p>Which is exactly what we do to deal with these derivatives.</p>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>