<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Introduction to Neural Networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="nn-introduction_files/libs/clipboard/clipboard.min.js"></script>
<script src="nn-introduction_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="nn-introduction_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="nn-introduction_files/libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="nn-introduction_files/libs/quarto-html/popper.min.js"></script>
<script src="nn-introduction_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="nn-introduction_files/libs/quarto-html/anchor.min.js"></script>
<link href="nn-introduction_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="nn-introduction_files/libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="nn-introduction_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="nn-introduction_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="nn-introduction_files/libs/bootstrap/bootstrap-9e3ffae467580fdb927a41352e75a2e0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Introduction to Neural Networks</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Neural networks are a very flexible model through which to learn complex, high dimensional functions. First, we will apply the model in Pytorch, and then implement it by hand, using multivariable calculus to derive the parameter updates.</p>
<p>In this example, we will apply a neural network to an image classification problem. We have a dataset containing images clothes, with each image having a label describing the item of clothing in the image. Each <span class="math inline">\(28 \times 28\)</span>-pixel image is represented as a length <span class="math inline">\(784\)</span> vector <span class="math inline">\(\mathbf{x}\)</span>. We use bold font to indicate a vector. There are 10 possible items of clothing (e.g.&nbsp;t-shirt, hat) in our dataset, so <span class="math inline">\(y\)</span> is an integer in the range <span class="math inline">\([0, 10]\)</span>. Together, our sample space is pairings of images and labels, <span class="math inline">\((\mathbf{x}_i, y_i) \sim (\mathcal{X}, \mathcal{Y})\)</span> (i.e.&nbsp;a single image, label pair index by <span class="math inline">\(i\)</span> can be drawn from the sample space of all pairs of images and labels). s</p>
<p>We want to learn the conditional distribution <span class="math inline">\(p(y| \mathbf{x})\)</span> i.e.&nbsp;what is the probability of the label <span class="math inline">\(y\)</span> given an image, <span class="math inline">\(\mathbf{x}\)</span>. For example, what is the probability this is an image <span class="math inline">\(\mathbf{x}\)</span> is of a t-shirt? In this case, we want to learn a function <span class="math inline">\(f(\mathbf{x}) : \mathbf{x} \rightarrow \mathbf{y}\)</span>. It will take a vector of length <span class="math inline">\(784\)</span> and output a vector of length <span class="math inline">\(10\)</span>, with each element of the output vector assigning some weight related to the probability of image <span class="math inline">\(\mathbf{x}\)</span> being a particular label <span class="math inline">\(y\)</span>. These unnormalised weights output by the model are called ‘logits’.</p>
<p align="center">
<img src="./nn-introduction/nn-image-1.png" width="400">
</p>
<p>In this example, we will first follow the <a href="https://docs.pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html">Pytorch</a> ‘Getting Started’ page to train a simple model for this classification task. Then, we will get a deeper grip on how these models work by implementing the model ourselves in Numpy, which requires calculating the derivatives required for model training ourselves.</p>
<section id="coding-up-a-neural-network-in-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="coding-up-a-neural-network-in-pytorch">Coding up a Neural Network in Pytorch</h2>
<p>This section exactly follows <a href="https://docs.pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html">the Pytorch introduction</a>.</p>
<section id="the-data" class="level3">
<h3 class="anchored" data-anchor-id="the-data">The data</h3>
<p>We will use the FashionMINST dataset, containing <span class="math inline">\(28 \times 28\)</span> images of clothing, 10 possible labels. In total there are <span class="math inline">\(60,000\)</span> image sin the training set and <span class="math inline">\(10,000\)</span> images in the test set.</p>
<p align="center">
<img src="./nn-introduction/FashionMNIST.png" width="500">
</p>
<div id="4c808e140659e8f8" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2025-12-17T01:17:05.924223Z&quot;,&quot;start_time&quot;:&quot;2025-12-17T01:17:05.877298Z&quot;}}" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>capture</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># DataLoader is an iterable around DataSet, which stores samples and their corresponding labels.</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> datasets</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms <span class="im">import</span> ToTensor</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>training_data <span class="op">=</span> datasets.FashionMNIST(</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span><span class="st">"data"</span>,  <span class="co"># root directory</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    train<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    transform<span class="op">=</span>ToTensor()</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>test_data <span class="op">=</span> datasets.FashionMNIST(</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span><span class="st">"data"</span>,</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    train<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    transform<span class="op">=</span>ToTensor()</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="aaf7d687" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2025-12-17T01:17:05.946413Z&quot;,&quot;start_time&quot;:&quot;2025-12-17T01:17:05.930643Z&quot;}}" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create data loaders</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader(training_data, batch_size<span class="op">=</span>batch_size)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>test_dataloader <span class="op">=</span> DataLoader(test_data, batch_size<span class="op">=</span>batch_size)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> X, y <span class="kw">in</span> test_dataloader:</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Shape of X [N, C, H, W]: </span><span class="sc">{</span>X<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Shape of y: </span><span class="sc">{</span>y<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> </span><span class="sc">{</span>y<span class="sc">.</span>dtype<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">break</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])
Shape of y: torch.Size([64]) torch.int64</code></pre>
</div>
</div>
<p>Our data is a 4D array with dimensions: <span class="math display">\[
\text{(batch size, num channels (RGB), image height, image width)}
\]</span></p>
<p>Each image in the batch has a single label (e.g.&nbsp;<span class="math inline">\(y = \text{t-shirt}\)</span>), so we have <span class="math inline">\(64\)</span> labels per-batch.</p>
<p><strong>Mini-batch</strong> training is a way to reduce the noise associated with updating the parameters after running a single image through the network. During mini-batch training, the mini-batch of images (here <span class="math inline">\(64\)</span> images) is passed through the network without updating any parameter. The average loss over the entire batch is calculated.</p>
<p>Next, backpopagation proceeds by takes the derivative of this average loss with respect to the parameters. This gives a lower-variance estimate of the gradients than using a single sample.</p>
<p>An <strong>Epoch</strong> is one pass over the dataset.</p>
</section>
<section id="the-model" class="level3">
<h3 class="anchored" data-anchor-id="the-model">The Model</h3>
<div id="b0e4b7b8959226d1" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2025-12-17T01:17:05.966632Z&quot;,&quot;start_time&quot;:&quot;2025-12-17T01:17:05.955158Z&quot;}}" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Using </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NeuralNetwork(nn.Module):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Linear layers are:  yᵢ = φ( ∑ⱼ xⱼ Wᵀⱼᵢ + bᵢ ) so together y = φ(xW^T + b)</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># where W is a (output_features, input_features) set of weights, b is vector of biases</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># so we can easily control output shape. Layers are fully connected.</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_relu_stack <span class="op">=</span> nn.Sequential(</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>, <span class="dv">512</span>),</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, <span class="dv">512</span>),</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, <span class="dv">10</span>)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.linear_relu_stack(x)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> NeuralNetwork().to(device)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Using cuda
NeuralNetwork(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=784, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=10, bias=True)
  )
)</code></pre>
</div>
</div>
<section id="optimising-the-model" class="level4">
<h4 class="anchored" data-anchor-id="optimising-the-model">Optimising the Model</h4>
<p>We use the cross-entropy loss (going into this in more detail in the next section).</p>
<div id="8970d571e3f7ae62" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2025-12-17T01:17:05.982365Z&quot;,&quot;start_time&quot;:&quot;2025-12-17T01:17:05.977660Z&quot;}}" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># We can see we have Weights, Biases, Weights, Biases, Weights, Biases (3 layers)</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The model parameters"</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>      <span class="bu">print</span>(</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f"Name: </span><span class="sc">{</span>name<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f"Type: </span><span class="sc">{</span><span class="bu">type</span>(param)<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f"Size: </span><span class="sc">{</span>param<span class="sc">.</span>size()<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>      )</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>The model parameters
Name: linear_relu_stack.0.weight
Type: &lt;class 'torch.nn.parameter.Parameter'&gt;
Size: torch.Size([512, 784])

Name: linear_relu_stack.0.bias
Type: &lt;class 'torch.nn.parameter.Parameter'&gt;
Size: torch.Size([512])

Name: linear_relu_stack.2.weight
Type: &lt;class 'torch.nn.parameter.Parameter'&gt;
Size: torch.Size([512, 512])

Name: linear_relu_stack.2.bias
Type: &lt;class 'torch.nn.parameter.Parameter'&gt;
Size: torch.Size([512])

Name: linear_relu_stack.4.weight
Type: &lt;class 'torch.nn.parameter.Parameter'&gt;
Size: torch.Size([10, 512])

Name: linear_relu_stack.4.bias
Type: &lt;class 'torch.nn.parameter.Parameter'&gt;
Size: torch.Size([10])
</code></pre>
</div>
</div>
</section>
</section>
<section id="training-the-model" class="level3">
<h3 class="anchored" data-anchor-id="training-the-model">Training the Model</h3>
<p>We train the model over the (batched) training data, updating the parametrs based on the derivative of the loss with respect to the parameters.</p>
<div id="da357b6fe6802954" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2025-12-17T01:17:14.011311Z&quot;,&quot;start_time&quot;:&quot;2025-12-17T01:17:05.997031Z&quot;}}" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(dataloader, model, loss_fn, optimizer):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    size <span class="op">=</span> <span class="bu">len</span>(dataloader.dataset)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    model.train(mode<span class="op">=</span><span class="va">True</span>)  <span class="co"># put into 'training mode'</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch, (X, y) <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        X, y <span class="op">=</span> X.to(device), y.to(device)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pred is (64, 10) tuple of predictions for this batch</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># y is (64, 1) (classes)</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Cross entropy loss https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> model(X)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(pred, y)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>        optimizer.step()  <span class="co"># perform one step θt &lt;- f(θ_{t-1})</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()  <span class="co"># zero the accumulated gradients, ready for the next step</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>train(train_dataloader, model, loss_fn, optimizer)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="testing-the-model" class="level3">
<h3 class="anchored" data-anchor-id="testing-the-model">Testing the Model</h3>
<p>Now run the model over the test data and compute the accuracy:</p>
<div id="7d2a9d6f22016a93" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2025-12-17T01:18:17.002612Z&quot;,&quot;start_time&quot;:&quot;2025-12-17T01:17:14.028427Z&quot;}}" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test(dataloader, model, loss_fn):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">""""""</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    size <span class="op">=</span> <span class="bu">len</span>(dataloader.dataset)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    num_batches <span class="op">=</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()  <span class="co"># Go from train to eval mode</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    test_loss, correct <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():  <span class="co"># this just turns of gradient computation for speed</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> batch, (X, y) <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>            X, y <span class="op">=</span> X.to(device), y.to(device)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>            pred <span class="op">=</span> model(X)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># pred_i = torch.argmax(torch.exp(pred) / torch.sum(torch.exp(pred)), axis=1)</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>            pred_i <span class="op">=</span> pred.argmax(<span class="dv">1</span>)  <span class="co"># of course, it doesn't matter if the logits are passed through softmax, which maintains transitivity</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>            correct <span class="op">+=</span> (pred_i <span class="op">==</span> y).<span class="bu">type</span>(torch.<span class="bu">float</span>).<span class="bu">sum</span>().item()</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>            test_loss <span class="op">+=</span> loss_fn(pred, y)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>        test_loss <span class="op">/=</span> num_batches</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>        correct <span class="op">/=</span> size</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Test Error: </span><span class="ch">\n</span><span class="ss"> Accuracy: </span><span class="sc">{</span>(<span class="dv">100</span><span class="op">*</span>correct)<span class="sc">:&gt;0.1f}</span><span class="ss">%, Avg loss: </span><span class="sc">{</span>test_loss<span class="sc">:&gt;8f}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Epoch 1"</span>)</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>test(test_dataloader, model, loss_fn)</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i <span class="op">+</span> <span class="dv">2</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>    train(train_dataloader, model, loss_fn, optimizer)</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>    test(test_dataloader, model, loss_fn)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1
Test Error: 
 Accuracy: 48.6%, Avg loss: 2.163036 

Epoch 2
Test Error: 
 Accuracy: 55.4%, Avg loss: 1.900973 

Epoch 3
Test Error: 
 Accuracy: 61.5%, Avg loss: 1.528527 

Epoch 4
Test Error: 
 Accuracy: 64.0%, Avg loss: 1.254724 

Epoch 5
Test Error: 
 Accuracy: 65.1%, Avg loss: 1.087651 

Epoch 6
Test Error: 
 Accuracy: 66.1%, Avg loss: 0.980660 
</code></pre>
</div>
</div>
<p>What is the Model Learning?</p>
<ol type="1">
<li>We are learning p(y|x). We are not learning p(y, x), p(x), p(y) etc.</li>
<li>One interpretation is learning dcision boundaries across a 784 dimension space</li>
<li>another inteprestation is our weight matrices are learning 10 templates, but not quite</li>
<li>link the CS module</li>
</ol>
<p>TODO: https://arxiv.org/abs/1311.2901 http://neuralnetworksanddeeplearning.com/ https://cs231n.github.io/linear-classify/ https://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf https://www.jasonosajima.com/backprop.html + the initialisation paper</p>
</section>
</section>
<section id="training-a-neural-network-by-hand---a-simple-example" class="level2">
<h2 class="anchored" data-anchor-id="training-a-neural-network-by-hand---a-simple-example">Training a Neural Network by Hand - A ‘Simple’ Example</h2>
<p>Now, we will implement the same model that we created in Python, but this time ‘by-hand’ in Numpy. To do this, we will have to calculate the derivatives of the parameters with respect to the loss function, in order to update them during training. First, we will review a simplified version of the model that contains only weights, but no bias or nonlinear functions.</p>
<section id="the-model-1" class="level3">
<h3 class="anchored" data-anchor-id="the-model-1">The Model</h3>
Let <span class="math inline">\(l^1\)</span>, <span class="math inline">\(l^2\)</span> and <span class="math inline">\(l^3\)</span> be vector-valued functions representing the layers <span class="math inline">\(1\)</span>, <span class="math inline">\(2\)</span>, <span class="math inline">\(3\)</span> of the network. <span class="math inline">\(l^1\)</span> takes our length <span class="math inline">\(784\)</span> row vector <span class="math inline">\(\mathbf{x}\)</span> and returns a length <span class="math inline">\(512\)</span> vector. <span class="math inline">\(l^2\)</span> takes this length <span class="math inline">\(512\)</span> vector as input and returns another length <span class="math inline">\(512\)</span> vector. Our final layer <span class="math inline">\(l^3\)</span> takes this a length <span class="math inline">\(512\)</span> vector as input and outputs a length <span class="math inline">\(10\)</span> vector.
<span class="math display">\[\begin{aligned}
    &amp;l^1(\mathbf{x}) = \mathbf{x}W^1 \ \ \ \ \text{(1, 784) x (784, 512) = (1, 512)} \\
    &amp;l^2(\mathbf{l^1}) = \mathbf{l^1}W^2 \ \ \ \ \text{(1, 512) x (512, 512) = (1, 512)} \\
    &amp;l^3(\mathbf{l^2}) = \mathbf{l^2}W^3 \ \ \ \ \text{(1, 512) x (512, 10) = (1, 10)}
\end{aligned}\]</span>
<p>All vectors are row vectors. The <span class="math inline">\(W\)</span> are our matrices of weights with shape (input, output): <span class="math inline">\(W^1\)</span> is <span class="math inline">\((784, 512)\)</span>, <span class="math inline">\(W^2\)</span> is <span class="math inline">\((512, 512)\)</span> and <span class="math inline">\(W^3\)</span> is <span class="math inline">\((512, 10)\)</span>. We can therefore index individual weights as <span class="math inline">\(W^3_{i, o}\)</span> where <span class="math inline">\(i\)</span> is the index of the input node (i.e.&nbsp;the node the weight connects from), and <span class="math inline">\(o\)</span> is the index of the output node (i.e.&nbsp;the node the weight connects to).</p>
<p align="center">
<img src="./nn-introduction/nn-image-labelled.png" width="400">
</p>
<p>Note that in practice this ‘network’ reduces to a single linear classifier of shape <span class="math inline">\((784, 10)\)</span>, so of course in reality we would never structure it like this. However, we will do it in this way first as a nice example.</p>
<p><em>(An aside)</em>: This notation is a little non-standard (often vectors are column vectors, and the weight matrix is shape (output, input) but this notation makes the derivations below much simpler. Also the layers are functions <span class="math inline">\(l^3(\cdot)\)</span> but can be treated as vectors <span class="math inline">\(\mathbf{l^3}\)</span> when evaluated. We will typically write them as vectors.</p>
</section>
<section id="the-loss" class="level3">
<h3 class="anchored" data-anchor-id="the-loss">The Loss</h3>
<p>The <span class="math inline">\((1, 10)\)</span> vector of logits output from the final layer (also called ‘scores’, one for each label) is used to compute the loss. We will use the cross-entropy loss:</p>
<p><span class="math display">\[
L(\mathbf{l^3}, y) = -\log \dfrac{ \exp{ l^3_y }}{ \sum_k \exp{ l^3_k }}
\]</span></p>
<p>Here <span class="math inline">\(l^3_n\)</span> is the <span class="math inline">\(n\)</span>th element of our vector of logits, and <span class="math inline">\(y\)</span> is the index of the correct label for this image. The term after the <span class="math inline">\(\log\)</span> is the <a href="https://en.wikipedia.org/wiki/Softmax_function#:~:text=The%20softmax%20function%2C%20also%20known,used%20in%20multinomial%20logistic%20regression">softmax function</a> which normalises the logits to probabilities. Therefore, given an input image <span class="math inline">\(\mathbf{x}\)</span> the model returns a probability distribution over the labels <span class="math inline">\(y\)</span>. <a href="https://cs231n.github.io/linear-classify/">This article</a> has a nice, deeper discussion of the Cross Entropy Loss.</p>
<p>Clearly, we want this probability to be high (ideally <span class="math inline">\(1\)</span> for the correct label, <span class="math inline">\(0\)</span> for every other label). So this loss makes intuitive sense, we are maximising the probability the network returns for the correct label. Here, we equivalently minimise the negative log probability.</p>
</section>
<section id="predicting-a-label-from-an-image---the-forward-pass" class="level3">
<h3 class="anchored" data-anchor-id="predicting-a-label-from-an-image---the-forward-pass">Predicting a label from an image - the forward pass</h3>
<p>To predict a label for an image <span class="math inline">\(\mathbf{x}\)</span>, we apply the defined model to compute the logits:</p>
<span class="math display">\[\begin{aligned}
\mathbf{l^3} &amp;= l^3(l^2(l^1(\mathbf{x}))) \\
             &amp;= ((\mathbf{x}W^1) W^2) W^3 \\
             &amp;=  \mathbf{x}W^1 W^2 W^3 \\
\end{aligned}\]</span>
<p>The predicted label is the one that maximises the probability as computed by the softmax function:</p>
<p><span class="math display">\[
\hat{y} = \arg\max_{y} \, \mathrm{softmax}(\mathbf{l^3})_y
\]</span></p>
<p>In other words, the predicted label is the one the model assigns the highest probability to (recall <span class="math inline">\(y\)</span> is the index of the correct label).</p>
</section>
<section id="backpropagation-in-our-simple-network" class="level3">
<h3 class="anchored" data-anchor-id="backpropagation-in-our-simple-network">Backpropagation in our simple network</h3>
<p>We want to find the set of weights that minimise the loss function. The loss function is a multivariate function over the space of parameters. Therefore, we can minimise the loss with respect to the parameters through <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a>. For this, we need to compute the derivative of the loss with respect to the parameters.</p>
<p>For example, take <span class="math inline">\(W^3_{1,2}\)</span> that connects the first neuron from layer 2 to the second neuron in layer 3. How does a small change in this weight change the output of our loss function? A small change in <span class="math inline">\(W^3_{1,2}\)</span> will result in a small change in the second neuron in layer 3 (<span class="math inline">\(l^3_2\)</span>)—which will directly affect the loss function <span class="math inline">\(L(\mathbf{l^3})\)</span>. This ‘chain’ of dependencies is captured by the chain rule:</p>
<span class="math display">\[\begin{aligned}
&amp;\dfrac{\partial L(\mathbf{l^3})}{\partial W^3_{1,2}} = \dfrac{\partial L(\mathbf{l^3})}{\partial l^3_2}  \dfrac{\partial l^3_2 }{\partial W^3_{1,2}}
\end{aligned}\]</span>
<p>Instead of focusing on a single node in layer 2, we can write the derivative with respect to the vector of layer 2 nodes. Because we have <span class="math inline">\(512\)</span> layer 2 nodes, this will be a <span class="math inline">\((1, 512)\)</span> vector of partial derivatives where each element indicates how the loss changes with a layer 2 node <span class="math inline">\(l^2_i\)</span>.</p>
<p>Also, from now on, we will write the loss as <span class="math inline">\(L\)</span> instead of <span class="math inline">\(L(\mathbf{l^3})\)</span> for brevity, but it is useful to remember it is a function of the output layer. Together:</p>
<span class="math display">\[\begin{aligned}
&amp;\dfrac{\partial L}{\partial W^3} = \dfrac{\partial L}{\partial \mathbf{l^3}}   \dfrac{\partial \mathbf{l^3} }{\partial W^3}
\end{aligned}\]</span>
<p>and for the other weights:</p>
<span class="math display">\[\begin{aligned}
&amp;\dfrac{\partial L}{\partial W^2} = \dfrac{\partial L}{\partial \mathbf{l^3}}  \dfrac{\partial \mathbf{l^3}}{\partial \mathbf{l^2}}  \dfrac{\partial \mathbf{l^2} }{\partial W^2}  \\
&amp;\dfrac{\partial L}{\partial W^1} = \dfrac{\partial L}{\partial \mathbf{l^3}}  \dfrac{\partial \mathbf{l^3}}{\partial \mathbf{l^2}}  \dfrac{\partial \mathbf{l^2}}{\partial \mathbf{l^1}}  \dfrac{\partial \mathbf{l^1} }{\partial W^1}
\end{aligned}\]</span>
<p>A visualisation of <span class="math inline">\(\dfrac{\partial L}{\partial W^2_{1,1}}\)</span>:</p>
<p align="center">
<img src="./nn-introduction/nn-image-derivatives.png" width="500">
</p>
<section id="understanding-each-term-with-matrix-calculus" class="level4">
<h4 class="anchored" data-anchor-id="understanding-each-term-with-matrix-calculus">Understanding each term with <a href="https://en.wikipedia.org/wiki/Matrix_calculus">matrix calculus</a></h4>
<p>To review what these terms are, because the notation is doing a lot of heavy lifting and hiding complexity:</p>
<p><span class="math inline">\(\dfrac{\partial L}{\partial W^3}\)</span>, <span class="math inline">\(L\)</span> is a scalar-valued function (it takes as an input a vector of length <span class="math inline">\(10\)</span>, the output of layer 3, and returns a scalar). We take the derivative of this with respect to a matrix. This is a <span class="math inline">\((512, 10)\)</span> matrix (the size of <span class="math inline">\(W^3\)</span>) where each entry indicates how each individual weight <span class="math inline">\(W^3_{i,o}\)</span> affects the loss.</p>
<p><span class="math inline">\(\dfrac{\partial \mathbf{l^3}}{\partial \mathbf{l^2}}\)</span> is the derivative of a vector-valued function (it outputs a vector, of length <span class="math inline">\(10\)</span>) with respect to a vector (length <span class="math inline">\(512\)</span>). This is the Jacobian, with each elements indicating how element in <span class="math inline">\(l^2\)</span> affects each element in <span class="math inline">\(l^3\)</span>. As layer 3 has <span class="math inline">\(10\)</span> neurons and layer 2 has <span class="math inline">\(512\)</span>, we will have a <span class="math inline">\((10, 512)\)</span> matrix of partial derivatives.</p>
<p><span class="math inline">\(\dfrac{\partial \mathbf{l^2} }{\partial W^2}\)</span> captures how a small change in each weight in <span class="math inline">\(W^2\)</span> will affect each dimension in <span class="math inline">\(\mathbf{l^2}\)</span>. We will have a <span class="math inline">\((512, 512, 512)\)</span>, rank-3 tensor!</p>
</section>
<section id="computing-the-derivatives-of-the-loss-with-respect-to-w3" class="level4">
<h4 class="anchored" data-anchor-id="computing-the-derivatives-of-the-loss-with-respect-to-w3">Computing the derivatives of the loss with respect to <span class="math inline">\(W^3\)</span></h4>
<p>Fortunately the structure of our network means we can simplify this a lot as many of these partial derivatives are zero. First we will look in detail at computing the derivative of the loss with respect to the weights connected to the final layer:</p>
<p><span class="math display">\[
\dfrac{\partial L}{\partial W^3} = \dfrac{\partial L}{\partial \mathbf{l^3}}   \dfrac{\partial \mathbf{l^3} }{\partial W^3}
\]</span></p>
<section id="dfracpartial-lpartial-w3" class="level5">
<h5 class="anchored" data-anchor-id="dfracpartial-lpartial-w3"><span class="math inline">\(\dfrac{\partial L}{\partial W^3}\)</span></h5>
<p>This is the exact derivative we want to compute to update the weights for the third layer, <span class="math inline">\(W^3\)</span>. It will be a <span class="math inline">\((512, 10)\)</span> matrix, with each element being how a small change in that weight affects the loss e.g.&nbsp;(we drop the superscript for brevity):</p>
<p><span class="math display">\[
    \dfrac{\partial L }{\partial W} =
    \begin{bmatrix}
        \dfrac{ \partial L }{ \partial W_{1,1} } &amp; \dfrac{ \partial L }{ \partial W_{1,2} } &amp; ... &amp; \dfrac{ \partial L }{ \partial W_{1,10} }\\
        \dfrac{ \partial L }{ \partial W_{2,1} } &amp; \ddots &amp; &amp; \vdots \\
        \vdots \\
        \dfrac{ \partial L }{ \partial W_{512,1} } &amp; \dots &amp; &amp;   \dfrac{ \partial L }{ \partial W_{512,10} }
    \end{bmatrix}
\]</span></p>
</section>
<section id="dfracpartial-lpartial-mathbfl3" class="level5">
<h5 class="anchored" data-anchor-id="dfracpartial-lpartial-mathbfl3"><span class="math inline">\(\dfrac{\partial L}{\partial \mathbf{l^3}}\)</span></h5>
<p>This is the derivative of a scalar-valued loss function with respect to a vector. It is a <span class="math inline">\(10\)</span>-dimensional vector of partial derivatives, where each entry captures the loss changes with respect to a change in that node of layer 3.</p>
<p>We can evaluate the derivative of the cross entropy loss directly:</p>
<p><span class="math display">\[
\dfrac{\partial L }{\partial l^3_n} = \text{softmax}(l^3)_n - \delta_{n, y}
\]</span></p>
<p>Where <span class="math inline">\(\delta\)</span> is the <a href="https://en.wikipedia.org/wiki/Kronecker_delta">Kronecker delta</a> that equals <span class="math inline">\(1\)</span> when <span class="math inline">\(n=y\)</span> and <span class="math inline">\(0\)</span> otherwise. This says that our vector of derivatives is <span class="math inline">\(\text{softmax}(l^3)_n\)</span> for all output logits in layer 3 that are for the incorrect labels, and <span class="math inline">\(\text{softmax}(l^3)_n - 1\)</span> for the index of the true label <span class="math inline">\(y\)</span>.</p>
<p>See <strong>Appendix 1</strong> for the derivation. In full (show as column vectors here) this looks like:</p>
<p><span class="math display">\[
\dfrac{ \partial L}{ \partial \mathbf{l^3} } =
\begin{bmatrix}
    \dfrac{\partial L}{\partial l^3_1} \\
    \dfrac{\partial L}{\partial l^3_2} \\
    \vdots\\
    \dfrac{\partial L}{\partial l^3_{10}}
\end{bmatrix}
=
\begin{bmatrix}
    \text{softmax}(l^3_1) - \delta_{1, y} \\
    \text{softmax}(l^3_2) - \delta_{2, y} \\
    \vdots \\
    \text{softmax}(l^3_{10}) - \delta_{10, y}
\end{bmatrix}
\]</span></p>
</section>
<section id="dfracpartial-mathbfl3-partial-w3" class="level5">
<h5 class="anchored" data-anchor-id="dfracpartial-mathbfl3-partial-w3"><span class="math inline">\(\dfrac{\partial \mathbf{l^3} }{\partial W^3}\)</span></h5>
<p>As mentioned above, this is a 3-rank tensor with the shape <span class="math inline">\((512, 10, 10)\)</span> (how each element of <span class="math inline">\(l^3_n\)</span> changes with each weight <span class="math inline">\(W^3_{i,o}\)</span>). How can we even deal with this in our computation?</p>
<p>A strategy is to break the problem down to look at individual elements of <span class="math inline">\(\dfrac{\partial L}{\partial W^3_{i,o}}\)</span> and compute these using the total derivative rule (<strong>Appendix 2</strong>):</p>
<p><span class="math display">\[\begin{align}
\dfrac{\partial L}{\partial W^3_{i,o}}
&amp;= \dfrac{\partial L}{\partial l^3} \dfrac{\partial l^3}{\partial W^3_{i,o}}  \\
&amp;= \sum_n \dfrac{\partial L}{\partial l^3_n} \dfrac{\partial l^3_n}{\partial W^3_{i,o}} \\
&amp;= \dfrac{\partial L}{\partial l^3_o} \dfrac{\partial l^3_o}{\partial W^3_{i,o}} \\
&amp;= \dfrac{\partial L}{\partial l^3_o} l^2_i
\end{align}\]</span></p>
<p>The notation can get a little tricky to follow here. In words, to find out how the loss changes with respect to a single weight <span class="math inline">\(W^3_{i,o}\)</span> we will see how changing this weight changes layer 3, and then how changing layer 3 changes the loss. To compute this, we will take the sum over how each weight <span class="math inline">\(W^3_{i,o}\)</span> affects the node <span class="math inline">\(n\)</span> in layer 3, <span class="math inline">\(l^3_n\)</span>, then how a change in this node affects the loss. This weight only connects to one node, <span class="math inline">\(o\)</span> and so the effect of changing that weight on all other nodes is zero.</p>
<p>So we can take the usual scalar derivative here <span class="math inline">\(\dfrac{\partial l^3_o}{\partial W^3_{i,o}} = \dfrac{\partial \ l^2_i W^3_{i,o}}{\partial W^3_{i,o}} = l^2_i\)</span>. This makes intuitive sense, the change in <span class="math inline">\(l^3_o\)</span> when we make a small change <span class="math inline">\(\delta W^3_{i,o}\)</span> is exactly the value of the input node, <span class="math inline">\(l^2_i\)</span>.</p>
</section>
<section id="putting-this-all-together-dfracpartial-lpartial-w3-dfracpartial-lpartial-mathbfl3-dfracpartial-mathbfl3-partial-w3" class="level5">
<h5 class="anchored" data-anchor-id="putting-this-all-together-dfracpartial-lpartial-w3-dfracpartial-lpartial-mathbfl3-dfracpartial-mathbfl3-partial-w3">Putting this all together: <span class="math inline">\(\dfrac{\partial L}{\partial W^3} = \dfrac{\partial L}{\partial \mathbf{l^3}} \dfrac{\partial \mathbf{l^3} }{\partial W^3}\)</span></h5>
<p>We can collect the derivatives together in a matrix. All the weights are all <span class="math inline">\(W^3\)</span> matrix and we omit the superscript inside the matrix for brevity:</p>
<p><span class="math display">\[
\dfrac{ \partial L}{ \partial W^3} =
\begin{bmatrix}
\dfrac{\partial L}{ \partial l^3_1 } \dfrac{\partial l^3_1 }{ \partial W_{1,1} }
&amp; \dfrac{\partial L}{ \partial l^3_2 } \dfrac{\partial l^3_2 }{ \partial W_{1,2} }
&amp; \dots
&amp; \dfrac{\partial L}{ \partial l^3_{10} } \dfrac{\partial l^3_{10} }{ \partial W_{1, 10} }  \\
\dfrac{\partial L}{ \partial l^3_1 } \dfrac{\partial l^3_1 }{ \partial W_{2,1} }
&amp; \ddots \\
\vdots \\
\dfrac{\partial L}{ \partial l^3_1 } \dfrac{\partial l^3_1 }{ \partial W_{512,1} }
&amp; \dots &amp;
&amp; \dfrac{\partial L}{ \partial l^3_{10} } \dfrac{\partial l^3_{10} }{ \partial W_{512,10} }
\end{bmatrix}
=
\begin{bmatrix}
[\text{sm}(l^3_1) - \delta_{1, y}]l^2_1 &amp;
[\text{sm}(l^3_2) - \delta_{2, y}] l^2_1
&amp; \dots
&amp; [\text{sm}(l^3_{10}) - \delta_{10, y}] l^2_1 &amp; \\
[\text{sm}(l^3_1) - \delta_{1, y}] l^2_2 &amp; \ddots &amp; &amp; \vdots \\
\vdots \\
[\text{sm}(l^3_1) - \delta_{1, y}] l^2_{512} &amp; \dots &amp;
&amp; [\text{sm}(l^3_{10}) - \delta_{10, y}] l^2_{512}
\end{bmatrix}
\]</span></p>
<p>i.e.&nbsp;for each weight, we see the effect of changing that weight on the layer 3 node it is connected to and multiply it with the effect of changing that layer 3 node on the loss.</p>
<p>We can write all of this as the <a href="https://en.wikipedia.org/wiki/Outer_product">outer product</a> of two vectors (recall all vectors are row vectors here, so the transpose is to a column vector):</p>
<p><span class="math display">\[
\dfrac{\partial L}{\partial W^3} = (\mathbf{l^2})^T \dfrac{\partial L}{\partial \mathbf{l^3}}
\]</span></p>
</section>
</section>
<section id="computing-the-derivatives-with-respect-to-w2" class="level4">
<h4 class="anchored" data-anchor-id="computing-the-derivatives-with-respect-to-w2">Computing the derivatives with respect to <span class="math inline">\(W^2\)</span></h4>
<p>Next, we are interested in how changing a weight in layer 2 <span class="math inline">\(W^2_{i,o}\)</span> changes the output of layer 2, how this changes layer 3, and how this affects the loss.</p>
<p><span class="math display">\[
\dfrac{\partial L}{\partial W^2} = \dfrac{\partial L}{\partial \mathbf{l^3}}  \dfrac{\partial \mathbf{l^3}}{\partial \mathbf{l^2}}  \dfrac{\partial \mathbf{l^2} }{\partial W^2}
\]</span></p>
<p>We have seen similar terms to all of these above, except for the middle term on the right hand side, the Jacobian.</p>
<section id="dfracpartial-mathbfl3partial-mathbfl2" class="level5">
<h5 class="anchored" data-anchor-id="dfracpartial-mathbfl3partial-mathbfl2"><span class="math inline">\(\dfrac{\partial \mathbf{l^3}}{\partial \mathbf{l^2}}\)</span></h5>
<p>This is the Jacobian. It is <span class="math inline">\((10, 512)\)</span> and contains the derivative of each element in <span class="math inline">\(\mathbf{l^3}\)</span> with respect to each element in <span class="math inline">\(\mathbf{l^2}\)</span>:</p>
<span class="math display">\[\begin{aligned}
\dfrac{\partial \mathbf{l^3}}{\partial \mathbf{l^2}} =
\begin{bmatrix}
    \dfrac{\partial l^3_1}{\partial l^2_1} &amp; \dfrac{\partial l^3_1}{\partial l^2_2} &amp; \dots &amp; \dfrac{\partial l^3_1}{\partial l^2_{512}} \\
    \dfrac{\partial l^3_2}{\partial l^2_1} &amp; \ddots &amp; &amp; \vdots \\
    \vdots &amp; &amp; \\
    \dfrac{\partial l^3_{10}}{\partial l^2_1} &amp; \dots &amp; &amp; \dfrac{\partial l^3_{10}}{\partial l^2_{512}}
\end{bmatrix}
\end{aligned}\]</span>
<p>We can compute <span class="math inline">\(\dfrac{ \partial L }{ \partial \mathbf{l^2}} = \dfrac{ \partial L }{ \partial \mathbf{l^3}} \dfrac{ \partial  \mathbf{l^3} }{ \partial \mathbf{l^2}}\)</span> as matrix multiplication with shapes <span class="math inline">\((1, 10) \times (10, 512)\)</span>, with the resulting <span class="math inline">\((1, 512)\)</span> vector being:</p>
<span class="math display">\[\begin{aligned}
\begin{bmatrix}
    \sum_o \dfrac{ \partial L }{ \partial l^3_o } \dfrac{\partial l^3_o }{ \partial l^2_1 } &amp;
    \sum_o \dfrac{ \partial L }{ \partial l^3_o } \dfrac{\partial l^3_o }{ \partial l^2_2 } &amp;
    \cdots &amp;
    \sum_o \dfrac{ \partial L }{ \partial l^3_o } \dfrac{\partial l^3_o }{ \partial l^2_{512} }
\end{bmatrix}
\end{aligned}\]</span>
<p>Because each node of layer 2 is connected to every node of layer 3, so to understand how a change in node in layer 2 affects the loss, we sum over its effect on every node in layer 3. While the derivatives don’t go to zero like in the <span class="math inline">\(W^3\)</span> case above, each individual derivative is simple to compute:</p>
<p><span class="math display">\[
\dfrac{\partial l^3_o }{ \partial l^2_i } = \dfrac{\partial l^2_i W^3_{i, o} }{ \partial l^2_i } = W^3_{i, o}
\]</span></p>
<p>And so we can compute this with the matrix vector calculation:</p>
<p><span class="math display">\[
\dfrac{ \partial L }{ \partial \mathbf{l^2} } = \dfrac{ \partial L}{ \partial \mathbf{l^3}} (W^3)^T
\]</span></p>
</section>
<section id="dfracpartial-mathbfl2-partial-w2" class="level5">
<h5 class="anchored" data-anchor-id="dfracpartial-mathbfl2-partial-w2"><span class="math inline">\(\dfrac{\partial \mathbf{l^2} }{\partial W^2}\)</span></h5>
<p>Now, we have the multiplication of this <span class="math inline">\((1, 512)\)</span> 2-rank tensor <span class="math inline">\(\dfrac{ \partial L }{ \partial \mathbf{l^2} }\)</span> with the <span class="math inline">\((512, 512, 512)\)</span> 3-rank tensor <span class="math inline">\(\dfrac{\partial \mathbf{l^2} }{\partial W^2}\)</span>. In tensor multiplication, we need to be explicit about what dimensions we sum over.</p>
<p>Because each <span class="math inline">\(W^2_{i, o}\)</span> is connected to a single layer 2 node <span class="math inline">\(l^2_o\)</span>, things are the same as in the <span class="math inline">\(W^3\)</span> case and most of the derivatives go to zero. Therefore using the same logic, we have:</p>
<p><span class="math display">\[
\dfrac{ \partial L }{ \partial W^2 } = (\mathbf{l^1})^T \dfrac{ \partial L }{ \partial \mathbf{l^2} }
\]</span></p>
<p>i.e.&nbsp;the outer product of a <span class="math inline">\((512, 1)\)</span> vector by a <span class="math inline">\((1, 512)\)</span> vector to give us a <span class="math inline">\((512, 512)\)</span> matrix of derivatives.</p>
</section>
<section id="computing-the-partial-derivatives-of-w1" class="level5">
<h5 class="anchored" data-anchor-id="computing-the-partial-derivatives-of-w1">Computing the partial derivatives of <span class="math inline">\(W^1\)</span></h5>
<p>Since we have done through all of the hard work of really inspecting what is going on under the hood, we can really simplify the notation going forward. This compact notation really highlights the ‘chain’ aspect of the chain rule:</p>
<p><span class="math display">\[
\dfrac{\partial L}{\partial W^1} = \dfrac{\partial L}{\partial \mathbf{l^3}}  \dfrac{\partial \mathbf{l^3}}{\partial \mathbf{l^2}}  \dfrac{\partial \mathbf{l^2}}{\partial \mathbf{l^1}}  \dfrac{\partial \mathbf{l^1} }{\partial W^1}
\]</span></p>
<p>We are using the total derivative rule to condense all intermediate calculations, just like above:</p>
<p>First:</p>
<p><span class="math display">\[
\mathbf{g_1} = \dfrac{\partial L}{\partial \mathbf{l^2}} = \dfrac{\partial L}{\partial \mathbf{l^3}}  \dfrac{\partial \mathbf{l^3}}{\partial \mathbf{l^2}} = \dfrac{\partial L}{\partial \mathbf{l^3}} (W^3)^T
\]</span></p>
<p><span class="math inline">\((1, 512) = (1, 10) \times (10, 512)\)</span></p>
<p><span class="math display">\[
\mathbf{g_2} = \dfrac{\partial L}{\partial \mathbf{l^1}} =  \dfrac{\partial L}{\partial \mathbf{l^2}} \dfrac{\partial \mathbf{l^2}}{\partial \mathbf{l^1}} = \mathbf{g_1}(W^2)^T
\]</span></p>
<p><span class="math inline">\((1, 512) = (1, 512) \times (512, 512)\)</span></p>
<p>Finally we have the outer product calculation:</p>
<p><span class="math display">\[
\dfrac{\partial L}{\partial W^1} = \dfrac{\partial L}{\partial \mathbf{l^1}} \dfrac{\partial \mathbf{l^1}}{\partial W^1} = (\mathbf{x})^T \mathbf{g_2}
\]</span></p>
<p><span class="math inline">\((512, 512) = (512, 1) \times (1, 512)\)</span></p>
<p><strong>It’s worth reflecting on this. We have such complexity here. And it reduces really nicely.</strong> It’s awesome to see how changing the derivatives, backwards from the final layer, greatly simplifies computing the derivatives for our complex, fully connected network.</p>
<p>It is also very easy to compute, because we already have our weight vectors and we compute the output of each layer as part of our forward pass!</p>
</section>
</section>
</section>
</section>
<section id="the-code" class="level2">
<h2 class="anchored" data-anchor-id="the-code">The Code</h2>
<div id="9e907e85b1133c4e" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2025-12-17T01:18:17.136876Z&quot;,&quot;start_time&quot;:&quot;2025-12-17T01:18:17.126061Z&quot;}}">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>run <span class="op">=</span> <span class="va">False</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MyBasicNetwork:</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, learning_rate<span class="op">=</span><span class="fl">0.02</span>):</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.a <span class="op">=</span> learning_rate</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Define weight matrix (output dim, input dim) by convention</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use zero-mean Xavier init (good for sigmoid, it has little</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># effect here as we don't use activation functions,</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># but useful for comparison.)</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W1 <span class="op">=</span> np.random.randn(<span class="dv">784</span>, <span class="dv">512</span>) <span class="op">*</span> np.sqrt(<span class="dv">1</span> <span class="op">/</span> <span class="dv">784</span>)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W2 <span class="op">=</span> np.random.randn(<span class="dv">512</span>, <span class="dv">512</span>) <span class="op">*</span> np.sqrt(<span class="dv">1</span> <span class="op">/</span> <span class="dv">512</span>)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W3 <span class="op">=</span> np.random.randn(<span class="dv">512</span>, <span class="dv">10</span>) <span class="op">*</span> np.sqrt(<span class="dv">1</span> <span class="op">/</span> <span class="dv">512</span>)</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss(<span class="va">self</span>, l3, y):</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> <span class="va">self</span>.softmax(l3)[<span class="dv">0</span>][y]</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="op">-</span>np.log( p <span class="op">+</span> <span class="fl">1e-15</span>)</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> softmax(<span class="va">self</span>, vec):</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>        C <span class="op">=</span> np.<span class="bu">max</span>(vec)</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.exp(vec <span class="op">-</span> C) <span class="op">/</span> np.<span class="bu">sum</span>(np.exp(vec <span class="op">-</span> C))</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># forward pass through the network</span></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.reshape(<span class="dv">1</span>, x.size)</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>        l1 <span class="op">=</span> x <span class="op">@</span> <span class="va">self</span>.W1</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>        l2 <span class="op">=</span> l1 <span class="op">@</span> <span class="va">self</span>.W2</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>        l3 <span class="op">=</span> l2 <span class="op">@</span> <span class="va">self</span>.W3</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> np.argmax(<span class="va">self</span>.softmax(l3))</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pred, l1, l2, l3</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_weights(<span class="va">self</span>, x, y, verbose<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>        _, l1, l2, l3 <span class="op">=</span> <span class="va">self</span>.predict(x)</span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="va">self</span>.loss(l3, y)</span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> verbose:</span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Loss: </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the derivatives</span></span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a>        dloss_dl3 <span class="op">=</span> <span class="va">self</span>.softmax(l3) </span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a>        dloss_dl3[<span class="dv">0</span>][y] <span class="op">-=</span> <span class="dv">1</span></span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a>        dloss_dW3 <span class="op">=</span> l2.T <span class="op">@</span> dloss_dl3       <span class="co"># (512, 10) = (512, 1) x (1, 10)</span></span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a>        dloss_dl2 <span class="op">=</span> dloss_dl3 <span class="op">@</span> <span class="va">self</span>.W3.T  <span class="co"># (1, 512) = (1, 10) x (10, 512)</span></span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a>        dloss_dW2 <span class="op">=</span> l1.T <span class="op">@</span> dloss_dl2       <span class="co"># (512, 512) = (512, 1) x (1, 512)</span></span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a>        dloss_dl1 <span class="op">=</span> dloss_dl2 <span class="op">@</span> <span class="va">self</span>.W2.T  <span class="co"># (1, 512) = (1, 512) x (512, 512)</span></span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a>        dloss_dW1 <span class="op">=</span> x.T <span class="op">@</span> dloss_dl1        <span class="co"># (784, 512) = (781, 1) x (1, 512)</span></span>
<span id="cb11-58"><a href="#cb11-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-59"><a href="#cb11-59" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W3 <span class="op">-=</span> <span class="va">self</span>.a <span class="op">*</span> dloss_dW3</span>
<span id="cb11-60"><a href="#cb11-60" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W2 <span class="op">-=</span> <span class="va">self</span>.a <span class="op">*</span> dloss_dW2</span>
<span id="cb11-61"><a href="#cb11-61" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W1 <span class="op">-=</span> <span class="va">self</span>.a <span class="op">*</span> dloss_dW1</span>
<span id="cb11-62"><a href="#cb11-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-63"><a href="#cb11-63" aria-hidden="true" tabindex="-1"></a><span class="co"># We won't run this here because it is very slow,</span></span>
<span id="cb11-64"><a href="#cb11-64" aria-hidden="true" tabindex="-1"></a><span class="co"># but it gives an accuracy of ~73%</span></span>
<span id="cb11-65"><a href="#cb11-65" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> run:</span>
<span id="cb11-66"><a href="#cb11-66" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialise and train the model (no batching)</span></span>
<span id="cb11-67"><a href="#cb11-67" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> MyBasicNetwork()</span>
<span id="cb11-68"><a href="#cb11-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-69"><a href="#cb11-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-70"><a href="#cb11-70" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (X, y) <span class="kw">in</span> <span class="bu">enumerate</span>(training_data):</span>
<span id="cb11-71"><a href="#cb11-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-72"><a href="#cb11-72" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> np.asarray(X[<span class="dv">0</span>, :, :])</span>
<span id="cb11-73"><a href="#cb11-73" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> <span class="bu">int</span>(y)</span>
<span id="cb11-74"><a href="#cb11-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-75"><a href="#cb11-75" aria-hidden="true" tabindex="-1"></a>        model.update_weights(x, y)</span>
<span id="cb11-76"><a href="#cb11-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-77"><a href="#cb11-77" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb11-78"><a href="#cb11-78" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Training iteration: sample: </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-79"><a href="#cb11-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-80"><a href="#cb11-80" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check the model accuracy</span></span>
<span id="cb11-81"><a href="#cb11-81" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> np.empty(<span class="bu">len</span>(test_data))</span>
<span id="cb11-82"><a href="#cb11-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-83"><a href="#cb11-83" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (X, y) <span class="kw">in</span> <span class="bu">enumerate</span>(test_data):</span>
<span id="cb11-84"><a href="#cb11-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-85"><a href="#cb11-85" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> np.asarray(X[<span class="dv">0</span>, :, :])</span>
<span id="cb11-86"><a href="#cb11-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-87"><a href="#cb11-87" aria-hidden="true" tabindex="-1"></a>        results[i] <span class="op">=</span> model.predict(x)[<span class="dv">0</span>] <span class="op">==</span> y</span>
<span id="cb11-88"><a href="#cb11-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-89"><a href="#cb11-89" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Percent Correct: </span><span class="sc">{</span>np<span class="sc">.</span>mean(results) <span class="op">*</span> <span class="dv">100</span><span class="sc">}</span><span class="ss">%"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="a-better-model" class="level2">
<h2 class="anchored" data-anchor-id="a-better-model">A Better Model</h2>
<p>We will now extend the model to a standard neural network by including a bias term and activation function. add some simple changes to improve the performance of the model.</p>
<p>In our simple version, we essentially had <span class="math inline">\(\mathbf{l} = \mathbf{x}W\)</span> where <span class="math inline">\(W\)</span> is a <span class="math inline">\((784, 10)\)</span> matrix. One way to interpret this is a matrix encoding <span class="math inline">\(10\)</span> hyperplanes (their normal vectors) in a <span class="math inline">\(784\)</span>-dimension space. The dot product between the image vector <span class="math inline">\(\mathbf{x}\)</span> and a column of this <span class="math inline">\(W\)</span> captures the alignment between that hyperplane’s normal vector and the image vector.</p>
<p>A simple change we will make is to include a bias term <span class="math inline">\(\mathbf{b}\)</span>. We can interpret this as adding an offset to each hyperplane, meaning our data does not need to be centered at zero.</p>
<p>We will make the network nonlinear by passing the output of each node through a nonlinear activation function. While each layer still applies a linear transformation, the composition of these transformations with nonlinear activations makes the overall mapping nonlinear. As a result, the network can represent complex, nonlinear decision boundaries in high-dimensional space.</p>
<p>Another way to think about the benefit of non-linear activation function is to think of each node as an individual function, and the composition across layers as the composition of these functions. In the linear case, they are compositions if linear functions, which are still linear. For example, (omitting biases here for convenience) the output of layer 2 node 1 is: <span class="math inline">\(\sum_o \sum_i x_i W^1_{i,o} W^2_{o, 1}\)</span> which is still linear. This node is a function mapping a vector to a scalar (its linear, so a hyperplane); it’s domain is <span class="math inline">\(\mathbf{x} \in \mathcal{X}\)</span>. If we add nonlinear activations, the function this node can represent are much more flexible as they are the composition of nonlinear functions. <a href="https://www.youtube.com/watch?v=CqOfi41LfDw&amp;t=925s">This</a> is a brilliant video on this idea.</p>
<p>Our network is now:</p>
<p><span class="math display">\[
\begin{aligned}
    l^1 &amp;= \phi(\mathbf{x}W^1 + \mathbf{b^1}) \\
    l^2 &amp;= \phi(\mathbf{l^1}W^2 + \mathbf{b^2}) \\
    l^3 &amp;= \mathbf{l^2}W^3 + \mathbf{b^3} \\
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\phi(\cdot)\)</span> is our nonlinear activation function. <span class="math inline">\(\mathbf{b^i}\)</span> is a <span class="math inline">\((1, n)\)</span> vector of biases, one for each of <span class="math inline">\(n\)</span> nodes layer <span class="math inline">\(i\)</span>. The output of <span class="math inline">\(\mathbf{l^{n-1}}W^n+\mathbf{b^n}\)</span> in each layer is still a vector (e.g.&nbsp;<span class="math inline">\((1, 10)\)</span>) and we apply the nonlinear activation function element-wise to this vector.</p>
<p>We use the sigmoid function because it has interesting derivatives, but in general ReLu is preferred in larger, modern networks due to the <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">vanishing gradient problem</a>. We don’t apply the sigmoid function to the last layer, as we feed this directly to the cross-entropy loss which is already doing a similar mapping with the softmax function.</p>
<p>Now, we will take the derivatives of the loss with respect to the weights <em>and</em> biases.</p>
<section id="computing-the-derivatives" class="level3">
<h3 class="anchored" data-anchor-id="computing-the-derivatives">Computing the derivatives</h3>
<section id="derivatives-of-the-sigmoid-activation-function" class="level4">
<h4 class="anchored" data-anchor-id="derivatives-of-the-sigmoid-activation-function">Derivatives of the sigmoid activation function</h4>
<p>The sigmoid function maps the interval <span class="math inline">\((-\infty, \infty)\)</span> to <span class="math inline">\((0, 1)\)</span>.</p>
<p><span class="math display">\[
\phi(z(t)) = \dfrac{1}{1 + e^{-z(t)}}
\]</span></p>
<p>Let <span class="math inline">\(z\)</span> is some arbitrary function of the variable <span class="math inline">\(t\)</span>. Then the derivative by the chain rule is:</p>
<p><span class="math display">\[
\dfrac{d}{dt} (1 + e^{-z(t)})^{-1} =  \dfrac{ e^{-z(t)} }{ (1 + e^{-z(t)})^2 } \dfrac{d}{dt} z(t)
\]</span></p>
<p>So, we will need account for include these additional terms in our derivative computations.</p>
</section>
<section id="w3-and-mathbfb3" class="level4">
<h4 class="anchored" data-anchor-id="w3-and-mathbfb3"><span class="math inline">\(W^3\)</span> and <span class="math inline">\(\mathbf{b^3}\)</span></h4>
<p>As we don’t apply the sigmoid to the last layer, this is exactly the same as the simple example:</p>
<p><span class="math display">\[
\dfrac{\partial L}{\partial W^3} = (\mathbf{l^2})^T \dfrac{\partial L}{\partial \mathbf{l^3}}
\]</span></p>
<p>The derivative of layer 3 with respect to the <strong>bias</strong> is <span class="math inline">\(1\)</span>, <span class="math inline">\(\dfrac{\partial l^3_o}{\partial b^3_0} = \dfrac{\partial } {\partial b^3_o} l^2_i W^3_{i, o} + b^3_o = 1\)</span>. This makes intuitive sense, when we make a small <span class="math inline">\(\delta b^3_o\)</span> change, it just changes the output of layer 3 exactly by this small change <span class="math inline">\(\delta b^3_o\)</span>, because it is simply added to the output. Therefore:</p>
<p><span class="math display">\[
\dfrac{\partial L}{\partial \mathbf{b^3}} = \dfrac{\partial L}{\partial \mathbf{l^3}}
\]</span></p>
</section>
<section id="w2-and-mathbfb2" class="level4">
<h4 class="anchored" data-anchor-id="w2-and-mathbfb2"><span class="math inline">\(W^2\)</span> and <span class="math inline">\(\mathbf{b^2}\)</span></h4>
<p>As in the simple case, the chance in the loss with a change in the weights of layer 2 is:</p>
<p><span class="math display">\[
\dfrac{\partial L}{\partial W^2} = \dfrac{\partial L}{\partial \mathbf{l^3}}  \dfrac{\partial \mathbf{l^3}}{\partial \mathbf{l^2}}  \dfrac{\partial \mathbf{l^2} }{\partial W^2}
\]</span></p>
<section id="dfrac-partial-l-partial-mathbfl2" class="level5">
<h5 class="anchored" data-anchor-id="dfrac-partial-l-partial-mathbfl2"><span class="math inline">\(\dfrac{ \partial L }{ \partial \mathbf{l^2}}\)</span></h5>
<p>Because layer 3 does not have an activation function (and the derivative of the new bias term goes to zero) this is identical to the simple case:</p>
<p><span class="math display">\[
\dfrac{ \partial L }{ \partial \mathbf{l^2}} = \dfrac{ \partial L }{ \partial \mathbf{l^3}} (W^3)^T
\]</span></p>
</section>
<section id="dfrac-partial-l-partial-w2" class="level5">
<h5 class="anchored" data-anchor-id="dfrac-partial-l-partial-w2"><span class="math inline">\(\dfrac{ \partial L }{ \partial W^2 }\)</span></h5>
<p>Next, to compute the derivative of the loss with respect to the layer 2 weight matrix and the bias. First starting with the weights, again considering each weight in isolation:</p>
<p><span class="math display">\[
\dfrac{ \partial L }{\partial W^2_{i, o}} = \dfrac{ \partial L }{ \partial l^2_o } \dfrac{ \partial l^2_o }{ \partial W^2_{i, o}}
\]</span></p>
<p>We calculated the first term above, so looking at the second term:</p>
<p><span class="math display">\[
\dfrac{ \partial l^2_o }{ \partial W^2_{i, o}} = \dfrac{ \partial }{ \partial W^2_{i, o}} \phi( \sum_n l^1_n W^2_{n, o} + b^2_o)
\]</span></p>
<p>The term inside <span class="math inline">\(\phi\)</span> with respect to the weight <span class="math inline">\(W^2_{i,o}\)</span> is <span class="math inline">\(l^1_i\)</span> as it is <span class="math inline">\(0\)</span> when <span class="math inline">\(n \neq i\)</span> (because that weight does not connect to that layer 1 input node). Recalling the form of the derivative of the sigmoid function above, and letting <span class="math inline">\(\hat{l^2_o} = \sum_i l^1_i W^2_{i, o} + b^2_o\)</span>:</p>
<p><span class="math display">\[
\dfrac{ \partial l^2_o }{ \partial W^2_{i,o}} = \dfrac{ e^{-\hat{l^2_o}} }{ (1 + e^{-\hat{l^2_o}})^2 } \ l^1_i
\]</span></p>
<p>We can implement this in matrix form. <span class="math inline">\(\hat{l^2}\)</span> is a <span class="math inline">\((1, 10)\)</span> vector and so the element-wise multiplication is what we need here (officilally called the <a href="https://en.wikipedia.org/wiki/Hadamard_product_(matrices)">Hadamard product</a> with notation <span class="math inline">\(\circ\)</span>). Bringing this all together:</p>
<p><span class="math display">\[
\begin{aligned}
    \dfrac{ \partial L }{\partial W^2} = (\mathbf{l^1})^T   \bigg(  \dfrac{ e^{-\mathbf{\hat{l^2}}} }{ (1 + e^{-\mathbf{\hat{l^2}}})^2 }   \circ \dfrac{ \partial L }{ \partial \mathbf{l^2} } \bigg)
\end{aligned}
\]</span></p>
<p><span class="math display">\[
(512, 512) = (512, 1) \times (1, 512) \circ (1, 512)
\]</span></p>
<p>So in fact, the derivative looks very similar to before we had the activation function, except now we have these extra terms from the chain rule.</p>
<p>The bias calculation is simple, if we take the above derivative with respect to <span class="math inline">\(b_o\)</span> then the inside derivative goes to <span class="math inline">\(1\)</span>:</p>
<p><span class="math display">\[
    \dfrac{ \partial L }{\partial \mathbf{b^2}} =  \dfrac{ e^{-\mathbf{\hat{l^2}}} }{ (1 + e^{-\mathbf{\hat{l^2}}})^2 } \circ \dfrac{ \partial L }{ \partial \mathbf{l^2} }
\]</span></p>
</section>
</section>
<section id="w1-and-mathbfb1" class="level4">
<h4 class="anchored" data-anchor-id="w1-and-mathbfb1"><span class="math inline">\(W^1\)</span> and <span class="math inline">\(\mathbf{b^1}\)</span></h4>
<p>The derivation for layer 1 follows all the same ideas that we explored for layer 2.</p>
<p><span class="math display">\[
\dfrac{\partial L }{ \partial W^1} = \dfrac{\partial L }{ \partial \mathbf{l^3} } \dfrac{\partial \mathbf{l^3} }{ \partial \mathbf{l^2} } \dfrac{\partial \mathbf{l^2} }{ \partial \mathbf{l^1} } \dfrac{\partial \mathbf{l^1} }{ \partial \mathbf{W^1} }
\]</span></p>
<p>We just need to compute <span class="math inline">\(\dfrac{\partial L }{ \partial \mathbf{l^1} }\)</span> again, now we have the nonlinear activation function to deal with.</p>
</section>
<section id="dfracpartial-l-partial-mathbfl1" class="level4">
<h4 class="anchored" data-anchor-id="dfracpartial-l-partial-mathbfl1"><span class="math inline">\(\dfrac{\partial L}{ \partial \mathbf{l^1} }\)</span></h4>
<p>Again, this will be a <span class="math inline">\((1, 512)\)</span> vector where each element is how the loss changes with a small change in the corresponding node of layer 1. Similar to the simple case, we can compute each element individually using the total derivative rule:</p>
<p><span class="math display">\[
\begin{aligned}
\dfrac{\partial L}{\partial l^1_i }  &amp;= \sum_o \dfrac{\partial L}{\partial l^2_o } \dfrac{\partial l^2_o}{\partial l^1_i }
\end{aligned}
\]</span></p>
<p>and note the output of a single layer 2 node is:</p>
<p><span class="math display">\[
l^2_o = \phi \left( \sum_n l^1_n W^2_{n, o} + b^2_o \right)
\]</span></p>
<p>For the derivative of the term inside the activation function, this is exactly the same as the simple case. We take the derivative of <span class="math inline">\(\sum_n l^1_n W^2_{n, o} + b^2_o\)</span> with respect to a specific input node <span class="math inline">\(l^1_i\)</span> then all terms are zero except for the case where <span class="math inline">\(i = n\)</span>. Therefore, the derivative with respect to <span class="math inline">\(l^1_i\)</span> is <span class="math inline">\(W^2_{i,o}\)</span>.</p>
<p>Again, we will introduce the notation <span class="math inline">\(\mathbf{\hat{l^2}} = \mathbf{l^1}W^2 + \mathbf{b^2}\)</span>, i.e.&nbsp;<span class="math inline">\(\mathbf{\hat{l^2}}\)</span> is the output of layer 2 before we put it through the activation function.</p>
<p>Putting this together with the derivative of the activation function, as above:</p>
<p><span class="math display">\[
\begin{aligned}
\dfrac{\partial L}{\partial l^1_i } &amp;= \sum_o \dfrac{\partial L}{\partial l^2_o } \dfrac{\partial l^2_o}{\partial l^1_i }  \\
&amp;= \sum_o \dfrac{\partial L}{\partial l^2_o }  \dfrac{\partial }{\partial l^1_i } \phi({\hat{l^2_o}} ) \\
&amp;= \sum_o \dfrac{\partial L}{\partial l^2_o } \dfrac{ e^{-\hat{l^2_o}} }{ (1 + e^{-\hat{l^2_o}})^2 } W^2_{i, o}
\end{aligned}
\]</span></p>
<p>So we can represent this in matrix form as:</p>
<p><span class="math display">\[
\dfrac{ \partial L }{ \partial \mathbf{l^1} } = \bigg( \dfrac{ \partial L }{ \partial \mathbf{l^2} }  \circ  \dfrac{ e^{-\mathbf{\hat{l^2}}} }{ (1 + e^{-\mathbf{\hat{l^2}}})^2 } \bigg) (W^2)^T
\]</span></p>
</section>
<section id="so-putting-it-all-together-for-w1-and-mathbfb1" class="level4">
<h4 class="anchored" data-anchor-id="so-putting-it-all-together-for-w1-and-mathbfb1">So putting it all together for <span class="math inline">\(W^1\)</span> and <span class="math inline">\(\mathbf{b^1}\)</span></h4>
<p>Again, we use <span class="math inline">\(\hat{\mathbf{l^1}} = \mathbf{x}W^1 + \mathbf{b^1}\)</span> for the output of layer 1 before inputting to the activation function.</p>
<p>So as in the simple case, we will go through these piece-by-piece:</p>
<p><span class="math display">\[
\begin{aligned}
    \mathbf{g_1} &amp;= \dfrac{\partial L }{ \partial \mathbf{l^2} } = \dfrac{\partial L }{ \partial \mathbf{l^3} } \dfrac{\partial \mathbf{l^3} }{ \partial \mathbf{l^2} }  \\
    &amp;=  \dfrac{ \partial L }{ \partial \mathbf{l^3} } (W^3)^T
\end{aligned}
\]</span></p>
<p><span class="math inline">\((1, 512) = (1, 10) \circ (1, 10) \times (10, 512)\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{g_2} &amp;= \dfrac{ \partial L }{ \partial \mathbf{l^1 }} \\
&amp;= \mathbf{g_1} \dfrac{\partial \mathbf{l^2} }{ \partial \mathbf{l^1} } \\
&amp;= \bigg( \mathbf{g_1} \circ \dfrac{ e^{-\hat{\mathbf{l^2}} } }{ (1 + e^{-\hat{\mathbf{l^2}}})^2 } \bigg) (W^2)^T
\end{aligned}
\]</span></p>
<p><span class="math inline">\((1, 512) = (1, 512) \circ (1, 512) \times (512, 512)\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\dfrac{\partial L}{ W^1} &amp;= \mathbf{g_2} \dfrac{\partial \mathbf{l^1} }{ \partial W^1 }\\
&amp;= \mathbf{x}^T \bigg( \dfrac{ e^{-\hat{\mathbf{l^1}} } }{ (1 + e^{-\hat{\mathbf{l^1}} })^2 } \circ \mathbf{g_2} \bigg)
\end{aligned}
\]</span></p>
<p><span class="math display">\[
(784, 512) = (784, 1) \times (1, 512) * (1, 512)
\]</span></p>
<p>When taking the derivative with respect to the bias, all steps are the same except the last step <span class="math inline">\(\mathbf{x}\)</span> is <span class="math inline">\(1\)</span> (to see this, take the derivative of <span class="math inline">\(l^1_o = x_i W^1_{i, o} + b^1_o\)</span> with respect to <span class="math inline">\(b_0\)</span> or <span class="math inline">\(W^1_{i,o}\)</span>):</p>
<p><span class="math display">\[
\dfrac{ \partial L}{ \partial \mathbf{b^1}} = \dfrac{ e^{-\hat{\mathbf{l^1}} } }{ (1 + e^{-\hat{\mathbf{l^1}} })^2 } \circ \mathbf{g_2}
\]</span></p>
<p>This network is implemented below, and we see the accuracy in this case has increased by 10%, from 73% to 83%.</p>
<div id="a1fdbd87" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;ExecuteTime&quot;,&quot;value&quot;:{&quot;end_time&quot;:&quot;2025-12-17T01:18:17.202698Z&quot;,&quot;start_time&quot;:&quot;2025-12-17T01:18:17.191179Z&quot;}}">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>run <span class="op">=</span> <span class="va">False</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MyBetterNetwork:</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, learning_rate<span class="op">=</span><span class="fl">0.02</span>):</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.a <span class="op">=</span> learning_rate</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Define weight matrix (output dim, input dim) by convention</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use zero-mean Xavier init (good for sigmoid)</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This makes a huge differences vs uniform.</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W1 <span class="op">=</span> np.random.randn(<span class="dv">784</span>, <span class="dv">512</span>) <span class="op">*</span> np.sqrt(<span class="dv">1</span> <span class="op">/</span> <span class="dv">784</span>)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W2 <span class="op">=</span> np.random.randn(<span class="dv">512</span>, <span class="dv">512</span>) <span class="op">*</span> np.sqrt(<span class="dv">1</span> <span class="op">/</span> <span class="dv">512</span>)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W3 <span class="op">=</span> np.random.randn(<span class="dv">512</span>, <span class="dv">10</span>) <span class="op">*</span> np.sqrt(<span class="dv">1</span> <span class="op">/</span> <span class="dv">512</span>)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b1 <span class="op">=</span> np.zeros((<span class="dv">1</span>, <span class="dv">512</span>))</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b2 <span class="op">=</span> np.zeros((<span class="dv">1</span>, <span class="dv">512</span>))</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b3 <span class="op">=</span> np.zeros((<span class="dv">1</span>, <span class="dv">10</span>))</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss(<span class="va">self</span>, l3, y):</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> <span class="va">self</span>.softmax(l3)[<span class="dv">0</span>][y]</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="op">-</span>np.log( p <span class="op">+</span> <span class="fl">1e-15</span>)</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> softmax(<span class="va">self</span>, vec):</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>        C <span class="op">=</span> np.<span class="bu">max</span>(vec)</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.exp(vec <span class="op">-</span> C) <span class="op">/</span> np.<span class="bu">sum</span>(np.exp(vec <span class="op">-</span> C))</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># forward pass through the network</span></span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.reshape(<span class="dv">1</span>, x.size)</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>        l1_hat <span class="op">=</span> x <span class="op">@</span> <span class="va">self</span>.W1 <span class="op">+</span> <span class="va">self</span>.b1</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>        l1 <span class="op">=</span> <span class="va">self</span>.phi(l1_hat)</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>        l2_hat <span class="op">=</span> l1 <span class="op">@</span> <span class="va">self</span>.W2 <span class="op">+</span> <span class="va">self</span>.b2</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>        l2 <span class="op">=</span> <span class="va">self</span>.phi(l2_hat)</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>        l3 <span class="op">=</span> l2 <span class="op">@</span> <span class="va">self</span>.W3 <span class="op">+</span> <span class="va">self</span>.b3</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> np.argmax(<span class="va">self</span>.softmax(l3))</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pred, l1_hat, l1, l2_hat, l2, l3</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> phi(<span class="va">self</span>, vec):</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>vec))</span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> dphi_dvec(<span class="va">self</span>, vec):</span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.exp(<span class="op">-</span>vec) <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>vec))<span class="op">**</span><span class="dv">2</span></span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_weights(<span class="va">self</span>, x, y, verbose<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.reshape(<span class="dv">1</span>, x.size)</span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a>        _, l1_hat, l1, l2_hat, l2, l3 <span class="op">=</span> <span class="va">self</span>.predict(x)</span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="va">self</span>.loss(l3, y)</span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> verbose:</span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Loss: </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the derivatives</span></span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a>        dloss_dl3 <span class="op">=</span> <span class="va">self</span>.softmax(l3)</span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a>        dloss_dl3[<span class="dv">0</span>][y] <span class="op">-=</span> <span class="dv">1</span></span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a>        dloss_dW3 <span class="op">=</span> l2.T <span class="op">@</span> dloss_dl3</span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a>        dloss_db3 <span class="op">=</span> dloss_dl3</span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a>        dloss_dl2 <span class="op">=</span> dloss_dl3 <span class="op">@</span> <span class="va">self</span>.W3.T                               <span class="co"># (1, 512) = (1, 10) x (10, 512)</span></span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a>        dloss_dW2 <span class="op">=</span> l1.T <span class="op">@</span> (<span class="va">self</span>.dphi_dvec(l2_hat) <span class="op">*</span> dloss_dl2)         <span class="co"># (512, 512) = (512, 1) x (1, 512) * (1, 512)</span></span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a>        dloss_db2 <span class="op">=</span> <span class="va">self</span>.dphi_dvec(l2_hat) <span class="op">*</span> dloss_dl2                  <span class="co"># (1, 512) = (512, 1) x (1, 512)</span></span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a>        dloss_dl1 <span class="op">=</span> (dloss_dl2 <span class="op">*</span> <span class="va">self</span>.dphi_dvec(l2_hat)) <span class="op">@</span> <span class="va">self</span>.W2.T    <span class="co"># (1, 512) = (1, 512) * (1, 512) x (512, 512)</span></span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a>        dloss_dW1 <span class="op">=</span> x.T <span class="op">@</span> (<span class="va">self</span>.dphi_dvec(l1_hat) <span class="op">*</span> dloss_dl1)          <span class="co"># (784, 512) = (784, 1) x (1, 512) * (1, 512)</span></span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a>        dloss_db1 <span class="op">=</span> <span class="va">self</span>.dphi_dvec(l1_hat) <span class="op">*</span> dloss_dl1                  <span class="co"># (1, 512) = (1, 512) * (1, 512)</span></span>
<span id="cb12-75"><a href="#cb12-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-76"><a href="#cb12-76" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W3 <span class="op">-=</span> <span class="va">self</span>.a <span class="op">*</span> dloss_dW3</span>
<span id="cb12-77"><a href="#cb12-77" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W2 <span class="op">-=</span> <span class="va">self</span>.a <span class="op">*</span> dloss_dW2</span>
<span id="cb12-78"><a href="#cb12-78" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W1 <span class="op">-=</span> <span class="va">self</span>.a <span class="op">*</span> dloss_dW1</span>
<span id="cb12-79"><a href="#cb12-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-80"><a href="#cb12-80" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b3 <span class="op">-=</span> <span class="va">self</span>.a <span class="op">*</span> dloss_db3</span>
<span id="cb12-81"><a href="#cb12-81" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b2 <span class="op">-=</span> <span class="va">self</span>.a <span class="op">*</span> dloss_db2</span>
<span id="cb12-82"><a href="#cb12-82" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b1 <span class="op">-=</span> <span class="va">self</span>.a <span class="op">*</span> dloss_db1</span>
<span id="cb12-83"><a href="#cb12-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-84"><a href="#cb12-84" aria-hidden="true" tabindex="-1"></a><span class="co"># We won't run this here because it is very slow,</span></span>
<span id="cb12-85"><a href="#cb12-85" aria-hidden="true" tabindex="-1"></a><span class="co"># but it gives an accuracy of ~83%</span></span>
<span id="cb12-86"><a href="#cb12-86" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> run:</span>
<span id="cb12-87"><a href="#cb12-87" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-88"><a href="#cb12-88" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialise and train the model (no batching)</span></span>
<span id="cb12-89"><a href="#cb12-89" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> MyBetterNetwork()</span>
<span id="cb12-90"><a href="#cb12-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-91"><a href="#cb12-91" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (X, y) <span class="kw">in</span> <span class="bu">enumerate</span>(training_data):</span>
<span id="cb12-92"><a href="#cb12-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-93"><a href="#cb12-93" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> np.asarray(X[<span class="dv">0</span>, :, :])</span>
<span id="cb12-94"><a href="#cb12-94" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> <span class="bu">int</span>(y)</span>
<span id="cb12-95"><a href="#cb12-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-96"><a href="#cb12-96" aria-hidden="true" tabindex="-1"></a>        model.update_weights(x, y, verbose<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb12-97"><a href="#cb12-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-98"><a href="#cb12-98" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb12-99"><a href="#cb12-99" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Training iteration: sample: </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-100"><a href="#cb12-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-101"><a href="#cb12-101" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check the model accuracy</span></span>
<span id="cb12-102"><a href="#cb12-102" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> np.empty(<span class="bu">len</span>(test_data))</span>
<span id="cb12-103"><a href="#cb12-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-104"><a href="#cb12-104" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (X, y) <span class="kw">in</span> <span class="bu">enumerate</span>(test_data):</span>
<span id="cb12-105"><a href="#cb12-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-106"><a href="#cb12-106" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> np.asarray(X[<span class="dv">0</span>, :, :])</span>
<span id="cb12-107"><a href="#cb12-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-108"><a href="#cb12-108" aria-hidden="true" tabindex="-1"></a>        results[i] <span class="op">=</span> model.predict(x)[<span class="dv">0</span>] <span class="op">==</span> y</span>
<span id="cb12-109"><a href="#cb12-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-110"><a href="#cb12-110" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Percent Correct: </span><span class="sc">{</span>np<span class="sc">.</span>mean(results) <span class="op">*</span> <span class="dv">100</span><span class="sc">}</span><span class="ss">%"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p><strong>Appendix 1</strong></p>
<p>The cross entropy loss is:</p>
<span class="math display">\[\begin{aligned}
    L &amp;= -\log \dfrac{ \exp{ l_{3, y} } }{ \sum_k \exp{ l_{3, k} } } \\
    &amp;= -\bigg[ \log \exp{ l_{3, y} } - \log \sum_k \exp{ l_{3, k} } \bigg] \\
    &amp;=  \log \sum_k \exp{ l_{3, k} } - l_{3, y}
\end{aligned}\]</span>
<p>(by the log laws). i.e.&nbsp;we take the logit of layer 3 that matches the correct label <span class="math inline">\(y\)</span>, normalise it to a probability with the softmax function and take the negative log.</p>
<p>Let’s start by taking the derivative with respect to <span class="math inline">\(l_{3, y}\)</span> where this is shorthand for <span class="math inline">\(l_{3, i}\)</span>, <span class="math inline">\(i=y\)</span> i.e. the layer 3 logit for the label that is correct for this image. We are asking: how does a small change in this logit effect the loss?</p>
<p><span class="math display">\[
\begin{aligned}
\dfrac{ \partial L }{ \partial l_{3, y} } &amp;= \dfrac{ \partial }{ \partial l_{3, y} } \left( \log \sum_k \exp{ l_{3, k} } - l_{3, y} \right)
&amp;=  \dfrac{ \partial }{ \partial l_{3, y} } \log \sum_k \exp{ l_{3, k} } - \dfrac{ \partial }{ \partial l_{3, y} }  l_{3, y}  \\
&amp;= \dfrac{1}{ \sum_k \exp{ l_{3, k} } }  \dfrac{ \partial }{ \partial l_{3, y} } \sum_k \exp{ l_{3, k} } - 1
\end{aligned}
\]</span></p>
<p>(by the derivative of <span class="math inline">\(\log x\)</span> rule). Note that the last term will be <span class="math inline">\(0\)</span> when the the input dimension is not <span class="math inline">\(y\)</span> (because it is treated as a scalar).</p>
<p>We see that in the sum, the derivative of $ $ w.r.t <span class="math inline">\(l_{3, k}\)</span> is <span class="math inline">\(\exp{ l_{3, i} }\)</span> when <span class="math inline">\(i = k\)</span> and <span class="math inline">\(0\)</span> otherwise (as it is treated as a scalar). So whatever dimension <span class="math inline">\(i\)</span> of <span class="math inline">\(l_3\)</span>, we will input to the loss, we get <span class="math inline">\(\text{softmax}(\mathbf{l_3})_i\)</span> as the first term. But only when <span class="math inline">\(i = y\)</span> do we get <span class="math inline">\(-1\)</span> in the second term.</p>
<p><strong>Appendix 2</strong></p>
<p>In multivariate calculus, the total derivative rule captures how a function changes with respect to a variable that affects the function in multiple ways (through multiple intermediary functions).</p>
<p>For example, let’s say we have the variable <span class="math inline">\(t\)</span> and let:</p>
<p><span class="math display">\[
w = f(x(t), y(t))
\]</span></p>
<p>The total derivative rule tells us how the output, <span class="math inline">\(w\)</span> changes with a small change in <span class="math inline">\(t\)</span>. Intuitively, it is the sum of how <span class="math inline">\(\delta t\)</span> changes <span class="math inline">\(w\)</span> through <span class="math inline">\(x(t)\)</span> and how <span class="math inline">\(\delta t\)</span> changes <span class="math inline">\(w\)</span> through <span class="math inline">\(y(t)\)</span>:</p>
<p><span class="math display">\[
\dfrac{dw}{dt} = \dfrac{\partial w}{ \partial x(t)} \dfrac{\partial x(t)}{ \partial t} + \dfrac{\partial w}{ \partial y(t)} \dfrac{\partial y(t)}{ \partial t}
\]</span></p>
<p>Note this is exactly analogous to our set up with the layers and weights. A small change in a node in layer 2 will effect the loss through every node in layer 3. Let’s look at node 1 from layer 2 (<span class="math inline">\(l^2_1\)</span>) as an example. Here layer 3 nodes are a function taking <span class="math inline">\(l^2_1\)</span>, which is like <span class="math inline">\(t\)</span> in the above example, as an input:</p>
<p><span class="math display">\[
\text{loss} = L( \ l^3_1(l^2_1), \ l^3_2(l^2_1), \ ..., \ l^3_{10}(l^2_1) \ )
\]</span></p>
<p><span class="math display">\[
\dfrac{\partial L}{ \partial l^2_1 } = \dfrac{\partial L}{ \partial l^3_1 } \dfrac{ \partial l^3_1}{ \partial l^2_1 } + \dfrac{\partial L}{ \partial l^3_2 } \dfrac{\partial l^3_2 }{ \partial l^2_1 } + ... + \dfrac{\partial L}{ \partial l^3_{10} } \dfrac{\partial l^3_{10} }{ \partial l^2_1 }
\]</span></p>
<p>Which is exactly what we do to deal with these derivatives.</p>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>